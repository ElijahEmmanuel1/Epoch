/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   Course Data Model & Sample Content
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

export interface CourseNode {
  id: string;
  title: string;
  shortTitle: string;
  description: string;
  status: 'locked' | 'available' | 'in-progress' | 'completed';
  progress: number; // 0-100
  dependencies: string[];
  category: 'fundamentals' | 'architectures' | 'training' | 'advanced';
  exercises: Exercise[];
  theory: TheoryBlock[];
  codeTemplate: string;
  expectedOutput?: string;
}

export interface Exercise {
  id: string;
  title: string;
  instructions: string;
  starterCode: string;
  solution: string;
  hints: string[];
  completed: boolean;
}

export interface TheoryBlock {
  type: 'text' | 'equation' | 'diagram' | 'callout';
  content: string;
  label?: string;
  highlightVar?: string; // variable name that links to code
}

// â”€â”€ Sample Course Data â”€â”€
// Content enriched from "Understanding Deep Learning" by Simon J.D. Prince (MIT Press)
export const courseNodes: CourseNode[] = [
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 1 â€” SUPERVISED LEARNING INTRO
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'supervised-learning',
    title: 'Introduction Ã  l\'Apprentissage SupervisÃ©',
    shortTitle: 'SupervisÃ©',
    description: 'Les bases de l\'apprentissage supervisÃ© : modÃ¨les, paramÃ¨tres, fonctions de perte et entraÃ®nement.',
    status: 'available',
    progress: 0,
    dependencies: [],
    category: 'fundamentals',
    theory: [
      {
        type: 'text',
        content: `En **apprentissage supervisÃ©**, on construit un modÃ¨le **f[x, Ï•]** qui prend une entrÃ©e **x** et produit une prÃ©diction **y**. Le modÃ¨le est une Ã©quation mathÃ©matique de forme fixe contenant des **paramÃ¨tres Ï•** qui dÃ©terminent la relation entre entrÃ©e et sortie.`,
      },
      {
        type: 'equation',
        content: 'y = f[\\mathbf{x}, \\boldsymbol{\\phi}]',
        label: 'ModÃ¨le de prÃ©diction',
        highlightVar: 'y',
      },
      {
        type: 'text',
        content: `**EntraÃ®ner** un modÃ¨le consiste Ã  trouver les paramÃ¨tres **Ï•** qui minimisent une **fonction de perte L[Ï•]**. Cette perte quantifie l'Ã©cart entre les prÃ©dictions du modÃ¨le et les sorties rÃ©elles du jeu de donnÃ©es d'entraÃ®nement {xáµ¢, yáµ¢}.`,
      },
      {
        type: 'equation',
        content: '\\hat{\\boldsymbol{\\phi}} = \\underset{\\boldsymbol{\\phi}}{\\text{argmin}} \\; \\mathcal{L}[\\boldsymbol{\\phi}]',
        label: 'Minimisation de la perte',
        highlightVar: 'loss',
      },
      {
        type: 'text',
        content: `L'exemple le plus simple est la **rÃ©gression linÃ©aire 1D** : le modÃ¨le dÃ©crit une droite y = Ï•â‚€ + Ï•â‚x, oÃ¹ Ï•â‚€ est l'ordonnÃ©e Ã  l'origine et Ï•â‚ la pente. La perte des **moindres carrÃ©s** mesure la somme des carrÃ©s des Ã©carts :`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}[\\boldsymbol{\\phi}] = \\sum_{i=1}^{I} (f[x_i, \\boldsymbol{\\phi}] - y_i)^2',
        label: 'Perte des moindres carrÃ©s (Least Squares)',
        highlightVar: 'loss',
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ AprÃ¨s l\'entraÃ®nement, on Ã©value le modÃ¨le sur des **donnÃ©es de test** sÃ©parÃ©es pour mesurer sa capacitÃ© de **gÃ©nÃ©ralisation** â€” c\'est-Ã -dire sa performance sur des exemples qu\'il n\'a jamais vus.',
      },
    ],
    exercises: [
      {
        id: 'sl-ex1',
        title: 'RÃ©gression linÃ©aire depuis zÃ©ro',
        instructions: 'ImplÃ©mentez une rÃ©gression linÃ©aire simple y = Ï•â‚€ + Ï•â‚x et calculez la perte des moindres carrÃ©s.',
        starterCode: `import torch

# DonnÃ©es d'entraÃ®nement
x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
y = torch.tensor([2.1, 3.9, 6.2, 7.8, 10.1])

# ParamÃ¨tres du modÃ¨le (Ã  initialiser)
phi_0 = ___  # ordonnÃ©e Ã  l'origine
phi_1 = ___  # pente

# PrÃ©diction : y = phi_0 + phi_1 * x
y_pred = ___

# Perte des moindres carrÃ©s
loss = ___

print(f"PrÃ©dictions: {y_pred}")
print(f"Perte: {loss.item():.4f}")`,
        solution: `import torch

# DonnÃ©es d'entraÃ®nement
x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
y = torch.tensor([2.1, 3.9, 6.2, 7.8, 10.1])

# ParamÃ¨tres du modÃ¨le
phi_0 = torch.tensor(0.0)  # ordonnÃ©e Ã  l'origine
phi_1 = torch.tensor(2.0)  # pente

# PrÃ©diction : y = phi_0 + phi_1 * x
y_pred = phi_0 + phi_1 * x

# Perte des moindres carrÃ©s
loss = torch.sum((y_pred - y) ** 2)

print(f"PrÃ©dictions: {y_pred}")
print(f"Perte: {loss.item():.4f}")`,
        hints: [
          'Initialisez phi_0 et phi_1 avec torch.tensor(valeur)',
          'y_pred = phi_0 + phi_1 * x',
          'loss = torch.sum((y_pred - y) ** 2)',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch

# â•â• Apprentissage SupervisÃ© â€” RÃ©gression LinÃ©aire â•â•

# DonnÃ©es d'entraÃ®nement (entrÃ©e â†’ sortie)
x_train = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
y_train = torch.tensor([2.1, 3.9, 6.2, 7.8, 10.1])

# ModÃ¨le : y = phi_0 + phi_1 * x
phi_0 = torch.tensor(0.0, requires_grad=True)
phi_1 = torch.tensor(0.0, requires_grad=True)

# EntraÃ®nement par descente de gradient
learning_rate = 0.01
for epoch in range(100):
    # Forward pass
    y_pred = phi_0 + phi_1 * x_train
    
    # Perte des moindres carrÃ©s
    loss = torch.sum((y_pred - y_train) ** 2)
    
    # Backward pass
    loss.backward()
    
    # Mise Ã  jour des paramÃ¨tres
    with torch.no_grad():
        phi_0 -= learning_rate * phi_0.grad
        phi_1 -= learning_rate * phi_1.grad
        phi_0.grad.zero_()
        phi_1.grad.zero_()
    
    if (epoch + 1) % 20 == 0:
        print(f"Epoch {epoch+1}: loss={loss.item():.4f}, y={phi_1.item():.2f}x + {phi_0.item():.2f}")

print(f"\\nModÃ¨le final: y = {phi_1.item():.2f}x + {phi_0.item():.2f}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 2 â€” TENSEURS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'tensors',
    title: 'Tenseurs & OpÃ©rations',
    shortTitle: 'Tenseurs',
    description: 'Comprendre les tenseurs â€” la structure de donnÃ©es fondamentale du Deep Learning â€” et leurs opÃ©rations.',
    status: 'available',
    progress: 0,
    dependencies: ['supervised-learning'],
    category: 'fundamentals',
    theory: [
      {
        type: 'text',
        content: `Un **tenseur** est la structure de donnÃ©es fondamentale du Deep Learning. C'est une gÃ©nÃ©ralisation des matrices Ã  N dimensions. En Deep Learning, toutes les donnÃ©es (images, texte, audio) et tous les paramÃ¨tres des modÃ¨les sont reprÃ©sentÃ©s comme des tenseurs.\n\n- **Scalaire** : tenseur de rang 0 (un seul nombre)\n- **Vecteur** : tenseur de rang 1 (liste de nombres)\n- **Matrice** : tenseur de rang 2 (grille 2D)\n- **Tenseur 3D+** : rang 3+ (ex: image RGB = 3Ã—HÃ—W)`,
      },
      {
        type: 'equation',
        content: '\\mathbf{T} \\in \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_n}',
        label: 'Forme d\'un tenseur de rang n',
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ En PyTorch, `torch.tensor()` crÃ©e un tenseur depuis des donnÃ©es Python. Utilisez `.shape` pour inspecter ses dimensions et `.dtype` pour son type.',
      },
      {
        type: 'text',
        content: `La **multiplication matricielle** est l'opÃ©ration la plus fondamentale en Deep Learning. C'est elle qui implÃ©mente les transformations linÃ©aires au cÅ“ur de chaque couche neuronale. Pour que AÃ—B soit dÃ©fini, le nombre de colonnes de A doit Ã©galer le nombre de lignes de B.`,
      },
      {
        type: 'equation',
        content: '\\mathbf{C} = \\mathbf{A} \\mathbf{B} \\quad \\text{oÃ¹ } C_{ij} = \\sum_{k} A_{ik} B_{kj}',
        label: 'Multiplication matricielle',
        highlightVar: 'result',
      },
      {
        type: 'text',
        content: `Le **broadcasting** permet d'effectuer des opÃ©rations entre tenseurs de tailles diffÃ©rentes. PyTorch aligne automatiquement les dimensions en partant de la droite et expand les dimensions de taille 1.`,
      },
    ],
    exercises: [
      {
        id: 'tensors-ex1',
        title: 'CrÃ©er et manipuler des tenseurs',
        instructions: 'CrÃ©ez un tenseur 3Ã—4 rempli de uns, multipliez-le par un tenseur 4Ã—2 alÃ©atoire, et affichez les formes.',
        starterCode: `import torch

# CrÃ©ez un tenseur 3x4 rempli de uns
A = ___

# CrÃ©ez un tenseur 4x2 alÃ©atoire (normal)
B = ___

# Multiplication matricielle
result = ___

print(f"Shape de A: {A.shape}")
print(f"Shape de B: {B.shape}")
print(f"Shape du rÃ©sultat: {result.shape}")
print(f"RÃ©sultat:\\n{result}")`,
        solution: `import torch

# CrÃ©ez un tenseur 3x4 rempli de uns
A = torch.ones(3, 4)

# CrÃ©ez un tenseur 4x2 alÃ©atoire (normal)
B = torch.randn(4, 2)

# Multiplication matricielle
result = torch.matmul(A, B)

print(f"Shape de A: {A.shape}")
print(f"Shape de B: {B.shape}")
print(f"Shape du rÃ©sultat: {result.shape}")
print(f"RÃ©sultat:\\n{result}")`,
        hints: [
          'torch.ones(rows, cols) crÃ©e un tenseur de uns',
          'torch.randn(rows, cols) gÃ©nÃ¨re des valeurs alÃ©atoires normales',
          'torch.matmul(A, B) ou A @ B pour la multiplication matricielle',
        ],
        completed: false,
      },
      {
        id: 'tensors-ex2',
        title: 'Broadcasting et opÃ©rations',
        instructions: 'CrÃ©ez un tenseur 3Ã—1 et un tenseur 1Ã—4. Additionnez-les pour obtenir un tenseur 3Ã—4 grÃ¢ce au broadcasting.',
        starterCode: `import torch

# Tenseur colonne 3x1
col = ___

# Tenseur ligne 1x4
row = ___

# Broadcasting : 3x1 + 1x4 â†’ 3x4
result = ___

print(f"col shape: {col.shape}")
print(f"row shape: {row.shape}")
print(f"result shape: {result.shape}")
print(f"result:\\n{result}")`,
        solution: `import torch

# Tenseur colonne 3x1
col = torch.tensor([[1.0], [2.0], [3.0]])

# Tenseur ligne 1x4
row = torch.tensor([[10.0, 20.0, 30.0, 40.0]])

# Broadcasting : 3x1 + 1x4 â†’ 3x4
result = col + row

print(f"col shape: {col.shape}")
print(f"row shape: {row.shape}")
print(f"result shape: {result.shape}")
print(f"result:\\n{result}")`,
        hints: [
          'Utilisez torch.tensor([[1.0], [2.0], [3.0]]) pour une colonne 3x1',
          'Le broadcasting aligne les dimensions depuis la droite',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch

# â•â• Exploration des Tenseurs â•â•

# Scalaire (rang 0)
scalar = torch.tensor(42.0)
print(f"Scalaire: {scalar}, shape: {scalar.shape}, ndim: {scalar.ndim}")

# Vecteur (rang 1)
vector = torch.tensor([1.0, 2.0, 3.0])
print(f"Vecteur: {vector}, shape: {vector.shape}")

# Matrice (rang 2)
matrix = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float32)
print(f"Matrice shape: {matrix.shape}")

# Tenseur 3D (rang 3) â€” comme une image RGB
image = torch.randn(3, 224, 224)
print(f"Image tensor shape: {image.shape}")

# â”€â”€ OpÃ©rations fondamentales â”€â”€
A = torch.ones(3, 4)
B = torch.randn(4, 2)
result = A @ B  # multiplication matricielle
print(f"\\n{A.shape} Ã— {B.shape} = {result.shape}")

# Broadcasting
col = torch.tensor([[1.0], [2.0], [3.0]])  # 3x1
row = torch.tensor([[10.0, 20.0, 30.0]])    # 1x3
print(f"\\nBroadcasting: {col.shape} + {row.shape} = {(col + row).shape}")
print(col + row)
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 3 â€” RÃ‰SEAUX SUPERFICIELS
  // (Ch. 3 â€” Understanding Deep Learning)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'shallow-networks',
    title: 'RÃ©seaux de Neurones Superficiels',
    shortTitle: 'Shallow Net',
    description: 'Comprendre les rÃ©seaux Ã  une couche cachÃ©e, ReLU, les rÃ©gions linÃ©aires et le thÃ©orÃ¨me d\'approximation universelle (Ch. 3 â€” UDL).',
    status: 'available',
    progress: 0,
    dependencies: ['tensors'],
    category: 'fundamentals',
    theory: [
      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 1 : INTRODUCTION â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `Le chapitre 2 a introduit la rÃ©gression linÃ©aire (une droite). Mais une droite ne peut pas capturer de relations complexes. Les **rÃ©seaux superficiels** (shallow neural networks) dÃ©crivent des **fonctions linÃ©aires par morceaux** (piecewise linear functions) suffisamment expressives pour approximer n'importe quelle relation entre entrÃ©es et sorties.`,
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ Un rÃ©seau "superficiel" dÃ©signe un rÃ©seau avec **une seule couche cachÃ©e** (hidden layer). Ce terme contraste avec "profond" (deep), qui dÃ©signe les rÃ©seaux Ã  plusieurs couches cachÃ©es (Ch. 4).',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 2 : L'EXEMPLE DU RÃ‰SEAU â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.1 â€” Exemple de rÃ©seau neuronal\n\nConsidÃ©rons un rÃ©seau avec **10 paramÃ¨tres** Ï• = {Ï•â‚€, Ï•â‚, Ï•â‚‚, Ï•â‚ƒ, Î¸â‚â‚€, Î¸â‚â‚, Î¸â‚‚â‚€, Î¸â‚‚â‚, Î¸â‚ƒâ‚€, Î¸â‚ƒâ‚} qui transforme un scalaire x en un scalaire y. Le calcul se fait en **3 Ã©tapes** :`,
      },
      {
        type: 'diagram',
        content: `      Ã‰TAPE 1             Ã‰TAPE 2              Ã‰TAPE 3
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ 3 fonctions â”‚   â”‚  Activation  â”‚   â”‚   Combinaison     â”‚
  â”‚ linÃ©aires   â”‚â”€â”€â–¶â”‚   ReLU a[â€¢]  â”‚â”€â”€â–¶â”‚   linÃ©aire        â”‚
  â”‚ de l'entrÃ©e â”‚   â”‚  (clip < 0)  â”‚   â”‚   + offset Ï•â‚€     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Î¸â‚â‚€ + Î¸â‚â‚Â·x  â”€â”€â–¶  hâ‚ = ReLU[â€¢] â”€â”€â”
                                      â”œâ”€â”€â–¶ y = Ï•â‚€ + Ï•â‚hâ‚ + Ï•â‚‚hâ‚‚ + Ï•â‚ƒhâ‚ƒ
  Î¸â‚‚â‚€ + Î¸â‚‚â‚Â·x  â”€â”€â–¶  hâ‚‚ = ReLU[â€¢] â”€â”€â”¤
                                      â”‚
  Î¸â‚ƒâ‚€ + Î¸â‚ƒâ‚Â·x  â”€â”€â–¶  hâ‚ƒ = ReLU[â€¢] â”€â”€â”˜`,
        label: 'Fig. 3.3 â€” Pipeline de calcul d\'un rÃ©seau superficiel',
      },
      {
        type: 'equation',
        content: 'y = f[x, \\boldsymbol{\\phi}] = \\phi_0 + \\phi_1 \\, a[\\theta_{10} + \\theta_{11} x] + \\phi_2 \\, a[\\theta_{20} + \\theta_{21} x] + \\phi_3 \\, a[\\theta_{30} + \\theta_{31} x]',
        label: 'Ã‰q. 3.1 â€” RÃ©seau superficiel (Shallow Network)',
        highlightVar: 'output',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 3 : ReLU â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.1.1 â€” La fonction d'activation ReLU\n\nLa fonction d'activation **a[â€¢]** est ce qui rend le rÃ©seau **non-linÃ©aire**. Sans elle, le rÃ©seau ne serait qu'une autre fonction linÃ©aire (voir ProblÃ¨me 3.1). Le choix le plus courant est le **ReLU** (Rectified Linear Unit) :`,
      },
      {
        type: 'equation',
        content: 'a[z] = \\text{ReLU}[z] = \\max(0, z) = \\begin{cases} 0 & \\text{si } z < 0 \\\\ z & \\text{si } z \\geq 0 \\end{cases}',
        label: 'Ã‰q. 3.2 â€” Rectified Linear Unit (ReLU)',
        highlightVar: 'relu',
      },
      {
        type: 'diagram',
        content: `  y â–²
    â”‚        â•±
    â”‚       â•±
    â”‚      â•±   â† pente = 1
    â”‚     â•±
    â”‚    â•±
  â”€â”€â”¼â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ z
    â”‚  â•±
    â”‚ (clipped Ã  0 pour z < 0)
    â”‚`,
        label: 'Fig. 3.1 â€” Graphe du ReLU : retourne z si z â‰¥ 0, sinon 0',
      },
      {
        type: 'text',
        content: `**En PyTorch**, le ReLU existe sous 3 formes :\n\n- **\`torch.relu(tensor)\`** â€” fonction simple appliquÃ©e Ã  un tenseur\n- **\`torch.clamp(tensor, min=0)\`** â€” Ã©quivalent plus explicite\n- **\`nn.ReLU()\`** â€” module rÃ©utilisable dans un \`nn.Sequential\` ou \`nn.Module\`\n\n**Pourquoi ReLU est si populaire ?** Sa dÃ©rivÃ©e vaut 1 pour z > 0 et 0 pour z < 0. Cela rend le gradient stable pendant la backpropagation (contrairement au sigmoid/tanh qui "saturent").`,
      },
      {
        type: 'callout',
        content: 'âš  **Le problÃ¨me du "dying ReLU"** : si toutes les entrÃ©es d\'un neurone sont nÃ©gatives, le ReLU retourne toujours 0 et le gradient est nul. Ce neurone est "mort" â€” il ne peut plus apprendre. Solutions : Leaky ReLU (pente 0.01 pour z < 0), Parametric ReLU, ou ELU.',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 4 : UNITÃ‰S CACHÃ‰ES â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.1.2 â€” Les unitÃ©s cachÃ©es (Hidden Units)\n\nLe calcul se dÃ©compose naturellement en **unitÃ©s cachÃ©es** hâ‚, hâ‚‚, hâ‚ƒ. Chaque unitÃ© est un neurone qui applique une fonction linÃ©aire de l'entrÃ©e puis la passe par ReLU :`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} h_1 &= a[\\theta_{10} + \\theta_{11}x] \\\\ h_2 &= a[\\theta_{20} + \\theta_{21}x] \\\\ h_3 &= a[\\theta_{30} + \\theta_{31}x] \\end{aligned}',
        label: 'Ã‰q. 3.3 â€” Calcul des unitÃ©s cachÃ©es',
        highlightVar: 'relu',
      },
      {
        type: 'text',
        content: `Puis la sortie combine linÃ©airement ces unitÃ©s cachÃ©es :`,
      },
      {
        type: 'equation',
        content: 'y = \\phi_0 + \\phi_1 h_1 + \\phi_2 h_2 + \\phi_3 h_3',
        label: 'Ã‰q. 3.4 â€” Combinaison linÃ©aire de sortie',
        highlightVar: 'output',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 5 : PATTERNS D'ACTIVATION â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.1.3 â€” RÃ©gions linÃ©aires & Patterns d'activation\n\nChaque unitÃ© cachÃ©e crÃ©e un **"joint"** (coude) dans la fonction de sortie â€” le point oÃ¹ la droite Î¸â€¢â‚€ + Î¸â€¢â‚x croise zÃ©ro. De part et d'autre de ce joint, l'unitÃ© est soit **active** (z â‰¥ 0, passe l'entrÃ©e) soit **inactive** (z < 0, retourne 0).\n\nAvec 3 unitÃ©s cachÃ©es, on obtient jusqu'Ã  **4 rÃ©gions linÃ©aires** et **3 joints**. Chaque rÃ©gion correspond Ã  un **pattern d'activation** diffÃ©rent :`,
      },
      {
        type: 'diagram',
        content: `  y â–²
    â”‚         â•±â•²
    â”‚        â•±  â•²          â•±
    â”‚       â•±    â•²        â•±
    â”‚      â•±      â•²      â•±
    â”‚     â•±        â•²    â•±
    â”‚    â•±    R2    â•²  â•±
    â”‚   â•±            â•²â•±
    â”‚  â•±  R1      R3    R4
  â”€â”€â”¼â”€â•±â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–¶ x
    â”‚  jointâ‚   jointâ‚‚   jointâ‚ƒ

  R1 : hâ‚=off, hâ‚‚=off, hâ‚ƒ=off  â†’  pente = 0
  R2 : hâ‚=on,  hâ‚‚=off, hâ‚ƒ=off  â†’  pente = Ï•â‚Â·Î¸â‚â‚
  R3 : hâ‚=on,  hâ‚‚=on,  hâ‚ƒ=off  â†’  pente = Ï•â‚Â·Î¸â‚â‚ + Ï•â‚‚Â·Î¸â‚‚â‚
  R4 : hâ‚=on,  hâ‚‚=on,  hâ‚ƒ=on   â†’  pente = Ï•â‚Â·Î¸â‚â‚ + Ï•â‚‚Â·Î¸â‚‚â‚ + Ï•â‚ƒÂ·Î¸â‚ƒâ‚`,
        label: 'Fig. 3.2 â€” Fonction linÃ©aire par morceaux avec 4 rÃ©gions',
      },
      {
        type: 'callout',
        content: 'ğŸ§  **Intuition clÃ©** : la pente de chaque rÃ©gion est la somme des pentes Î¸â€¢â‚ Ã— Ï•â€¢ des unitÃ©s **actives** dans cette rÃ©gion. L\'offset Ï•â‚€ contrÃ´le la hauteur globale. C\'est ainsi qu\'on "dessine" des fonctions complexes morceau par morceau.',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 6 : NOTATION MATRICIELLE â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.2 â€” Notation matricielle & PyTorch\n\nOn regroupe le calcul en notation matricielle. Soit **Î²â‚€** le vecteur de biais de la couche cachÃ©e, **Î©â‚€** la matrice de poids d'entrÃ©e, **Î²â‚** le biais de sortie, et **Ï‰â‚** les poids de sortie :`,
      },
      {
        type: 'equation',
        content: '\\mathbf{h} = a\\!\\left[\\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x}\\right] \\qquad y = \\beta_1 + \\boldsymbol{\\omega}_1^T \\mathbf{h}',
        label: 'Notation matricielle compacte',
        highlightVar: 'hidden',
      },
      {
        type: 'text',
        content: `**En PyTorch, \`nn.Linear(in, out)\`** implÃ©mente exactement cette opÃ©ration :\n\n- Il stocke une matrice de poids **W** de taille (out Ã— in)\n- Un vecteur de biais **b** de taille (out)\n- La sortie est : **output = x @ W.T + b**\n\nUn rÃ©seau superficiel complet se construit avec :`,
      },
      {
        type: 'diagram',
        content: `  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  model = nn.Sequential(                          â”‚
  â”‚      nn.Linear(D_i, D),    # Î©â‚€Â·x + Î²â‚€          â”‚
  â”‚      nn.ReLU(),            # a[â€¢]                â”‚
  â”‚      nn.Linear(D, D_o)     # Ï‰â‚áµ€Â·h + Î²â‚          â”‚
  â”‚  )                                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  OÃ¹ :
    D_i = dimension d'entrÃ©e
    D   = nombre d'unitÃ©s cachÃ©es (hidden units)
    D_o = dimension de sortie`,
        label: 'Construction PyTorch d\'un rÃ©seau superficiel',
      },
      {
        type: 'text',
        content: `**Fonctions PyTorch utiles pour cette Ã©tape :**\n\n- **\`nn.Sequential(*layers)\`** : empile des couches en sÃ©quence, forward automatique\n- **\`nn.Linear(in_features, out_features)\`** : couche dense (transformation affine)\n- **\`nn.ReLU()\`** : module d'activation ReLU (rÃ©utilisable)\n- **\`model.parameters()\`** : itÃ©rateur sur tous les poids/biais du modÃ¨le\n- **\`sum(p.numel() for p in model.parameters())\`** : compte le nombre total de paramÃ¨tres`,
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 7 : NOMBRE DE PARAMÃˆTRES â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.3 â€” Comptage des paramÃ¨tres\n\nUn rÃ©seau superficiel avec **Dáµ¢** entrÃ©es, **D** unitÃ©s cachÃ©es et **Dâ‚’** sorties a :`,
      },
      {
        type: 'equation',
        content: 'N_{\\text{params}} = \\underbrace{D \\cdot (D_i + 1)}_{\\text{couche cachÃ©e}} + \\underbrace{D_o \\cdot (D + 1)}_{\\text{couche de sortie}}',
        label: 'Ã‰q. â€” Nombre de paramÃ¨tres (ProblÃ¨me 3.17)',
      },
      {
        type: 'text',
        content: `Exemple : Dáµ¢ = 1, D = 3, Dâ‚’ = 1 â†’ 3Ã—(1+1) + 1Ã—(3+1) = 6 + 4 = **10 paramÃ¨tres** â€” exactement les 10 de l'Ã‰q. 3.1 !\n\nExemple 2 : Dáµ¢ = 784 (image 28Ã—28), D = 100, Dâ‚’ = 10 â†’ 100Ã—785 + 10Ã—101 = 78,500 + 1,010 = **79,510 paramÃ¨tres**.`,
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 8 : THÃ‰ORÃˆME D'APPROXIMATION â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.4 â€” ThÃ©orÃ¨me d'approximation universelle\n\nAvec D unitÃ©s cachÃ©es et ReLU, le rÃ©seau crÃ©e au maximum **D + 1 rÃ©gions linÃ©aires**. Plus la fonction cible est complexe, plus il faut de rÃ©gions (et donc d'unitÃ©s) pour l'approximer :`,
      },
      {
        type: 'diagram',
        content: `  D = 2           D = 5               D = 20
  â–²               â–²                   â–²
  â”‚ â•±â•²            â”‚    â•±â•²              â”‚  Â·âˆ¼âˆ¼âˆ¼âˆ¼Â·
  â”‚â•±  â•²  â•±       â”‚   â•±  â•²  â•±â•²        â”‚ âˆ«   f(x) dx
  â”‚    â•²â•±         â”‚  â•±    â•²â•±  â•² â•±    â”‚ â‰ˆ somme de
  â”‚               â”‚ â•±          â•²â•±     â”‚   segments
  â””â”€â”€â”€â”€â”€â”€â–¶       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
  3 rÃ©gions       6 rÃ©gions           â‰ˆ courbe lisse`,
        label: 'Fig. 3.5 â€” Approximation : plus de hidden units â†’ plus de rÃ©gions â†’ meilleure fidÃ©litÃ©',
      },
      {
        type: 'callout',
        content: 'ğŸ§  **ThÃ©orÃ¨me d\'approximation universelle** (Cybenko 1989, Hornik 1991) : pour toute fonction continue f dÃ©finie sur un compact et tout Îµ > 0, il existe un rÃ©seau superficiel avec suffisamment d\'unitÃ©s cachÃ©es tel que |f(x) - rÃ©seau(x)| < Îµ pour tout x. En d\'autres termes, un rÃ©seau Ã  **une seule couche cachÃ©e peut approximer n\'importe quelle fonction continue** !',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 9 : ENTRÃ‰ES/SORTIES MULTIDIMENSIONNELLES â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.5 â€” EntrÃ©es et sorties multidimensionnelles\n\n**EntrÃ©es multiples (Dáµ¢ > 1)** : chaque unitÃ© cachÃ©e reÃ§oit une combinaison linÃ©aire de **toutes** les entrÃ©es. Par exemple avec 2 entrÃ©es x = [xâ‚, xâ‚‚]áµ€ :`,
      },
      {
        type: 'equation',
        content: 'h_d = a\\!\\left[\\theta_{d0} + \\theta_{d1} x_1 + \\theta_{d2} x_2\\right]',
        label: 'Ã‰q. 3.9 â€” UnitÃ© cachÃ©e avec 2 entrÃ©es',
      },
      {
        type: 'text',
        content: `En 2D, le ReLU crÃ©e des **hyperplans** (droites) qui divisent le plan d'entrÃ©e en **rÃ©gions convexes polygonales**. Chaque rÃ©gion a une surface linÃ©aire diffÃ©rente.\n\n**Sorties multiples (Dâ‚’ > 1)** : on utilise une combinaison linÃ©aire **diffÃ©rente** des mÃªmes unitÃ©s cachÃ©es pour chaque sortie. Les joints restent aux mÃªmes positions, mais les pentes varient.`,
      },
      {
        type: 'text',
        content: `**Nombre de rÃ©gions en haute dimension** :\n\nAvec Dáµ¢ â‰¥ 2 et D unitÃ©s cachÃ©es, le nombre maximum de rÃ©gions est donnÃ© par la formule de Zaslavsky (1975) :`,
      },
      {
        type: 'equation',
        content: 'N_{\\text{regions}} = \\sum_{j=0}^{D_i} \\binom{D}{j}',
        label: 'Formule de Zaslavsky â€” Nombre max de rÃ©gions',
      },
      {
        type: 'text',
        content: `Avec Dáµ¢ dimensions et D â‰¥ Dáµ¢ unitÃ©s cachÃ©es, on crÃ©e au minimum **2^Dáµ¢** rÃ©gions (en alignant chaque hyperplan avec un axe de coordonnÃ©es). Exemple : D = 500, Dáµ¢ = 100 â†’ plus de **10Â¹â°â·** rÃ©gions !`,
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 10 : TERMINOLOGIE â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.6 â€” Terminologie\n\nLe rÃ©seau est dÃ©crit en **couches** (layers) :\n\n- **Input layer** (couche d'entrÃ©e) : les donnÃ©es x\n- **Hidden layer** (couche cachÃ©e) : les neurones hd avec ReLU\n- **Output layer** (couche de sortie) : la prÃ©diction y\n\nAutres termes importants :`,
      },
      {
        type: 'diagram',
        content: `  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘                    VOCABULAIRE                            â•‘
  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
  â•‘ Multi-layer      â”‚ Tout rÃ©seau avec â‰¥ 1 couche cachÃ©e    â•‘
  â•‘ perceptron (MLP) â”‚ (terme historique)                    â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ Neurone / Unit   â”‚ Un Ã©lÃ©ment de la couche cachÃ©e        â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ PrÃ©-activation   â”‚ Valeur AVANT le ReLU : Î¸â‚€ + Î¸â‚x      â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ Activation       â”‚ Valeur APRÃˆS le ReLU : a[Î¸â‚€ + Î¸â‚x]   â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ Weights (poids)  â”‚ ParamÃ¨tres de pente (Î¸â‚â‚, Ï•â‚, â€¦)     â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ Biases (biais)   â”‚ ParamÃ¨tres d'offset (Î¸â‚â‚€, Ï•â‚€, â€¦)     â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ Feed-forward     â”‚ Graphe acyclique (pas de boucles)     â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ Fully connected  â”‚ Chaque neurone connectÃ© Ã  tous les    â•‘
  â•‘                  â”‚ neurones de la couche suivante        â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`,
        label: 'Fig. 3.12 â€” Terminologie des rÃ©seaux de neurones',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 11 : AUTRES ACTIVATIONS â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.7 â€” Autres fonctions d'activation\n\nLe ReLU n'est pas la seule option. Voici les alternatives les plus importantes :`,
      },
      {
        type: 'diagram',
        content: `  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Activation      â”‚ Formule                               â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Sigmoid Ïƒ(z)    â”‚  1 / (1 + eâ»á¶»)          âˆˆ (0, 1)     â”‚
  â”‚ Tanh            â”‚  (eá¶» - eâ»á¶»)/(eá¶» + eâ»á¶»)  âˆˆ (-1, 1)   â”‚
  â”‚ Leaky ReLU      â”‚  max(0.01z, z)                        â”‚
  â”‚ Parametric ReLU â”‚  max(Î±z, z)   Î± appris                â”‚
  â”‚ ELU             â”‚  z si zâ‰¥0, Î±(eá¶»-1) sinon             â”‚
  â”‚ Swish / SiLU    â”‚  z Â· Ïƒ(Î²z)    Î² appris                â”‚
  â”‚ GELU            â”‚  z Â· Î¦(z)     Î¦ = CDF gaussienne      â”‚
  â”‚ Softplus        â”‚  log(1 + eá¶»)  version lisse du ReLU   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`,
        label: 'Fig. 3.13 â€” Catalogue des fonctions d\'activation',
      },
      {
        type: 'text',
        content: `**En PyTorch**, chaque activation est disponible en module :\n\n- \`nn.ReLU()\`, \`nn.LeakyReLU(0.01)\`, \`nn.ELU(alpha=1.0)\`\n- \`nn.Sigmoid()\`, \`nn.Tanh()\`, \`nn.SiLU()\` (= Swish)\n- \`nn.GELU()\`, \`nn.Softplus()\`\n- \`nn.PReLU()\` â€” le paramÃ¨tre Î± est appris pendant l'entraÃ®nement`,
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 12 : RÃ‰SUMÃ‰ â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'callout',
        content: 'âš¡ **RÃ©sumÃ© du Chapitre 3** :\n(1) Les rÃ©seaux superficiels calculent des fonctions linÃ©aires par morceaux\n(2) Chaque unitÃ© cachÃ©e ajoute un "joint" et une rÃ©gion linÃ©aire\n(3) Avec assez d\'unitÃ©s, on approxime n\'importe quelle fonction continue\n(4) Le ReLU est l\'activation standard car son gradient est simple et stable\n(5) Le nombre de paramÃ¨tres est DÂ·(Dáµ¢+1) + Dâ‚’Â·(D+1)',
      },
    ],
    exercises: [
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 1 â€” THÃ‰ORIQUE : Activation linÃ©aire
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-th1',
        title: 'ğŸ§  ThÃ©orie â€” Activation linÃ©aire (Prob. 3.1)',
        instructions: 'ProblÃ¨me 3.1 du livre : que se passe-t-il si la fonction d\'activation est **linÃ©aire** a[z] = Ïˆâ‚€ + Ïˆâ‚z au lieu de ReLU ? Prouvez-le en code : crÃ©ez un rÃ©seau avec activation linÃ©aire et montrez que le rÃ©sultat est toujours une simple droite (fonction affine), quel que soit le nombre d\'unitÃ©s cachÃ©es.',
        starterCode: `import torch

# Activation linÃ©aire : a[z] = psi_0 + psi_1 * z
psi_0, psi_1 = 0.5, 2.0

def linear_activation(z):
    return psi_0 + psi_1 * z

x = torch.tensor(1.5)

# ParamÃ¨tres couche cachÃ©e
theta = torch.tensor([[0.5, -1.0],
                       [-0.3, 0.8],
                       [0.1, 1.2]])

# ParamÃ¨tres sortie
phi = torch.tensor([0.2, 0.5, -0.3, 0.7])

# Calculez h1, h2, h3 avec activation LINÃ‰AIRE
h1 = ___
h2 = ___
h3 = ___

# Sortie
y = ___

print(f"h1={h1.item():.4f}, h2={h2.item():.4f}, h3={h3.item():.4f}")
print(f"y = {y.item():.4f}")

# Maintenant montrez que y = A*x + B (constantes)
# Calculez A et B thÃ©oriquement
A = psi_1 * (phi[1]*theta[0,1] + phi[2]*theta[1,1] + phi[3]*theta[2,1])
B_offset = psi_0 * (phi[1] + phi[2] + phi[3])
B_theta  = psi_1 * (phi[1]*theta[0,0] + phi[2]*theta[1,0] + phi[3]*theta[2,0])
B = phi[0] + B_offset + B_theta

print(f"\\nVÃ©rification : y = {A:.4f} * x + {B:.4f}")
print(f"Calcul direct : {A * 1.5 + B:.4f}")
print(f"Conclusion : avec activation linÃ©aire, le rÃ©seau est juste une DROITE !")`,
        solution: `import torch

psi_0, psi_1 = 0.5, 2.0

def linear_activation(z):
    return psi_0 + psi_1 * z

x = torch.tensor(1.5)

theta = torch.tensor([[0.5, -1.0],
                       [-0.3, 0.8],
                       [0.1, 1.2]])

phi = torch.tensor([0.2, 0.5, -0.3, 0.7])

h1 = linear_activation(theta[0, 0] + theta[0, 1] * x)
h2 = linear_activation(theta[1, 0] + theta[1, 1] * x)
h3 = linear_activation(theta[2, 0] + theta[2, 1] * x)

y = phi[0] + phi[1] * h1 + phi[2] * h2 + phi[3] * h3

print(f"h1={h1.item():.4f}, h2={h2.item():.4f}, h3={h3.item():.4f}")
print(f"y = {y.item():.4f}")

A = psi_1 * (phi[1]*theta[0,1] + phi[2]*theta[1,1] + phi[3]*theta[2,1])
B_offset = psi_0 * (phi[1] + phi[2] + phi[3])
B_theta  = psi_1 * (phi[1]*theta[0,0] + phi[2]*theta[1,0] + phi[3]*theta[2,0])
B = phi[0] + B_offset + B_theta

print(f"\\nVÃ©rification : y = {A:.4f} * x + {B:.4f}")
print(f"Calcul direct : {A * 1.5 + B:.4f}")
print(f"Conclusion : avec activation linÃ©aire, le rÃ©seau est juste une DROITE !")`,
        hints: [
          'h1 = linear_activation(theta[0,0] + theta[0,1] * x)',
          'y = phi[0] + phi[1]*h1 + phi[2]*h2 + phi[3]*h3',
          'Le rÃ©sultat est toujours de la forme AÂ·x + B, donc une droite !',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 2 â€” PRATIQUE : Forward pass ReLU
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-pr1',
        title: 'ğŸ’» Pratique â€” Forward pass avec ReLU',
        instructions: 'ImplÃ©mentez un rÃ©seau superficiel avec 3 unitÃ©s cachÃ©es et activation ReLU. Calculez la sortie pour x = -1.0, 0.0, 0.5, 1.5 et identifiez les patterns d\'activation (quelles unitÃ©s sont actives/inactives Ã  chaque x).',
        starterCode: `import torch

def relu(z):
    """ReLU : max(0, z)"""
    return ___

# ParamÃ¨tres (tirÃ©s de la Figure 3.2a du livre)
theta = torch.tensor([[-0.2, 0.4],   # Î¸â‚â‚€, Î¸â‚â‚
                       [-0.9, 0.9],   # Î¸â‚‚â‚€, Î¸â‚‚â‚
                       [ 1.1, -0.7]]) # Î¸â‚ƒâ‚€, Î¸â‚ƒâ‚

phi = torch.tensor([-0.23, -1.3, 1.3, 0.66])  # Ï•â‚€, Ï•â‚, Ï•â‚‚, Ï•â‚ƒ

def shallow_forward(x, theta, phi):
    """Forward pass d'un rÃ©seau superficiel"""
    # PrÃ©-activations
    z1 = ___
    z2 = ___
    z3 = ___
    
    # Activations (unitÃ©s cachÃ©es)
    h1 = relu(z1)
    h2 = relu(z2)
    h3 = relu(z3)
    
    # Sortie
    y = ___
    
    # Pattern d'activation : 1 si actif, 0 si inactif
    pattern = (f"h1={'ON' if h1 > 0 else 'off'}, "
               f"h2={'ON' if h2 > 0 else 'off'}, "
               f"h3={'ON' if h3 > 0 else 'off'}")
    
    return y.item(), pattern

# Test sur plusieurs entrÃ©es
for x_val in [-1.0, 0.0, 0.5, 1.0, 1.5, 2.5]:
    x = torch.tensor(x_val)
    y, pattern = shallow_forward(x, theta, phi)
    print(f"x={x_val:+.1f} â†’ y={y:.4f}  [{pattern}]")`,
        solution: `import torch

def relu(z):
    """ReLU : max(0, z)"""
    return torch.clamp(z, min=0)

theta = torch.tensor([[-0.2, 0.4],
                       [-0.9, 0.9],
                       [ 1.1, -0.7]])

phi = torch.tensor([-0.23, -1.3, 1.3, 0.66])

def shallow_forward(x, theta, phi):
    z1 = theta[0, 0] + theta[0, 1] * x
    z2 = theta[1, 0] + theta[1, 1] * x
    z3 = theta[2, 0] + theta[2, 1] * x
    
    h1 = relu(z1)
    h2 = relu(z2)
    h3 = relu(z3)
    
    y = phi[0] + phi[1] * h1 + phi[2] * h2 + phi[3] * h3
    
    pattern = (f"h1={'ON' if h1 > 0 else 'off'}, "
               f"h2={'ON' if h2 > 0 else 'off'}, "
               f"h3={'ON' if h3 > 0 else 'off'}")
    
    return y.item(), pattern

for x_val in [-1.0, 0.0, 0.5, 1.0, 1.5, 2.5]:
    x = torch.tensor(x_val)
    y, pattern = shallow_forward(x, theta, phi)
    print(f"x={x_val:+.1f} â†’ y={y:.4f}  [{pattern}]")`,
        hints: [
          'relu(z) = torch.clamp(z, min=0)',
          'z1 = theta[0, 0] + theta[0, 1] * x  (prÃ©-activation)',
          'y = phi[0] + phi[1]*h1 + phi[2]*h2 + phi[3]*h3',
          'Un neurone est ON si sa prÃ©-activation z > 0',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 3 â€” THÃ‰ORIQUE : HomogÃ©nÃ©itÃ© du ReLU
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-th2',
        title: 'ğŸ§  ThÃ©orie â€” PropriÃ©tÃ© du ReLU (Prob. 3.5)',
        instructions: 'ProblÃ¨me 3.5 : prouvez numÃ©riquement que ReLU(Î±Â·z) = Î±Â·ReLU(z) pour tout Î± â‰¥ 0 (propriÃ©tÃ© d\'homogÃ©nÃ©itÃ© non-nÃ©gative). Puis montrez que cette propriÃ©tÃ© NE tient PAS pour Î± < 0. Testez avec diffÃ©rentes valeurs de z et Î±.',
        starterCode: `import torch

def relu(z):
    return torch.clamp(z, min=0)

# Testez la propriÃ©tÃ© : ReLU(Î±Â·z) == Î±Â·ReLU(z) pour Î± â‰¥ 0
z_values = torch.tensor([-2.0, -1.0, 0.0, 1.0, 3.0])

print("â•â•â• Î± â‰¥ 0 : propriÃ©tÃ© VRAIE â•â•â•")
for alpha in [0.0, 0.5, 1.0, 2.0, 10.0]:
    lhs = relu(alpha * z_values)   # ReLU(Î±Â·z)
    rhs = alpha * relu(z_values)   # Î±Â·ReLU(z)
    equal = torch.allclose(lhs, rhs)
    print(f"  Î±={alpha:4.1f} : ReLU(Î±z) = {lhs.tolist()}")
    print(f"          Î±Â·ReLU(z) = {rhs.tolist()} â†’ {'âœ“ Ã‰GAL' if equal else 'âœ— DIFFÃ‰RENT'}")

print("\\nâ•â•â• Î± < 0 : propriÃ©tÃ© FAUSSE â•â•â•")
alpha = -1.0
lhs = relu(alpha * z_values)
rhs = alpha * relu(z_values)
print(f"  Î±={alpha:4.1f} : ReLU(Î±z) = {lhs.tolist()}")
print(f"          Î±Â·ReLU(z) = {rhs.tolist()}")
print(f"  â†’ {'âœ“ Ã‰GAL' if torch.allclose(lhs, rhs) else 'âœ— DIFFÃ‰RENT'}")

# Question bonus : pourquoi c'est important ?
print("\\nğŸ’¡ ConsÃ©quence (Prob 3.6) :")
print("   Si on multiplie Î¸â‚â‚€,Î¸â‚â‚ par Î±>0 et divise Ï•â‚ par Î±,")
print("   le rÃ©seau donne EXACTEMENT la mÃªme sortie.")
print("   â†’ Il y a une infinitÃ© de combinaisons de paramÃ¨tres Ã©quivalentes !")`,
        solution: `import torch

def relu(z):
    return torch.clamp(z, min=0)

z_values = torch.tensor([-2.0, -1.0, 0.0, 1.0, 3.0])

print("â•â•â• Î± â‰¥ 0 : propriÃ©tÃ© VRAIE â•â•â•")
for alpha in [0.0, 0.5, 1.0, 2.0, 10.0]:
    lhs = relu(alpha * z_values)
    rhs = alpha * relu(z_values)
    equal = torch.allclose(lhs, rhs)
    print(f"  Î±={alpha:4.1f} : ReLU(Î±z) = {lhs.tolist()}")
    print(f"          Î±Â·ReLU(z) = {rhs.tolist()} â†’ {'âœ“ Ã‰GAL' if equal else 'âœ— DIFFÃ‰RENT'}")

print("\\nâ•â•â• Î± < 0 : propriÃ©tÃ© FAUSSE â•â•â•")
alpha = -1.0
lhs = relu(alpha * z_values)
rhs = alpha * relu(z_values)
print(f"  Î±={alpha:4.1f} : ReLU(Î±z) = {lhs.tolist()}")
print(f"          Î±Â·ReLU(z) = {rhs.tolist()}")
print(f"  â†’ {'âœ“ Ã‰GAL' if torch.allclose(lhs, rhs) else 'âœ— DIFFÃ‰RENT'}")

print("\\nğŸ’¡ ConsÃ©quence (Prob 3.6) :")
print("   Si on multiplie Î¸â‚â‚€,Î¸â‚â‚ par Î±>0 et divise Ï•â‚ par Î±,")
print("   le rÃ©seau donne EXACTEMENT la mÃªme sortie.")
print("   â†’ Il y a une infinitÃ© de combinaisons de paramÃ¨tres Ã©quivalentes !")`,
        hints: [
          'Pour Î± â‰¥ 0 et z â‰¥ 0 : ReLU(Î±Â·z) = Î±Â·z = Î±Â·ReLU(z) âœ“',
          'Pour Î± â‰¥ 0 et z < 0 : ReLU(Î±Â·z) = 0 = Î±Â·0 = Î±Â·ReLU(z) âœ“',
          'Pour Î± < 0 et z > 0 : ReLU(Î±Â·z) = 0 â‰  Î±Â·z = Î±Â·ReLU(z) âœ—',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 4 â€” PRATIQUE : nn.Sequential
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-pr2',
        title: 'ğŸ’» Pratique â€” RÃ©seau PyTorch nn.Sequential',
        instructions: 'Construisez un rÃ©seau superficiel avec `nn.Sequential` : 1 entrÃ©e, D=20 unitÃ©s cachÃ©es, 1 sortie. Comptez les paramÃ¨tres, puis passez un batch de 50 entrÃ©es Ã  travers le rÃ©seau.',
        starterCode: `import torch
import torch.nn as nn

torch.manual_seed(42)

# Construisez le rÃ©seau superficiel
D = 20  # unitÃ©s cachÃ©es
model = nn.Sequential(
    ___,  # couche 1 : entrÃ©e â†’ cachÃ©e
    ___,  # activation ReLU
    ___,  # couche 2 : cachÃ©e â†’ sortie
)

# Comptez les paramÃ¨tres
n_params = ___
print(f"Architecture: {model}")
print(f"ParamÃ¨tres: {n_params}")

# VÃ©rification thÃ©orique
D_i, D_o = 1, 1
n_theorique = D * (D_i + 1) + D_o * (D + 1)
print(f"Formule: {D}Ã—({D_i}+1) + {D_o}Ã—({D}+1) = {n_theorique}")

# Passez un batch de 50 entrÃ©es
x = torch.linspace(-3, 3, 50).unsqueeze(1)  # (50, 1)
y = model(x)  # forward pass

print(f"\\nInput shape:  {x.shape}")
print(f"Output shape: {y.shape}")
print(f"PremiÃ¨res sorties: {y[:5].squeeze().tolist()}")

# Inspectez les poids de la couche cachÃ©e
W1 = model[0].weight  # matrice de poids
b1 = model[0].bias    # vecteur de biais
print(f"\\nPoids couche cachÃ©e: {W1.shape} â†’ {W1.numel()} poids")
print(f"Biais couche cachÃ©e: {b1.shape} â†’ {b1.numel()} biais")`,
        solution: `import torch
import torch.nn as nn

torch.manual_seed(42)

D = 20
model = nn.Sequential(
    nn.Linear(1, D),   # couche 1 : entrÃ©e â†’ cachÃ©e
    nn.ReLU(),          # activation ReLU
    nn.Linear(D, 1),   # couche 2 : cachÃ©e â†’ sortie
)

n_params = sum(p.numel() for p in model.parameters())
print(f"Architecture: {model}")
print(f"ParamÃ¨tres: {n_params}")

D_i, D_o = 1, 1
n_theorique = D * (D_i + 1) + D_o * (D + 1)
print(f"Formule: {D}Ã—({D_i}+1) + {D_o}Ã—({D}+1) = {n_theorique}")

x = torch.linspace(-3, 3, 50).unsqueeze(1)
y = model(x)

print(f"\\nInput shape:  {x.shape}")
print(f"Output shape: {y.shape}")
print(f"PremiÃ¨res sorties: {y[:5].squeeze().tolist()}")

W1 = model[0].weight
b1 = model[0].bias
print(f"\\nPoids couche cachÃ©e: {W1.shape} â†’ {W1.numel()} poids")
print(f"Biais couche cachÃ©e: {b1.shape} â†’ {b1.numel()} biais")`,
        hints: [
          'nn.Linear(1, D) pour la couche d\'entrÃ©e vers cachÃ©e',
          'nn.ReLU() comme activation',
          'nn.Linear(D, 1) pour la couche cachÃ©e vers sortie',
          'sum(p.numel() for p in model.parameters()) pour le total',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 5 â€” THÃ‰ORIQUE : Compter les rÃ©gions
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-th3',
        title: 'ğŸ§  ThÃ©orie â€” Compter les rÃ©gions linÃ©aires (Prob. 3.18)',
        instructions: 'ImplÃ©mentez la formule de Zaslavsky pour calculer le nombre maximum de rÃ©gions linÃ©aires. VÃ©rifiez que D=3 en 2D donne 7 rÃ©gions (comme Fig. 3.8j). Explorez comment le nombre de rÃ©gions explose en haute dimension.',
        starterCode: `import math

def binomial(n, k):
    """Coefficient binomial C(n, k)"""
    if k > n or k < 0:
        return 0
    return math.comb(n, k)

def max_regions(D, D_i):
    """
    Nombre max de rÃ©gions d'un shallow network
    D : nombre d'unitÃ©s cachÃ©es
    D_i : dimension de l'entrÃ©e
    Formule de Zaslavsky (1975)
    """
    total = ___  # Î£ C(D, j) pour j = 0 Ã  min(D, D_i)
    return total

# VÃ©rifications du livre
print("â•â•â• VÃ©rifications â•â•â•")
print(f"D=3, D_i=1 : {max_regions(3, 1)} rÃ©gions (attendu: 4)")
print(f"D=3, D_i=2 : {max_regions(3, 2)} rÃ©gions (attendu: 7)")
print(f"D=5, D_i=2 : {max_regions(5, 2)} rÃ©gions (attendu: 16)")

# Exploration
print("\\nâ•â•â• Explosion combinatoire â•â•â•")
for D_i in [1, 2, 5, 10, 50, 100]:
    D = max(D_i, 10)
    r = max_regions(D, D_i)
    print(f"D_i={D_i:3d}, D={D:3d} â†’ {r:.2e} rÃ©gions max")

# Cas massif du livre
D, D_i = 500, 100
r = max_regions(D, D_i)
print(f"\\nD=500, D_i=100 â†’ ~10^{math.log10(r):.0f} rÃ©gions !")`,
        solution: `import math

def binomial(n, k):
    if k > n or k < 0:
        return 0
    return math.comb(n, k)

def max_regions(D, D_i):
    total = sum(binomial(D, j) for j in range(min(D, D_i) + 1))
    return total

print("â•â•â• VÃ©rifications â•â•â•")
print(f"D=3, D_i=1 : {max_regions(3, 1)} rÃ©gions (attendu: 4)")
print(f"D=3, D_i=2 : {max_regions(3, 2)} rÃ©gions (attendu: 7)")
print(f"D=5, D_i=2 : {max_regions(5, 2)} rÃ©gions (attendu: 16)")

print("\\nâ•â•â• Explosion combinatoire â•â•â•")
for D_i in [1, 2, 5, 10, 50, 100]:
    D = max(D_i, 10)
    r = max_regions(D, D_i)
    print(f"D_i={D_i:3d}, D={D:3d} â†’ {r:.2e} rÃ©gions max")

D, D_i = 500, 100
r = max_regions(D, D_i)
print(f"\\nD=500, D_i=100 â†’ ~10^{math.log10(r):.0f} rÃ©gions !")`,
        hints: [
          'La formule est : Î£ C(D, j) pour j de 0 Ã  min(D, D_i)',
          'sum(binomial(D, j) for j in range(min(D, D_i) + 1))',
          'Pour D_i=1 : C(D,0) + C(D,1) = 1 + D = D+1 âœ“',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 6 â€” PRATIQUE : Entrainer un shallow network
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-pr3',
        title: 'ğŸ’» Pratique â€” EntraÃ®ner un shallow network',
        instructions: 'EntraÃ®nez un rÃ©seau superficiel pour approximer la fonction sin(x) sur [-Ï€, Ï€]. Observez comment le nombre d\'unitÃ©s cachÃ©es D affecte la qualitÃ© de l\'approximation.',
        starterCode: `import torch
import torch.nn as nn
import torch.optim as optim
import math

torch.manual_seed(42)

# DonnÃ©es : y = sin(x) sur [-Ï€, Ï€]
x_train = torch.linspace(-math.pi, math.pi, 200).unsqueeze(1)
y_train = torch.sin(x_train)

def train_shallow(D, n_epochs=2000, lr=0.01):
    """EntraÃ®ne un rÃ©seau superficiel avec D unitÃ©s cachÃ©es"""
    model = nn.Sequential(
        ___,   # entrÃ©e â†’ D unitÃ©s
        ___,   # ReLU
        ___,   # D unitÃ©s â†’ sortie
    )
    
    optimizer = optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()
    
    for epoch in range(n_epochs):
        pred = model(x_train)
        loss = loss_fn(pred, y_train)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    final_loss = loss_fn(model(x_train), y_train).item()
    return model, final_loss

# Comparer diffÃ©rentes capacitÃ©s
print("D (units)  â”‚  Params  â”‚  MSE Loss")
print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
for D in [3, 5, 10, 20, 50]:
    model, loss = train_shallow(D)
    n_params = sum(p.numel() for p in model.parameters())
    print(f"  D={D:3d}     â”‚  {n_params:5d}   â”‚  {loss:.6f}")

print("\\nâ†’ Plus de hidden units = meilleure approximation de sin(x)")
print("  Cela illustre le thÃ©orÃ¨me d'approximation universelle !")`,
        solution: `import torch
import torch.nn as nn
import torch.optim as optim
import math

torch.manual_seed(42)

x_train = torch.linspace(-math.pi, math.pi, 200).unsqueeze(1)
y_train = torch.sin(x_train)

def train_shallow(D, n_epochs=2000, lr=0.01):
    model = nn.Sequential(
        nn.Linear(1, D),
        nn.ReLU(),
        nn.Linear(D, 1),
    )
    
    optimizer = optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()
    
    for epoch in range(n_epochs):
        pred = model(x_train)
        loss = loss_fn(pred, y_train)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    final_loss = loss_fn(model(x_train), y_train).item()
    return model, final_loss

print("D (units)  â”‚  Params  â”‚  MSE Loss")
print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
for D in [3, 5, 10, 20, 50]:
    model, loss = train_shallow(D)
    n_params = sum(p.numel() for p in model.parameters())
    print(f"  D={D:3d}     â”‚  {n_params:5d}   â”‚  {loss:.6f}")

print("\\nâ†’ Plus de hidden units = meilleure approximation de sin(x)")
print("  Cela illustre le thÃ©orÃ¨me d'approximation universelle !")`,
        hints: [
          'nn.Linear(1, D) pour la couche d\'entrÃ©e',
          'nn.ReLU() pour l\'activation',
          'nn.Linear(D, 1) pour la couche de sortie',
          'Adam converge plus vite que SGD pour ce problÃ¨me',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 7 â€” THÃ‰ORIQUE : Pentes des rÃ©gions
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-th4',
        title: 'ğŸ§  ThÃ©orie â€” Pentes des rÃ©gions lin. (Prob. 3.3)',
        instructions: 'ProblÃ¨me 3.3 : calculez les positions des joints et les pentes de chaque rÃ©gion linÃ©aire. Montrez que la somme d\'une pente intermÃ©diaire est la somme des pentes des unitÃ©s actives dans cette rÃ©gion.',
        starterCode: `import torch

# ParamÃ¨tres de la Figure 3.2a du livre
theta = torch.tensor([[-0.2, 0.4],   # Î¸â‚â‚€, Î¸â‚â‚
                       [-0.9, 0.9],   # Î¸â‚‚â‚€, Î¸â‚‚â‚
                       [ 1.1, -0.7]]) # Î¸â‚ƒâ‚€, Î¸â‚ƒâ‚

phi = torch.tensor([-0.23, -1.3, 1.3, 0.66])

# â”€â”€ Positions des joints â”€â”€
# Un joint est lÃ  oÃ¹ Î¸_{d0} + Î¸_{d1}Â·x = 0
# Donc x_joint = -Î¸_{d0} / Î¸_{d1}
joint1 = ___  # = -theta[0,0] / theta[0,1]
joint2 = ___
joint3 = ___

print("â•â•â• Positions des joints â•â•â•")
print(f"Joint 1 (hâ‚ s'active) : x = {joint1:.4f}")
print(f"Joint 2 (hâ‚‚ s'active) : x = {joint2:.4f}")
print(f"Joint 3 (hâ‚ƒ s'active) : x = {joint3:.4f}")

# Triez les joints pour identifier les rÃ©gions
joints = sorted([(joint1.item(), 1), (joint2.item(), 2), (joint3.item(), 3)])
print(f"\\nJoints triÃ©s: {[(f'x={j:.2f}', f'h{i}') for j, i in joints]}")

# â”€â”€ Pentes des rÃ©gions â”€â”€
# La pente d'une rÃ©gion = Î£ (Ï•_d Â· Î¸_{d1}) pour chaque unitÃ© d ACTIVE
print("\\nâ•â•â• Pentes des rÃ©gions â•â•â•")
print(f"Ï•â‚Â·Î¸â‚â‚ = {phi[1]*theta[0,1]:.4f}")
print(f"Ï•â‚‚Â·Î¸â‚‚â‚ = {phi[2]*theta[1,1]:.4f}")
print(f"Ï•â‚ƒÂ·Î¸â‚ƒâ‚ = {phi[3]*theta[2,1]:.4f}")

# Identifiez quelles unitÃ©s sont actives dans chaque rÃ©gion
# et calculez la pente correspondante`,
        solution: `import torch

theta = torch.tensor([[-0.2, 0.4],
                       [-0.9, 0.9],
                       [ 1.1, -0.7]])

phi = torch.tensor([-0.23, -1.3, 1.3, 0.66])

joint1 = -theta[0, 0] / theta[0, 1]
joint2 = -theta[1, 0] / theta[1, 1]
joint3 = -theta[2, 0] / theta[2, 1]

print("â•â•â• Positions des joints â•â•â•")
print(f"Joint 1 (hâ‚ s'active) : x = {joint1:.4f}")
print(f"Joint 2 (hâ‚‚ s'active) : x = {joint2:.4f}")
print(f"Joint 3 (hâ‚ƒ s'active) : x = {joint3:.4f}")

joints = sorted([(joint1.item(), 1), (joint2.item(), 2), (joint3.item(), 3)])
print(f"\\nJoints triÃ©s: {[(f'x={j:.2f}', f'h{i}') for j, i in joints]}")

print("\\nâ•â•â• Pentes des rÃ©gions â•â•â•")
print(f"Ï•â‚Â·Î¸â‚â‚ = {phi[1]*theta[0,1]:.4f}")
print(f"Ï•â‚‚Â·Î¸â‚‚â‚ = {phi[2]*theta[1,1]:.4f}")
print(f"Ï•â‚ƒÂ·Î¸â‚ƒâ‚ = {phi[3]*theta[2,1]:.4f}")`,
        hints: [
          'Le joint du neurone d est Ã  x = -Î¸_{d0} / Î¸_{d1}',
          'La pente dans une rÃ©gion = somme de Ï•_d Â· Î¸_{d1} pour les neurones actifs',
          'joint1 = -theta[0, 0] / theta[0, 1]',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 8 â€” PRATIQUE : Comparer les activations
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-pr4',
        title: 'ğŸ’» Pratique â€” Comparer ReLU, Sigmoid, Tanh, GELU',
        instructions: 'CrÃ©ez 4 rÃ©seaux superficiels identiques (D=10) mais avec des activations diffÃ©rentes : ReLU, Sigmoid, Tanh, GELU. EntraÃ®nez-les sur y=sin(x) et comparez les pertes finales.',
        starterCode: `import torch
import torch.nn as nn
import torch.optim as optim
import math

torch.manual_seed(0)

# DonnÃ©es
x = torch.linspace(-math.pi, math.pi, 200).unsqueeze(1)
y = torch.sin(x)

D = 10  # unitÃ©s cachÃ©es

# Dictionnaire des activations Ã  tester
activations = {
    'ReLU':    ___,
    'Sigmoid': ___,
    'Tanh':    ___,
    'GELU':    ___,
}

results = {}

for name, act_fn in activations.items():
    torch.manual_seed(0)  # mÃªme init pour comparaison juste
    
    model = nn.Sequential(
        nn.Linear(1, D),
        act_fn,
        nn.Linear(D, 1)
    )
    
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    loss_fn = nn.MSELoss()
    
    for epoch in range(1000):
        pred = model(x)
        loss = loss_fn(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    final_loss = loss.item()
    results[name] = final_loss

# Affichage
print("â•â•â• Comparaison des activations (D=10, 1000 epochs) â•â•â•")
print(f"{'Activation':<12} â”‚ {'MSE Loss':>10}")
print(f"{'â”€'*12}â”€â”¼â”€{'â”€'*10}")
for name, loss in sorted(results.items(), key=lambda x: x[1]):
    bar = 'â–ˆ' * int(min(50, 50 * (1 - loss/max(results.values()))))
    print(f"{name:<12} â”‚ {loss:10.6f}  {bar}")`,
        solution: `import torch
import torch.nn as nn
import torch.optim as optim
import math

torch.manual_seed(0)

x = torch.linspace(-math.pi, math.pi, 200).unsqueeze(1)
y = torch.sin(x)

D = 10

activations = {
    'ReLU':    nn.ReLU(),
    'Sigmoid': nn.Sigmoid(),
    'Tanh':    nn.Tanh(),
    'GELU':    nn.GELU(),
}

results = {}

for name, act_fn in activations.items():
    torch.manual_seed(0)
    
    model = nn.Sequential(
        nn.Linear(1, D),
        act_fn,
        nn.Linear(D, 1)
    )
    
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    loss_fn = nn.MSELoss()
    
    for epoch in range(1000):
        pred = model(x)
        loss = loss_fn(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    final_loss = loss.item()
    results[name] = final_loss

print("â•â•â• Comparaison des activations (D=10, 1000 epochs) â•â•â•")
print(f"{'Activation':<12} â”‚ {'MSE Loss':>10}")
print(f"{'â”€'*12}â”€â”¼â”€{'â”€'*10}")
for name, loss in sorted(results.items(), key=lambda x: x[1]):
    bar = 'â–ˆ' * int(min(50, 50 * (1 - loss/max(results.values()))))
    print(f"{name:<12} â”‚ {loss:10.6f}  {bar}")`,
        hints: [
          'nn.ReLU(), nn.Sigmoid(), nn.Tanh(), nn.GELU()',
          'Utilisez torch.manual_seed(0) avant chaque modÃ¨le pour la reproductibilitÃ©',
          'GELU et Tanh tendent Ã  mieux approximer les fonctions lisses que ReLU',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 9 â€” PRATIQUE : nn.Module personnalisÃ©
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-pr5',
        title: 'ğŸ’» Pratique â€” nn.Module personnalisÃ©',
        instructions: 'ImplÃ©mentez un rÃ©seau superficiel en tant que classe `nn.Module` personnalisÃ©e (pas `nn.Sequential`). Ajoutez une mÃ©thode `count_params()` et une mÃ©thode `get_activation_pattern(x)` qui retourne quelles unitÃ©s sont actives.',
        starterCode: `import torch
import torch.nn as nn

class ShallowNetwork(nn.Module):
    def __init__(self, D_i, D, D_o):
        super().__init__()
        self.hidden = ___     # nn.Linear(D_i, D)
        self.output = ___     # nn.Linear(D, D_o)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        """Forward pass : x â†’ ReLU(Wx+b) â†’ sortie"""
        h = ___   # couche cachÃ©e + activation
        y = ___   # couche de sortie
        return y
    
    def count_params(self):
        """Retourne le nombre total de paramÃ¨tres"""
        return ___
    
    def get_activation_pattern(self, x):
        """Retourne un tenseur binaire (1=actif, 0=inactif)"""
        pre_act = self.hidden(x)       # prÃ©-activations
        pattern = (pre_act > 0).int()  # 1 si actif, 0 sinon
        return pattern

# CrÃ©ez et testez le rÃ©seau
model = ShallowNetwork(D_i=2, D=5, D_o=1)

print(f"Architecture: {model}")
print(f"ParamÃ¨tres: {model.count_params()}")

# Test
x = torch.tensor([[1.0, -0.5]])
y = model(x)
pattern = model.get_activation_pattern(x)

print(f"\\nEntrÃ©e:  {x.tolist()}")
print(f"Sortie:  {y.item():.4f}")
print(f"Pattern: {pattern.tolist()[0]} (1=ON, 0=off)")
print(f"UnitÃ©s actives: {pattern.sum().item()}/{model.hidden.out_features}")`,
        solution: `import torch
import torch.nn as nn

class ShallowNetwork(nn.Module):
    def __init__(self, D_i, D, D_o):
        super().__init__()
        self.hidden = nn.Linear(D_i, D)
        self.output = nn.Linear(D, D_o)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        h = self.relu(self.hidden(x))
        y = self.output(h)
        return y
    
    def count_params(self):
        return sum(p.numel() for p in self.parameters())
    
    def get_activation_pattern(self, x):
        pre_act = self.hidden(x)
        pattern = (pre_act > 0).int()
        return pattern

model = ShallowNetwork(D_i=2, D=5, D_o=1)

print(f"Architecture: {model}")
print(f"ParamÃ¨tres: {model.count_params()}")

x = torch.tensor([[1.0, -0.5]])
y = model(x)
pattern = model.get_activation_pattern(x)

print(f"\\nEntrÃ©e:  {x.tolist()}")
print(f"Sortie:  {y.item():.4f}")
print(f"Pattern: {pattern.tolist()[0]} (1=ON, 0=off)")
print(f"UnitÃ©s actives: {pattern.sum().item()}/{model.hidden.out_features}")`,
        hints: [
          'self.hidden = nn.Linear(D_i, D)',
          'h = self.relu(self.hidden(x)) â€” appliquer ReLU aux prÃ©-activations',
          'count_params: sum(p.numel() for p in self.parameters())',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 10 â€” THÃ‰ORIQUE : UnicitÃ© de la solution
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-th5',
        title: 'ğŸ§  ThÃ©orie â€” UnicitÃ© de la solution (Prob. 3.7)',
        instructions: 'ProblÃ¨me 3.7 : la perte des moindres carrÃ©s a-t-elle un minimum unique ? Montrez qu\'il existe une infinitÃ© de combinaisons de paramÃ¨tres qui donnent exactement la mÃªme fonction (mÃªme perte). DÃ©montrez-le en construisant 2 rÃ©seaux avec des paramÃ¨tres diffÃ©rents mais la mÃªme sortie.',
        starterCode: `import torch

def relu(z):
    return torch.clamp(z, min=0)

def shallow_net(x, theta, phi):
    h1 = relu(theta[0,0] + theta[0,1] * x)
    h2 = relu(theta[1,0] + theta[1,1] * x)
    h3 = relu(theta[2,0] + theta[2,1] * x)
    return phi[0] + phi[1]*h1 + phi[2]*h2 + phi[3]*h3

# RÃ©seau A â€” paramÃ¨tres originaux
theta_A = torch.tensor([[-0.2, 0.4], [-0.9, 0.9], [1.1, -0.7]])
phi_A   = torch.tensor([-0.23, -1.3, 1.3, 0.66])

# RÃ©seau B â€” MÃŠMES rÃ©sultats mais paramÃ¨tres DIFFÃ‰RENTS
# Astuce 1 : multiplier Î¸â‚ par Î± et diviser Ï•â‚ par Î± (Prob. 3.6)
alpha = 2.0
theta_B = theta_A.clone()
theta_B[0] = theta_A[0] * alpha  # multiplie Î¸â‚â‚€, Î¸â‚â‚ par Î±
phi_B = phi_A.clone()
phi_B[1] = phi_A[1] / alpha      # divise Ï•â‚ par Î±

# Astuce 2 : permuter les unitÃ©s cachÃ©es (hâ‚ â†” hâ‚‚)
theta_C = torch.tensor([[-0.9, 0.9], [-0.2, 0.4], [1.1, -0.7]])  # permutation
phi_C   = torch.tensor([-0.23, 1.3, -1.3, 0.66])  # Ï•â‚ â†” Ï•â‚‚ Ã©changÃ©s

# VÃ©rification
x_test = torch.linspace(-2, 3, 10)
print("   x    â”‚   Net A   â”‚   Net B   â”‚   Net C   â”‚  B=A?  C=A?")
print("â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
for x in x_test:
    yA = shallow_net(x, theta_A, phi_A).item()
    yB = shallow_net(x, theta_B, phi_B).item()
    yC = shallow_net(x, theta_C, phi_C).item()
    eq_B = "âœ“" if abs(yA - yB) < 1e-5 else "âœ—"
    eq_C = "âœ“" if abs(yA - yC) < 1e-5 else "âœ—"
    print(f"  {x:+.2f}  â”‚  {yA:+.4f}  â”‚  {yB:+.4f}  â”‚  {yC:+.4f}  â”‚  {eq_B}     {eq_C}")

print("\\nğŸ’¡ Conclusion : le minimum de la perte N'EST PAS unique !")
print("   â†’ Il existe des symÃ©tries : scaling (Ã—Î±) et permutation.")
print("   â†’ Le paysage de perte a une infinitÃ© de minima globaux Ã©quivalents.")`,
        solution: `import torch

def relu(z):
    return torch.clamp(z, min=0)

def shallow_net(x, theta, phi):
    h1 = relu(theta[0,0] + theta[0,1] * x)
    h2 = relu(theta[1,0] + theta[1,1] * x)
    h3 = relu(theta[2,0] + theta[2,1] * x)
    return phi[0] + phi[1]*h1 + phi[2]*h2 + phi[3]*h3

theta_A = torch.tensor([[-0.2, 0.4], [-0.9, 0.9], [1.1, -0.7]])
phi_A   = torch.tensor([-0.23, -1.3, 1.3, 0.66])

alpha = 2.0
theta_B = theta_A.clone()
theta_B[0] = theta_A[0] * alpha
phi_B = phi_A.clone()
phi_B[1] = phi_A[1] / alpha

theta_C = torch.tensor([[-0.9, 0.9], [-0.2, 0.4], [1.1, -0.7]])
phi_C   = torch.tensor([-0.23, 1.3, -1.3, 0.66])

x_test = torch.linspace(-2, 3, 10)
print("   x    â”‚   Net A   â”‚   Net B   â”‚   Net C   â”‚  B=A?  C=A?")
print("â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
for x in x_test:
    yA = shallow_net(x, theta_A, phi_A).item()
    yB = shallow_net(x, theta_B, phi_B).item()
    yC = shallow_net(x, theta_C, phi_C).item()
    eq_B = "âœ“" if abs(yA - yB) < 1e-5 else "âœ—"
    eq_C = "âœ“" if abs(yA - yC) < 1e-5 else "âœ—"
    print(f"  {x:+.2f}  â”‚  {yA:+.4f}  â”‚  {yB:+.4f}  â”‚  {yC:+.4f}  â”‚  {eq_B}     {eq_C}")

print("\\nğŸ’¡ Conclusion : le minimum de la perte N'EST PAS unique !")
print("   â†’ Il existe des symÃ©tries : scaling (Ã—Î±) et permutation.")
print("   â†’ Le paysage de perte a une infinitÃ© de minima globaux Ã©quivalents.")`,
        hints: [
          'Par homogÃ©nÃ©itÃ© du ReLU : ReLU(Î±Â·z) = Î±Â·ReLU(z) pour Î± > 0',
          'Donc (Î±Â·Î¸) passÃ© par ReLU puis (Ï•/Î±) = mÃªme rÃ©sultat',
          'Permuter les unitÃ©s cachÃ©es revient Ã  rÃ©arranger les indices',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RÃ©seaux Superficiels â€” Ch. 3 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. ImplÃ©mentation manuelle (Ã‰q. 3.1) â”€â”€
def shallow_network_manual(x, theta, phi):
    """
    RÃ©seau superficiel : y = Ï•â‚€ + Î£ Ï•d Â· ReLU(Î¸dâ‚€ + Î¸dâ‚Â·x)
    
    x:     entrÃ©e scalaire (tensor)
    theta: paramÃ¨tres couche cachÃ©e â€” shape (D, 2)
           theta[d, 0] = biais,  theta[d, 1] = pente
    phi:   paramÃ¨tres de sortie â€” shape (D+1,)
           phi[0] = offset, phi[1:] = poids de recombinaison
    """
    D = theta.shape[0]
    
    # PrÃ©-activations : z_d = Î¸_{d0} + Î¸_{d1}Â·x
    z = theta[:, 0] + theta[:, 1] * x
    
    # Activations : h_d = ReLU(z_d)
    h = torch.relu(z)
    
    # Sortie : y = Ï•â‚€ + Î£ Ï•_d Â· h_d
    y = phi[0] + torch.sum(phi[1:] * h)
    
    return y, z, h

# ParamÃ¨tres du livre (Figure 3.2a)
theta = torch.tensor([[-0.2, 0.4],
                       [-0.9, 0.9],
                       [ 1.1, -0.7]])
phi = torch.tensor([-0.23, -1.3, 1.3, 0.66])

print("â”€â”€ Forward pass manuel â”€â”€")
for x_val in [-1.5, 0.0, 0.5, 1.0, 2.0]:
    x = torch.tensor(x_val)
    y, z, h = shallow_network_manual(x, theta, phi)
    active = ["ON" if hi > 0 else "off" for hi in h]
    print(f"x={x_val:+.1f} â†’ y={y:.4f}  pattern=[{', '.join(active)}]")

# â”€â”€ 2. MÃªme rÃ©seau avec PyTorch nn.Module â”€â”€
print("\\nâ”€â”€ RÃ©seau PyTorch nn.Sequential â”€â”€")

# Initialiser avec les MÃŠMES paramÃ¨tres
model = nn.Sequential(
    nn.Linear(1, 3),  # couche cachÃ©e
    nn.ReLU(),
    nn.Linear(3, 1),  # couche de sortie
)

# Copier nos paramÃ¨tres dans le modÃ¨le PyTorch
with torch.no_grad():
    model[0].weight.copy_(theta[:, 1:])  # pentes
    model[0].bias.copy_(theta[:, 0])     # biais
    model[2].weight.copy_(phi[1:].unsqueeze(0))
    model[2].bias.copy_(phi[:1])

for x_val in [-1.5, 0.0, 0.5, 1.0, 2.0]:
    x = torch.tensor([[x_val]])
    y = model(x)
    print(f"x={x_val:+.1f} â†’ y={y.item():.4f}")

# â”€â”€ 3. Comptage de paramÃ¨tres â”€â”€
print(f"\\nâ”€â”€ ParamÃ¨tres â”€â”€")
for name, param in model.named_parameters():
    print(f"  {name}: {param.shape} = {param.numel()} params")
print(f"  Total: {sum(p.numel() for p in model.parameters())}")
print(f"  Formule: D(D_i+1) + D_o(D+1) = 3(1+1) + 1(3+1) = 10 âœ“")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 4 â€” RÃ‰SEAUX PROFONDS (Ch. 4)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'deep-networks',
    title: 'RÃ©seaux de Neurones Profonds',
    shortTitle: 'Deep Nets',
    description: 'Composition de rÃ©seaux, profondeur vs largeur, rÃ©gions linÃ©aires exponentielles et notation matricielle (Ch. 4 â€” UDL).',
    status: 'locked',
    progress: 0,
    dependencies: ['shallow-networks'],
    category: 'fundamentals',
    theory: [
      {
        type: 'text',
        content: `## 4.1 â€” Composition de rÃ©seaux\n\nUn **rÃ©seau profond** est obtenu en **composant** des rÃ©seaux superficiels : la sortie du premier devient l'entrÃ©e du second. Le premier rÃ©seau "plie" (**fold**) l'espace d'entrÃ©e : plusieurs valeurs de x sont mappÃ©es sur la mÃªme valeur y. Le second rÃ©seau applique sa fonction, qui est alors **dupliquÃ©e** Ã  chaque pli.`,
      },
      {
        type: 'diagram',
        content: `  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  RÃ©seau 1    â”‚     â”‚  RÃ©seau 2    â”‚
  â”‚  x â†’ y       â”‚â”€â”€â”€â”€â–¶â”‚  y â†’ y'      â”‚
  â”‚  3 hidden    â”‚     â”‚  3 hidden    â”‚
  â”‚  4 rÃ©gions   â”‚     â”‚  4 rÃ©gions   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                    â”‚
        â–¼                    â–¼
  3 "plis" du              Fonction dupliquÃ©e
  domaine x                3 Ã— 3 = 9 rÃ©gions !`,
        label: 'Fig. 4.1 â€” Composer 2 rÃ©seaux : pliage + duplication',
      },
      {
        type: 'callout',
        content: 'ğŸ§  **Intuition du pliage** : le premier rÃ©seau replie l\'espace d\'entrÃ©e. Le second rÃ©seau travaille sur l\'espace repliÃ©. En "dÃ©pliant", on voit que la fonction du second rÃ©seau est rÃ©pliquÃ©e Ã  chaque pli, variously flipped et rescaled.',
      },
      {
        type: 'text',
        content: `## 4.2 â€” De la composition au rÃ©seau profond\n\nCette composition est un **cas particulier** d'un rÃ©seau Ã  2 couches cachÃ©es. Le rÃ©seau gÃ©nÃ©ral est plus expressif car les poids entre couches sont **libres** (pas contraints au produit extÃ©rieur).`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} h_d\' &= a\\!\\left[\\psi_{d0} + \\psi_{d1}h_1 + \\psi_{d2}h_2 + \\psi_{d3}h_3\\right] \\end{aligned}',
        label: 'Ã‰q. 4.6 â€” Couche cachÃ©e 2 : fonction des activations de la couche 1',
        highlightVar: 'hidden',
      },
      {
        type: 'text',
        content: `## 4.3 â€” RÃ©seau profond gÃ©nÃ©ral\n\nUn rÃ©seau Ã  K couches cachÃ©es applique alternativement des transformations affines et des activations ReLU. Le calcul procÃ¨de couche par couche :\n\n1. **PrÃ©-activations** fâ‚– = Î²â‚– + Î©â‚–hâ‚– (transformation affine)\n2. **Activations** hâ‚–â‚Šâ‚ = a[fâ‚–] (ReLU "clippe" les nÃ©gatifs, crÃ©e de nouveaux joints)\n3. La sortie finale est une derniÃ¨re combinaison linÃ©aire`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} \\mathbf{h}_1 &= a[\\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x}] \\\\ \\mathbf{h}_k &= a[\\boldsymbol{\\beta}_{k-1} + \\boldsymbol{\\Omega}_{k-1} \\mathbf{h}_{k-1}] \\\\ \\mathbf{y} &= \\boldsymbol{\\beta}_K + \\boldsymbol{\\Omega}_K \\mathbf{h}_K \\end{aligned}',
        label: 'Ã‰q. 4.15 â€” RÃ©seau profond Ã  K couches cachÃ©es',
        highlightVar: 'hidden',
      },
      {
        type: 'diagram',
        content: `  x (Dáµ¢)                                         y (Dâ‚’)
  â”€â”¬â”€      Î©â‚€         Î©â‚         Î©â‚‚         Î©â‚ƒ
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
   â”œâ”€â–¶â”‚ Î²â‚€+Î©â‚€Â·x â”‚â–¶â”‚ Î²â‚+Î©â‚Â·hâ‚â”‚â–¶â”‚ Î²â‚‚+Î©â‚‚Â·hâ‚‚â”‚â–¶â”‚Î²â‚ƒ+Î©â‚ƒâ”‚â”€â”€â–¶ y
   â”‚  â”‚  ReLU    â”‚ â”‚  ReLU    â”‚ â”‚  ReLU    â”‚ â”‚Â·hâ‚ƒ   â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
   â”‚     hâ‚(Dâ‚)      hâ‚‚(Dâ‚‚)      hâ‚ƒ(Dâ‚ƒ)
   â”‚
  Dáµ¢=3, Dâ‚=4, Dâ‚‚=2, Dâ‚ƒ=3, Dâ‚’=2  (ex. Fig. 4.6)`,
        label: 'Fig. 4.6 â€” Architecture avec K=3 couches cachÃ©es',
      },
      {
        type: 'text',
        content: `## 4.4 â€” HyperparamÃ¨tres\n\nLe nombre de couches **K** (profondeur) et le nombre d'unitÃ©s par couche **Dâ‚, Dâ‚‚, ..., Dâ‚–** (largeur) sont des **hyperparamÃ¨tres** : ils sont fixÃ©s *avant* l'apprentissage des poids. Pour des hyperparamÃ¨tres fixÃ©s, les poids dÃ©finissent une fonction particuliÃ¨re. En changeant les hyperparamÃ¨tres, on explore une "famille de familles" de fonctions.`,
      },
      {
        type: 'text',
        content: `## 4.5 â€” Profondeur vs Largeur\n\n**Nombre de rÃ©gions linÃ©aires** : avec D unitÃ©s par couche et K couches, le nombre maximum de rÃ©gions est **(D+1)^K** (vs D+1 pour un rÃ©seau superficiel). L'explosion est exponentielle :`,
      },
      {
        type: 'diagram',
        content: `  K=1 (shallow)  â”‚  K=2           â”‚  K=5
  D=10            â”‚  D=10          â”‚  D=10
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  11 rÃ©gions      â”‚  121 rÃ©gions   â”‚  161,051 rÃ©gions
  31 params       â”‚  141 params    â”‚  471 params

  â†’ Avec le MÃŠME budget de paramÃ¨tres,
    le rÃ©seau profond crÃ©e exponentiellement plus de rÃ©gions !`,
        label: 'Fig. 4.7 â€” RÃ©gions linÃ©aires : shallow vs deep',
      },
      {
        type: 'callout',
        content: 'âš¡ **Depth efficiency** : certaines fonctions nÃ©cessitent un rÃ©seau superficiel avec **exponentiellement** plus d\'unitÃ©s cachÃ©es pour atteindre la mÃªme approximation qu\'un rÃ©seau profond. C\'est pourquoi en pratique, les meilleurs rÃ©sultats sont obtenus avec des dizaines ou centaines de couches.',
      },
      {
        type: 'text',
        content: `**En PyTorch**, un rÃ©seau profond se construit soit avec \`nn.Sequential\` soit avec \`nn.Module\` personnalisÃ© :\n\n- \`nn.Sequential(*layers)\` : empile les couches, forward automatique\n- \`nn.Module\` : plus flexible, permet des skip connections (voir ResNet)\n- \`model.named_parameters()\` : inspecte couche par couche\n- \`torchsummary.summary(model, input_size)\` : rÃ©sumÃ© complet`,
      },
      {
        type: 'text',
        content: `## 4.6 â€” Comptage des paramÃ¨tres (rÃ©seau profond)\n\nPour un rÃ©seau Ã  K couches avec Dâ‚– unitÃ©s par couche :`,
      },
      {
        type: 'equation',
        content: 'N_{\\text{params}} = \\sum_{k=0}^{K} D_{k+1} \\cdot (D_k + 1) \\quad \\text{oÃ¹ } D_0 = D_i,\\; D_{K+1} = D_o',
        label: 'Nombre de paramÃ¨tres d\'un rÃ©seau profond',
      },
      {
        type: 'callout',
        content: 'ğŸ§  **RÃ©sumÃ© Ch. 4** :\n(1) Composer des rÃ©seaux = plier l\'espace d\'entrÃ©e\n(2) Chaque couche clippe (ReLU) et crÃ©e de nouveaux joints\n(3) RÃ©gions max = (D+1)^K â€” croissance exponentielle\n(4) Le deep learning est efficace car les fonctions rÃ©elles sont souvent compositionnelles\n(5) Pour les grandes entrÃ©es structurÃ©es (images), le traitement local-to-global nÃ©cessite la profondeur',
      },
    ],
    exercises: [
      {
        id: 'deep-ex1',
        title: 'ğŸ’» Pratique â€” RÃ©seau Ã  3 couches cachÃ©es',
        instructions: 'CrÃ©ez un rÃ©seau nn.Module avec 3 couches cachÃ©es (784â†’256â†’128â†’64â†’10). Comptez les paramÃ¨tres par couche et au total.',
        starterCode: `import torch
import torch.nn as nn

class DeepNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = ___
        self.layer2 = ___
        self.layer3 = ___
        self.output = ___
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.relu(self.layer3(x))
        return self.output(x)

model = DeepNet()
print(model)

# Comptage par couche
for name, param in model.named_parameters():
    print(f"  {name:15s} : {str(list(param.shape)):15s} = {param.numel():>7,} params")

total = sum(p.numel() for p in model.parameters())
print(f"\\nTotal paramÃ¨tres: {total:,}")`,
        solution: `import torch
import torch.nn as nn

class DeepNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(784, 256)
        self.layer2 = nn.Linear(256, 128)
        self.layer3 = nn.Linear(128, 64)
        self.output = nn.Linear(64, 10)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.relu(self.layer3(x))
        return self.output(x)

model = DeepNet()
print(model)

for name, param in model.named_parameters():
    print(f"  {name:15s} : {str(list(param.shape)):15s} = {param.numel():>7,} params")

total = sum(p.numel() for p in model.parameters())
print(f"\\nTotal paramÃ¨tres: {total:,}")`,
        hints: [
          'nn.Linear(in_features, out_features) crÃ©e une couche dense',
          'Couche 1: 784â†’256, Couche 2: 256â†’128, Couche 3: 128â†’64, Sortie: 64â†’10',
        ],
        completed: false,
      },
      {
        id: 'deep-th1',
        title: 'ğŸ§  ThÃ©orie â€” RÃ©gions linÃ©aires (Prob. 4.8)',
        instructions: 'Calculez et comparez le nombre maximum de rÃ©gions linÃ©aires pour des rÃ©seaux de profondeur K=1 Ã  K=10, avec D=10 unitÃ©s par couche. VÃ©rifiez la formule (D+1)^K.',
        starterCode: `import torch

def max_regions_shallow(D):
    """RÃ©gions max pour rÃ©seau superficiel"""
    return D + 1

def max_regions_deep(D, K):
    """RÃ©gions max pour rÃ©seau profond Ã  K couches, D unitÃ©s/couche"""
    return ___

def count_params_deep(D_i, D, K, D_o):
    """Nombre de paramÃ¨tres d'un rÃ©seau profond"""
    # Couche 1 : D*(D_i+1)
    # Couches 2..K : (K-1)*D*(D+1)
    # Sortie : D_o*(D+1)
    return ___

D = 10
D_i, D_o = 1, 1

print(f"{'K':>3} â”‚ {'Params':>8} â”‚ {'RÃ©gions max':>15} â”‚ {'RÃ©gions/param':>15}")
print(f"{'â”€'*3}â”€â”¼â”€{'â”€'*8}â”€â”¼â”€{'â”€'*15}â”€â”¼â”€{'â”€'*15}")
for K in range(1, 11):
    n_params = count_params_deep(D_i, D, K, D_o)
    n_regions = max_regions_deep(D, K)
    ratio = n_regions / n_params
    print(f"{K:3d} â”‚ {n_params:8,} â”‚ {n_regions:15,} â”‚ {ratio:15.1f}")`,
        solution: `import torch

def max_regions_shallow(D):
    return D + 1

def max_regions_deep(D, K):
    return (D + 1) ** K

def count_params_deep(D_i, D, K, D_o):
    return D * (D_i + 1) + (K - 1) * D * (D + 1) + D_o * (D + 1)

D = 10
D_i, D_o = 1, 1

print(f"{'K':>3} â”‚ {'Params':>8} â”‚ {'RÃ©gions max':>15} â”‚ {'RÃ©gions/param':>15}")
print(f"{'â”€'*3}â”€â”¼â”€{'â”€'*8}â”€â”¼â”€{'â”€'*15}â”€â”¼â”€{'â”€'*15}")
for K in range(1, 11):
    n_params = count_params_deep(D_i, D, K, D_o)
    n_regions = max_regions_deep(D, K)
    ratio = n_regions / n_params
    print(f"{K:3d} â”‚ {n_params:8,} â”‚ {n_regions:15,} â”‚ {ratio:15.1f}")`,
        hints: [
          'RÃ©gions max = (D+1)^K',
          'Params couche 1 = D*(D_i+1), couches internes = D*(D+1), sortie = D_o*(D+1)',
        ],
        completed: false,
      },
      {
        id: 'deep-pr2',
        title: 'ğŸ’» Pratique â€” Deep vs Shallow (sin approximation)',
        instructions: 'Comparez un rÃ©seau superficiel (D=100) et un rÃ©seau profond (K=3, D=20) pour approximer sin(x). Les deux ont ~300 paramÃ¨tres â€” lequel converge le mieux ?',
        starterCode: `import torch
import torch.nn as nn
import torch.optim as optim
import math

torch.manual_seed(42)

x = torch.linspace(-math.pi, math.pi, 200).unsqueeze(1)
y = torch.sin(x)

# RÃ©seau SUPERFICIEL : 1 â†’ 100 â†’ 1
shallow = nn.Sequential(
    nn.Linear(1, 100), nn.ReLU(), nn.Linear(100, 1)
)

# RÃ©seau PROFOND : 1 â†’ 20 â†’ 20 â†’ 20 â†’ 1
deep = nn.Sequential(
    ___,  # 4 couches Ã  remplir
)

print(f"Shallow params: {sum(p.numel() for p in shallow.parameters())}")
print(f"Deep params:    {sum(p.numel() for p in deep.parameters())}")

def train(model, epochs=2000):
    opt = optim.Adam(model.parameters(), lr=0.01)
    loss_fn = nn.MSELoss()
    for _ in range(epochs):
        loss = loss_fn(model(x), y)
        opt.zero_grad(); loss.backward(); opt.step()
    return loss_fn(model(x), y).item()

loss_s = train(shallow)
loss_d = train(deep)
print(f"\\nShallow loss: {loss_s:.6f}")
print(f"Deep loss:    {loss_d:.6f}")
print(f"â†’ {'Deep' if loss_d < loss_s else 'Shallow'} gagne !")`,
        solution: `import torch
import torch.nn as nn
import torch.optim as optim
import math

torch.manual_seed(42)

x = torch.linspace(-math.pi, math.pi, 200).unsqueeze(1)
y = torch.sin(x)

shallow = nn.Sequential(
    nn.Linear(1, 100), nn.ReLU(), nn.Linear(100, 1)
)

deep = nn.Sequential(
    nn.Linear(1, 20), nn.ReLU(),
    nn.Linear(20, 20), nn.ReLU(),
    nn.Linear(20, 20), nn.ReLU(),
    nn.Linear(20, 1),
)

print(f"Shallow params: {sum(p.numel() for p in shallow.parameters())}")
print(f"Deep params:    {sum(p.numel() for p in deep.parameters())}")

def train(model, epochs=2000):
    opt = optim.Adam(model.parameters(), lr=0.01)
    loss_fn = nn.MSELoss()
    for _ in range(epochs):
        loss = loss_fn(model(x), y)
        opt.zero_grad(); loss.backward(); opt.step()
    return loss_fn(model(x), y).item()

loss_s = train(shallow)
loss_d = train(deep)
print(f"\\nShallow loss: {loss_s:.6f}")
print(f"Deep loss:    {loss_d:.6f}")
print(f"â†’ {'Deep' if loss_d < loss_s else 'Shallow'} gagne !")`,
        hints: [
          'nn.Linear(1, 20), nn.ReLU(), nn.Linear(20, 20), nn.ReLU(), ...',
          'Les deux modÃ¨les ont ~300 params mais le deep crÃ©e plus de rÃ©gions',
        ],
        completed: false,
      },
      {
        id: 'deep-th2',
        title: 'ğŸ§  ThÃ©orie â€” Activation linÃ©aire profonde (Prob. 4.1)',
        instructions: 'Prob. 4.1 : montrez que si on compose deux rÃ©seaux SANS activation (fonction identitÃ© au lieu de ReLU), le rÃ©sultat est encore une simple fonction linÃ©aire. DÃ©montrez-le numÃ©riquement.',
        starterCode: `import torch

# RÃ©seau 1 : y = phi0 + phi1*x (linÃ©aire)
# RÃ©seau 2 : y' = phi0' + phi1'*y (linÃ©aire)
# Composition : y' = phi0' + phi1'*(phi0 + phi1*x)
#             = (phi0' + phi1'*phi0) + (phi1'*phi1)*x
# â†’ Encore linÃ©aire !

# DÃ©monstration avec des couches PyTorch (sans ReLU)
import torch.nn as nn

# Deep network SANS activation
deep_linear = nn.Sequential(
    nn.Linear(1, 50),
    nn.Linear(50, 50),
    nn.Linear(50, 50),
    nn.Linear(50, 1),
)

# VÃ©rifier que c'est une droite
x = torch.linspace(-3, 3, 100).unsqueeze(1)
y = deep_linear(x).detach()

# Fit linÃ©aire : y â‰ˆ ax + b
x_np = x.squeeze().numpy()
y_np = y.squeeze().numpy()
a = (y_np[-1] - y_np[0]) / (x_np[-1] - x_np[0])
b = y_np[0] - a * x_np[0]

# VÃ©rifier que TOUS les points sont sur la droite
y_linear = a * x_np + b
max_error = max(abs(y_np - y_linear))
print(f"Pente a = {a:.4f}, offset b = {b:.4f}")
print(f"Erreur max vs droite : {max_error:.10f}")
print(f"â†’ {'âœ“ C\\'est bien une DROITE !' if max_error < 1e-5 else 'âœ— Pas linÃ©aire'}") 
print(f"\\nğŸ’¡ Sans activation non-linÃ©aire, empiler des couches")
print(f"   n'ajoute AUCUNE expressivitÃ©. C'est pourquoi le ReLU est essentiel !")`,
        solution: `import torch
import torch.nn as nn

deep_linear = nn.Sequential(
    nn.Linear(1, 50),
    nn.Linear(50, 50),
    nn.Linear(50, 50),
    nn.Linear(50, 1),
)

x = torch.linspace(-3, 3, 100).unsqueeze(1)
y = deep_linear(x).detach()

x_np = x.squeeze().numpy()
y_np = y.squeeze().numpy()
a = (y_np[-1] - y_np[0]) / (x_np[-1] - x_np[0])
b = y_np[0] - a * x_np[0]

y_linear = a * x_np + b
max_error = max(abs(y_np - y_linear))
print(f"Pente a = {a:.4f}, offset b = {b:.4f}")
print(f"Erreur max vs droite : {max_error:.10f}")
print(f"â†’ {'âœ“ C\\'est bien une DROITE !' if max_error < 1e-5 else 'âœ— Pas linÃ©aire'}")
print(f"\\nğŸ’¡ Sans activation non-linÃ©aire, empiler des couches")
print(f"   n'ajoute AUCUNE expressivitÃ©. C'est pourquoi le ReLU est essentiel !")`,
        hints: [
          'Composition de fonctions linÃ©aires = fonction linÃ©aire',
          'nn.Linear sans activation entre les couches',
          'La sortie sera toujours une droite y = ax + b',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RÃ©seaux Profonds â€” Ch. 4 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. RÃ©seau SUPERFICIEL vs PROFOND â”€â”€
shallow = nn.Sequential(
    nn.Linear(1, 100), nn.ReLU(), nn.Linear(100, 1)
)

deep = nn.Sequential(
    nn.Linear(1, 20), nn.ReLU(),
    nn.Linear(20, 20), nn.ReLU(),
    nn.Linear(20, 20), nn.ReLU(),
    nn.Linear(20, 1)
)

print(f"Shallow: {sum(p.numel() for p in shallow.parameters())} params")
print(f"Deep:    {sum(p.numel() for p in deep.parameters())} params")

# â”€â”€ 2. Forward pass couche par couche â”€â”€
x = torch.randn(5, 1)
print(f"\\nâ”€â”€ Forward pass dÃ©taillÃ© â”€â”€")
h = x
for i, layer in enumerate(deep):
    h = layer(h)
    print(f"  Couche {i}: {layer.__class__.__name__:10s} â†’ shape {list(h.shape)}")

# â”€â”€ 3. RÃ©gions linÃ©aires max â”€â”€
print(f"\\nâ”€â”€ RÃ©gions linÃ©aires â”€â”€")
D = 20
for K in [1, 2, 3, 5, 10]:
    regions = (D + 1) ** K
    print(f"  K={K:2d}, D={D}: {regions:>15,} rÃ©gions max")

# â”€â”€ 4. nn.Module personnalisÃ© â”€â”€
class FlexibleDeepNet(nn.Module):
    def __init__(self, dims):
        super().__init__()
        layers = []
        for i in range(len(dims) - 1):
            layers.append(nn.Linear(dims[i], dims[i+1]))
            if i < len(dims) - 2:
                layers.append(nn.ReLU())
        self.net = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.net(x)

model = FlexibleDeepNet([1, 50, 50, 50, 1])
print(f"\\nFlexible: {sum(p.numel() for p in model.parameters())} params")
print(f"Output: {model(torch.tensor([[1.0]])).item():.4f}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 5 â€” FONCTIONS DE PERTE (Ch. 5)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'loss-functions',
    title: 'Fonctions de Perte (Loss Functions)',
    shortTitle: 'Loss',
    description: 'Maximum de vraisemblance, MSE, Binary/Multi-class Cross-Entropy, rÃ©gression hÃ©tÃ©roscÃ©dastique (Ch. 5 â€” UDL).',
    status: 'locked',
    progress: 0,
    dependencies: ['deep-networks'],
    category: 'training',
    theory: [
      {
        type: 'text',
        content: `## 5.1 â€” La recette du Maximum de Vraisemblance\n\nLa **fonction de perte** mesure l'Ã©cart entre prÃ©dictions et rÃ©alitÃ©. Le framework universel est le **maximum de vraisemblance** en 4 Ã©tapes :\n\n1. **Choisir un modÃ¨le** de distribution Pr(y | f[x, Ï•])\n2. **Ã‰crire la vraisemblance** L = Î áµ¢ Pr(yáµ¢ | f[xáµ¢, Ï•])\n3. **Prendre le log nÃ©gatif** : âˆ’log L = âˆ’Î£áµ¢ log Pr(yáµ¢ | ...)\n4. **Minimiser** la perte LÌ‚ pour trouver Ï•Ì‚`,
      },
      {
        type: 'equation',
        content: '\\hat{\\boldsymbol{\\phi}} = \\underset{\\boldsymbol{\\phi}}{\\text{argmin}} \\left[ -\\sum_{i=1}^{I} \\log\\!\\left[ Pr(y_i \\,|\\, f[\\mathbf{x}_i, \\boldsymbol{\\phi}]) \\right] \\right]',
        label: 'Ã‰q. 5.2 â€” Estimateur du maximum de vraisemblance',
      },
      {
        type: 'callout',
        content: 'ğŸ§  **Pourquoi le log ?** Le produit de probabilitÃ©s â†’ somme de logs (plus stable numÃ©riquement). La maximisation â†’ minimisation du nÃ©gatif. Le rÃ©sultat est la **negative log-likelihood** (NLL).',
      },
      {
        type: 'text',
        content: `## 5.2 â€” RÃ©gression â†’ MSE (Gaussienne)\n\nSi on suppose que y suit une loi **normale** centrÃ©e sur la prÃ©diction du rÃ©seau f[x, Ï•] avec variance ÏƒÂ² :\n\nPr(y | f) = Normal_y[f, ÏƒÂ²]\n\nAlors le log nÃ©gatif donne :\nâˆ’log Pr = (y âˆ’ f)Â² / 2ÏƒÂ² + constante\n\nEn ignorant la constante et ÏƒÂ² fixe, on retrouve le **MSE** :`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}_{\\text{MSE}} = \\frac{1}{I}\\sum_{i=1}^{I}\\left(y_i - f[\\mathbf{x}_i, \\boldsymbol{\\phi}]\\right)^2',
        label: 'Ã‰q. 5.6 â€” MSE dÃ©rivÃ© de la vraisemblance gaussienne',
        highlightVar: 'loss',
      },
      {
        type: 'diagram',
        content: `  Distribution de y|x        DÃ©rivation
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  y ~ Normal(f[x,Ï•], ÏƒÂ²)    Pr(y|x) = N(y; f, ÏƒÂ²)
                             log Pr = -(y-f)Â²/(2ÏƒÂ²) + C
      â•­â”€â”€â•®                   -log Pr âˆ (y - f)Â²
    â•­â”€â•¯  â•°â”€â•®                
  â”€â”€â•¯ f[x,Ï•]â•°â”€â”€  â† Ïƒ â†’      â†’ MSE Loss ! âœ“`,
        label: 'Fig. 5.3 â€” Gaussienne â†’ MSE',
      },
      {
        type: 'text',
        content: `## 5.3 â€” Classification binaire â†’ BCE (Bernoulli)\n\nPour y âˆˆ {0, 1}, on modÃ©lise Pr(y=1|x) via la **sigmoÃ¯de** Ïƒ(f) = 1/(1+e^{-f}). La distribution est **Bernoulli** et la NLL donne la **Binary Cross-Entropy** :`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{I}\\sum_{i=1}^{I}\\left[ y_i \\log\\sigma(f_i) + (1-y_i)\\log(1-\\sigma(f_i)) \\right]',
        label: 'Ã‰q. 5.12 â€” Binary Cross-Entropy',
        highlightVar: 'loss',
      },
      {
        type: 'text',
        content: `## 5.4 â€” Classification multi-classe â†’ Softmax + CE (CatÃ©gorielle)\n\nPour K classes, le rÃ©seau produit K logits. Le **softmax** transforme ces logits en probabilitÃ©s positives qui somment Ã  1. La perte est la **cross-entropie catÃ©gorielle** :`,
      },
      {
        type: 'equation',
        content: '\\text{softmax}_k = \\frac{e^{f_k}}{\\sum_{j=1}^{K} e^{f_j}} \\qquad\\qquad \\mathcal{L}_{\\text{CE}} = -\\sum_{i=1}^{I} \\log\\!\\left( \\text{softmax}_{y_i}(\\mathbf{f}_i) \\right)',
        label: 'Ã‰q. 5.17/5.22 â€” Softmax + Cross-Entropy catÃ©gorielle',
        highlightVar: 'loss',
      },
      {
        type: 'diagram',
        content: `  Logits (sortie rÃ©seau)         Softmax            Loss
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  fâ‚ =  2.0  â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²        P(c=1) = 0.659    si y=1:
  fâ‚‚ =  1.0  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•‹â”€â”€â–¶    P(c=2) = 0.242    L = -log(0.659)
  fâ‚ƒ =  0.1  â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±        P(c=3) = 0.099       = 0.417
                        Î£=1.0 âœ“

  âš ï¸ PyTorch nn.CrossEntropyLoss prend les LOGITS,
     pas les probabilitÃ©s ! Le softmax est inclus.`,
        label: 'Fig. â€” Pipeline softmax â†’ cross-entropy',
      },
      {
        type: 'text',
        content: `## 5.5 â€” RÃ©gression hÃ©tÃ©roscÃ©dastique\n\nLe MSE standard suppose un bruit **constant** ÏƒÂ². Mais en rÃ©alitÃ©, l'incertitude peut varier selon x. Le rÃ©seau peut prÃ©dire **deux sorties** : la moyenne Î¼(x) ET la variance ÏƒÂ²(x). La perte devient :`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}_{\\text{hetero}} = \\sum_{i=1}^{I}\\left[ \\frac{(y_i - \\mu_i)^2}{2\\sigma_i^2} + \\frac{1}{2}\\log\\sigma_i^2 \\right]',
        label: 'Ã‰q. 5.8 â€” Perte hÃ©tÃ©roscÃ©dastique',
        highlightVar: 'loss',
      },
      {
        type: 'callout',
        content: 'âš¡ **RÃ©sumÃ© des loss functions** :\nâ€¢ RÃ©gression â†’ Normal â†’ **MSE** (\\`nn.MSELoss\\`)\nâ€¢ Binaire â†’ Bernoulli+sigmoid â†’ **BCE** (\\`nn.BCEWithLogitsLoss\\`)\nâ€¢ Multi-classe â†’ CatÃ©gorielle+softmax â†’ **CE** (\\`nn.CrossEntropyLoss\\`)\nâ€¢ Incertitude variable â†’ HÃ©tÃ©roscÃ©dastique (custom)\n\nCe sont TOUTES des cas particuliers de la NLL !',
      },
      {
        type: 'text',
        content: `**En PyTorch**, les fonctions de perte sont dans \`torch.nn\` :\n\n- \`nn.MSELoss()\` : rÃ©gression\n- \`nn.BCEWithLogitsLoss()\` : binaire (inclut sigmoid, plus stable)\n- \`nn.CrossEntropyLoss()\` : multi-classe (inclut softmax)\n- \`nn.NLLLoss()\` : NLL brute (si softmax dÃ©jÃ  appliquÃ© via \`nn.LogSoftmax\`)\n\nâš ï¸ \`nn.CrossEntropyLoss\` attend des **logits** (pas des probabilitÃ©s) et des **labels entiers** (pas one-hot).`,
      },
    ],
    exercises: [
      {
        id: 'loss-ex1',
        title: 'ğŸ’» Pratique â€” MSE manuelle vs PyTorch',
        instructions: 'ImplÃ©mentez le MSE manuellement et vÃ©rifiez qu\'il correspond Ã  nn.MSELoss. Calculez aussi la NLL gaussienne complÃ¨te.',
        starterCode: `import torch
import torch.nn as nn

predictions = torch.tensor([2.5, 3.2, 4.1, 1.8])
targets = torch.tensor([3.0, 3.0, 4.0, 2.0])

# 1. MSE manuelle
mse_manual = ___  # torch.mean((pred - target)Â²)

# 2. MSE PyTorch
mse_pytorch = nn.MSELoss()(predictions, targets)

# 3. NLL gaussienne complÃ¨te (avec sigma=0.5)
sigma = 0.5
nll = torch.mean(
    (targets - predictions)**2 / (2 * sigma**2) + torch.log(torch.tensor(sigma))
)

print(f"MSE manuelle:  {mse_manual.item():.6f}")
print(f"MSE PyTorch:   {mse_pytorch.item():.6f}")
print(f"NLL (Ïƒ={sigma}): {nll.item():.6f}")
print(f"\\nâ†’ MSE = NLL Ã— 2ÏƒÂ² = {nll.item() * 2 * sigma**2:.6f}")`,
        solution: `import torch
import torch.nn as nn

predictions = torch.tensor([2.5, 3.2, 4.1, 1.8])
targets = torch.tensor([3.0, 3.0, 4.0, 2.0])

mse_manual = torch.mean((predictions - targets) ** 2)
mse_pytorch = nn.MSELoss()(predictions, targets)

sigma = 0.5
nll = torch.mean(
    (targets - predictions)**2 / (2 * sigma**2) + torch.log(torch.tensor(sigma))
)

print(f"MSE manuelle:  {mse_manual.item():.6f}")
print(f"MSE PyTorch:   {mse_pytorch.item():.6f}")
print(f"NLL (Ïƒ={sigma}): {nll.item():.6f}")
print(f"\\nâ†’ MSE = NLL Ã— 2ÏƒÂ² = {nll.item() * 2 * sigma**2:.6f}")`,
        hints: [
          'mse_manual = torch.mean((predictions - targets) ** 2)',
          'La NLL gaussienne = (y-f)Â²/(2ÏƒÂ²) + log(Ïƒ)',
        ],
        completed: false,
      },
      {
        id: 'loss-ex2',
        title: 'ğŸ’» Pratique â€” BCE avec Sigmoid',
        instructions: 'Comparez nn.BCELoss (attend des probabilitÃ©s) et nn.BCEWithLogitsLoss (attend des logits). VÃ©rifiez qu\'ils donnent le mÃªme rÃ©sultat.',
        starterCode: `import torch
import torch.nn as nn

logits = torch.tensor([2.0, -1.0, 0.5, 3.0])
targets = torch.tensor([1.0, 0.0, 1.0, 1.0])

# 1. Avec BCEWithLogitsLoss (recommandÃ©)
loss_logits = nn.BCEWithLogitsLoss()(logits, targets)

# 2. Avec BCELoss (appliquer sigmoid d'abord)
probs = ___  # torch.sigmoid(logits)
loss_probs = nn.BCELoss()(probs, targets)

# 3. Manuelle
bce_manual = -torch.mean(
    targets * torch.log(probs + 1e-8) + (1 - targets) * torch.log(1 - probs + 1e-8)
)

print(f"BCEWithLogitsLoss: {loss_logits.item():.6f}")
print(f"BCELoss (sigmoid): {loss_probs.item():.6f}")
print(f"BCE manuelle:      {bce_manual.item():.6f}")
print(f"\\nâœ“ Identiques !" if abs(loss_logits.item() - loss_probs.item()) < 1e-5 else "âœ— DiffÃ©rents")`,
        solution: `import torch
import torch.nn as nn

logits = torch.tensor([2.0, -1.0, 0.5, 3.0])
targets = torch.tensor([1.0, 0.0, 1.0, 1.0])

loss_logits = nn.BCEWithLogitsLoss()(logits, targets)

probs = torch.sigmoid(logits)
loss_probs = nn.BCELoss()(probs, targets)

bce_manual = -torch.mean(
    targets * torch.log(probs + 1e-8) + (1 - targets) * torch.log(1 - probs + 1e-8)
)

print(f"BCEWithLogitsLoss: {loss_logits.item():.6f}")
print(f"BCELoss (sigmoid): {loss_probs.item():.6f}")
print(f"BCE manuelle:      {bce_manual.item():.6f}")
print(f"\\nâœ“ Identiques !" if abs(loss_logits.item() - loss_probs.item()) < 1e-5 else "âœ— DiffÃ©rents")`,
        hints: [
          'probs = torch.sigmoid(logits)',
          'BCEWithLogitsLoss = sigmoid + BCELoss en une seule opÃ©ration',
        ],
        completed: false,
      },
      {
        id: 'loss-th1',
        title: 'ğŸ§  ThÃ©orie â€” Softmax + CE multi-classe',
        instructions: 'ImplÃ©mentez softmax et cross-entropy manuellement. VÃ©rifiez contre nn.CrossEntropyLoss. Montrez que le softmax est invariant par translation.',
        starterCode: `import torch
import torch.nn as nn

logits = torch.tensor([[2.0, 1.0, 0.1],
                        [0.5, 2.5, 0.3]])
labels = torch.tensor([0, 1])

# 1. Softmax manuelle
def softmax_manual(z):
    e = torch.exp(z - z.max(dim=-1, keepdim=True).values)  # stabilitÃ©
    return e / e.sum(dim=-1, keepdim=True)

probs = softmax_manual(logits)
print(f"Softmax: {probs}")
print(f"Somme:   {probs.sum(dim=-1)}")

# 2. Cross-entropy manuelle
def cross_entropy_manual(logits, labels):
    probs = softmax_manual(logits)
    log_probs = torch.log(probs + 1e-8)
    return ___  # NLL

ce_manual = cross_entropy_manual(logits, labels)
ce_pytorch = nn.CrossEntropyLoss()(logits, labels)

print(f"\\nCE manuelle: {ce_manual.item():.6f}")
print(f"CE PyTorch:  {ce_pytorch.item():.6f}")

# 3. Invariance par translation
shifted = logits + 100  
print(f"\\nSoftmax invariant ? {torch.allclose(softmax_manual(logits), softmax_manual(shifted))}")`,
        solution: `import torch
import torch.nn as nn

logits = torch.tensor([[2.0, 1.0, 0.1],
                        [0.5, 2.5, 0.3]])
labels = torch.tensor([0, 1])

def softmax_manual(z):
    e = torch.exp(z - z.max(dim=-1, keepdim=True).values)
    return e / e.sum(dim=-1, keepdim=True)

probs = softmax_manual(logits)
print(f"Softmax: {probs}")
print(f"Somme:   {probs.sum(dim=-1)}")

def cross_entropy_manual(logits, labels):
    probs = softmax_manual(logits)
    log_probs = torch.log(probs + 1e-8)
    return -torch.mean(log_probs[range(len(labels)), labels])

ce_manual = cross_entropy_manual(logits, labels)
ce_pytorch = nn.CrossEntropyLoss()(logits, labels)

print(f"\\nCE manuelle: {ce_manual.item():.6f}")
print(f"CE PyTorch:  {ce_pytorch.item():.6f}")

shifted = logits + 100
print(f"\\nSoftmax invariant ? {torch.allclose(softmax_manual(logits), softmax_manual(shifted))}")`,
        hints: [
          'NLL = -mean(log_probs[range(N), labels])',
          'Soustraire le max pour la stabilitÃ© numÃ©rique (log-sum-exp trick)',
        ],
        completed: false,
      },
      {
        id: 'loss-pr3',
        title: 'ğŸ’» Pratique â€” RÃ©gression hÃ©tÃ©roscÃ©dastique',
        instructions: 'Construisez un rÃ©seau qui prÃ©dit Ã  la fois la moyenne Î¼(x) et la variance ÏƒÂ²(x), puis entraÃ®nez-le sur des donnÃ©es avec un bruit variable.',
        starterCode: `import torch
import torch.nn as nn
import torch.optim as optim
import math

torch.manual_seed(42)

# DonnÃ©es avec bruit VARIABLE
x = torch.linspace(-3, 3, 300).unsqueeze(1)
noise_std = 0.1 + 0.5 * torch.abs(x)  # bruit croissant avec |x|
y = torch.sin(x) + noise_std * torch.randn_like(x)

class HeteroNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(1, 64), nn.ReLU(), nn.Linear(64, 64), nn.ReLU()
        )
        self.head_mu = nn.Linear(64, 1)     # prÃ©dit Î¼
        self.head_logvar = nn.Linear(64, 1)  # prÃ©dit log(ÏƒÂ²)
    
    def forward(self, x):
        h = self.shared(x)
        mu = self.head_mu(h)
        log_var = self.head_logvar(h)
        return mu, log_var

model = HeteroNet()
opt = optim.Adam(model.parameters(), lr=0.005)

for epoch in range(1500):
    mu, log_var = model(x)
    # Perte hÃ©tÃ©roscÃ©dastique : (y-Î¼)Â²/(2ÏƒÂ²) + log(ÏƒÂ²)/2
    loss = torch.mean(
        ___  # Ã  complÃ©ter
    )
    opt.zero_grad(); loss.backward(); opt.step()
    if (epoch+1) % 500 == 0:
        print(f"Epoch {epoch+1}: loss = {loss.item():.4f}")

mu, log_var = model(x)
sigma = torch.exp(0.5 * log_var).detach()
print(f"\\nÏƒ moyen Ã  x=0: {sigma[150].item():.3f}")
print(f"Ïƒ moyen Ã  x=3: {sigma[-1].item():.3f}")
print(f"â†’ Le modÃ¨le a appris que le bruit augmente avec |x| !")`,
        solution: `import torch
import torch.nn as nn
import torch.optim as optim
import math

torch.manual_seed(42)

x = torch.linspace(-3, 3, 300).unsqueeze(1)
noise_std = 0.1 + 0.5 * torch.abs(x)
y = torch.sin(x) + noise_std * torch.randn_like(x)

class HeteroNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(1, 64), nn.ReLU(), nn.Linear(64, 64), nn.ReLU()
        )
        self.head_mu = nn.Linear(64, 1)
        self.head_logvar = nn.Linear(64, 1)
    
    def forward(self, x):
        h = self.shared(x)
        mu = self.head_mu(h)
        log_var = self.head_logvar(h)
        return mu, log_var

model = HeteroNet()
opt = optim.Adam(model.parameters(), lr=0.005)

for epoch in range(1500):
    mu, log_var = model(x)
    loss = torch.mean(
        (y - mu)**2 / (2 * torch.exp(log_var)) + 0.5 * log_var
    )
    opt.zero_grad(); loss.backward(); opt.step()
    if (epoch+1) % 500 == 0:
        print(f"Epoch {epoch+1}: loss = {loss.item():.4f}")

mu, log_var = model(x)
sigma = torch.exp(0.5 * log_var).detach()
print(f"\\nÏƒ moyen Ã  x=0: {sigma[150].item():.3f}")
print(f"Ïƒ moyen Ã  x=3: {sigma[-1].item():.3f}")
print(f"â†’ Le modÃ¨le a appris que le bruit augmente avec |x| !")`,
        hints: [
          'loss = (y - mu)**2 / (2 * exp(log_var)) + 0.5 * log_var',
          'On utilise log_var au lieu de ÏƒÂ² pour garantir la positivitÃ©',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Fonctions de Perte â€” Ch. 5 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. MSE (RÃ©gression) â”€â”€
pred = torch.tensor([2.5, 3.2, 4.1, 1.8])
target = torch.tensor([3.0, 3.0, 4.0, 2.0])

print("â•â•â• MSE (Gaussienne â†’ Distance quadratique) â•â•â•")
mse = nn.MSELoss()(pred, target)
mse_manual = torch.mean((pred - target) ** 2)
print(f"  PyTorch: {mse.item():.6f}")
print(f"  Manuel:  {mse_manual.item():.6f}")

# â”€â”€ 2. BCE (Classification binaire) â”€â”€
print("\\nâ•â•â• BCE (Bernoulli â†’ Binary Cross-Entropy) â•â•â•")
logits = torch.tensor([2.0, -1.0, 0.5])
labels = torch.tensor([1.0, 0.0, 1.0])
bce = nn.BCEWithLogitsLoss()(logits, labels)
print(f"  BCE (logits): {bce.item():.6f}")

# â”€â”€ 3. CE (Classification multi-classe) â”€â”€
print("\\nâ•â•â• CE (CatÃ©gorielle â†’ Cross-Entropy) â•â•â•")
logits_mc = torch.tensor([[2.0, 1.0, 0.1],
                           [0.5, 2.5, 0.3]])
labels_mc = torch.tensor([0, 1])
ce = nn.CrossEntropyLoss()(logits_mc, labels_mc)
print(f"  CE: {ce.item():.6f}")
probs = torch.softmax(logits_mc, dim=1)
print(f"  Softmax â†’ {probs[0].tolist()}")

# â”€â”€ 4. RÃ©sumÃ© â”€â”€
print("\\nâ•â•â• Tableau rÃ©capitulatif â•â•â•")
print("  TÃ¢che          â”‚ Distribution  â”‚ Loss      â”‚ PyTorch")
print("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print("  RÃ©gression     â”‚ Gaussienne    â”‚ MSE       â”‚ nn.MSELoss")
print("  Binaire        â”‚ Bernoulli     â”‚ BCE       â”‚ nn.BCEWithLogitsLoss")
print("  Multi-classe   â”‚ CatÃ©gorielle  â”‚ CE        â”‚ nn.CrossEntropyLoss")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 6 â€” DESCENTE DE GRADIENT (Ch. 6)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'gradient-descent',
    title: 'Descente de Gradient & Optimisation',
    shortTitle: 'Gradient',
    description: 'GD, SGD avec mini-batches, Momentum, Nesterov, Adam â€” comment entraÃ®ner un rÃ©seau (Ch. 6 â€” UDL).',
    status: 'locked',
    progress: 0,
    dependencies: ['loss-functions'],
    category: 'training',
    theory: [
      {
        type: 'text',
        content: `## 6.1 â€” Descente de gradient (GD)\n\nLa **descente de gradient** est l'algorithme itÃ©ratif standard pour minimiser la perte L[Ï•]. On part de paramÃ¨tres initiaux Ï•â‚€ alÃ©atoires, puis on rÃ©pÃ¨te :\n\n1. **Calculer le gradient** âˆ‚L/âˆ‚Ï• (direction de plus forte montÃ©e)\n2. **Faire un pas** dans la direction **opposÃ©e** (descente)`,
      },
      {
        type: 'equation',
        content: '\\boldsymbol{\\phi}_{t+1} \\leftarrow \\boldsymbol{\\phi}_t - \\alpha \\cdot \\frac{\\partial \\mathcal{L}[\\boldsymbol{\\phi}_t]}{\\partial \\boldsymbol{\\phi}}',
        label: 'Ã‰q. 6.3 â€” Mise Ã  jour Gradient Descent',
        highlightVar: 'grad',
      },
      {
        type: 'diagram',
        content: `  Paysage de perte L(Ï•)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  L â†‘
    â”‚    â•­â•®                  Î± trop grand
    â”‚   â•±  â•²    â•­â”€â”€â•®         â†’ oscillation
    â”‚  â•±    â•²  â•±    â•²
    â”‚ â•±      â•²â•±      â•²       Î± juste
    â”‚â•±  â† pas â†       â•²     â†’ convergence
    â”‚  â”€â”€â”€â”€â”€â–¶Ï•*         â•²
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ï• â†’
        minimum local`,
        label: 'Fig. 6.3 â€” Learning rate Î± trop grand vs juste',
      },
      {
        type: 'text',
        content: `## 6.2 â€” SGD avec mini-batches\n\nLe GD classique calcule le gradient sur **tout** le dataset â†’ coÃ»teux. Le **SGD** (Stochastic Gradient Descent) utilise un **mini-batch** B de taille b Ã  chaque itÃ©ration :\n\n- Un passage complet sur toutes les donnÃ©es = une **epoch**\n- Le gradient est bruitÃ© mais **non-biaisÃ©** en espÃ©rance\n- En pratique : b = 32, 64, 128, 256`,
      },
      {
        type: 'equation',
        content: '\\boldsymbol{\\phi}_{t+1} \\leftarrow \\boldsymbol{\\phi}_t - \\alpha \\cdot \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} \\frac{\\partial \\ell_i}{\\partial \\boldsymbol{\\phi}}',
        label: 'Ã‰q. 6.10 â€” SGD avec mini-batch',
        highlightVar: 'grad',
      },
      {
        type: 'text',
        content: `## 6.3 â€” Momentum\n\nLe SGD pur oscille dans les vallÃ©es Ã©troites. Le **Momentum** ajoute de l'inertie : on accumule une **moyenne mobile** des gradients passÃ©s. Cela lisse la trajectoire et accÃ©lÃ¨re dans les directions constantes :`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} \\mathbf{m}_{t+1} &= \\beta \\, \\mathbf{m}_t + (1 - \\beta) \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\phi}} \\\\ \\boldsymbol{\\phi}_{t+1} &= \\boldsymbol{\\phi}_t - \\alpha \\, \\mathbf{m}_{t+1} \\end{aligned}',
        label: 'Ã‰q. 6.11 â€” SGD avec Momentum (Î² â‰ˆ 0.9)',
      },
      {
        type: 'text',
        content: `## 6.4 â€” Adam (Adaptive Moment Estimation)\n\n**Adam** combine Momentum (moyenne mobile du gradient m) et **RMSProp** (moyenne mobile du gradientÂ²). Il adapte le learning rate **par paramÃ¨tre** â€” les paramÃ¨tres peu mis Ã  jour reÃ§oivent des pas plus grands :`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} \\mathbf{m}_t &= \\beta_1 \\mathbf{m}_{t-1} + (1-\\beta_1) \\mathbf{g}_t & \\text{(1er moment)} \\\\ \\mathbf{v}_t &= \\beta_2 \\mathbf{v}_{t-1} + (1-\\beta_2) \\mathbf{g}_t^2 & \\text{(2e moment)} \\\\ \\hat{\\mathbf{m}}_t &= \\frac{\\mathbf{m}_t}{1 - \\beta_1^t} \\;,\\; \\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1 - \\beta_2^t} & \\text{(correction biais)} \\\\ \\boldsymbol{\\phi}_t &= \\boldsymbol{\\phi}_{t-1} - \\alpha \\frac{\\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon} & \\text{(mise Ã  jour)} \\end{aligned}',
        label: 'Ã‰q. 6.15â€“6.18 â€” Algorithme Adam',
      },
      {
        type: 'diagram',
        content: `  Comparaison des optimiseurs
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                        Batch GD   SGD    Momentum  Adam
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Vitesse par step      Lent      Rapide  Rapide    Rapide
  StabilitÃ©             +++       +       ++        +++
  Learning rate adaptatif  Non   Non     Non       OUI
  Biais-correction         -      -       -        OUI
  Usage mÃ©moire           1Ã—      1Ã—      2Ã—        3Ã—
  DÃ©faut Î±                 -     0.01    0.01     0.001
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Î²â‚ = 0.9,  Î²â‚‚ = 0.999,  Îµ = 10â»â¸ (dÃ©fauts Adam)`,
        label: 'Tableau â€” Comparaison des optimiseurs',
      },
      {
        type: 'text',
        content: `## 6.5 â€” HyperparamÃ¨tres d'entraÃ®nement\n\n- **Learning rate Î±** : le PLUS important. Trop grand â†’ diverge. Trop petit â†’ trop lent.\n- **Batch size** : grand â†’ gradient stable mais moins de mises Ã  jour/epoch\n- **Epochs** : nombre de passes sur les donnÃ©es\n- **Learning rate schedule** : rÃ©duire Î± au fil du temps (step decay, cosine annealing, warmup)\n\n**Recherche d'hyperparamÃ¨tres** : grid search, random search (souvent meilleur), ou Bayesian optimization.`,
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ **Recette pratique** :\n1. Commencer avec **Adam** (Î±=0.001, Î²â‚=0.9, Î²â‚‚=0.999)\n2. Batch size = 32 ou 64\n3. Surveiller la **perte de validation** (early stopping)\n4. Si sous-optimal â†’ essayer SGD+Momentum avec learning rate schedule\n5. Ne jamais tuner sur les donnÃ©es de test !',
      },
    ],
    exercises: [
      {
        id: 'gd-ex1',
        title: 'ğŸ’» Pratique â€” SGD vs Adam',
        instructions: 'EntraÃ®nez le mÃªme modÃ¨le avec SGD, SGD+Momentum et Adam. Comparez la vitesse de convergence sur 200 epochs.',
        starterCode: `import torch
import torch.nn as nn
import torch.optim as optim

torch.manual_seed(42)

x = torch.randn(100, 1)
y = 3 * x + 2 + torch.randn(100, 1) * 0.1

loss_fn = nn.MSELoss()

# 3 copies du mÃªme modÃ¨le
model_sgd = nn.Linear(1, 1)
model_mom = nn.Linear(1, 1)
model_adam = nn.Linear(1, 1)

# Copier les mÃªmes poids initiaux
model_mom.load_state_dict(model_sgd.state_dict())
model_adam.load_state_dict(model_sgd.state_dict())

opt_sgd = optim.SGD(model_sgd.parameters(), lr=0.01)
opt_mom = optim.SGD(model_mom.parameters(), lr=0.01, momentum=___)
opt_adam = optim.Adam(model_adam.parameters(), lr=___)

for epoch in range(200):
    for model, opt, name in [(model_sgd, opt_sgd, 'SGD'),
                              (model_mom, opt_mom, 'Mom'),
                              (model_adam, opt_adam, 'Adam')]:
        loss = loss_fn(model(x), y)
        opt.zero_grad(); loss.backward(); opt.step()
    
    if (epoch+1) % 50 == 0:
        l1 = loss_fn(model_sgd(x), y).item()
        l2 = loss_fn(model_mom(x), y).item()
        l3 = loss_fn(model_adam(x), y).item()
        print(f"Epoch {epoch+1:3d}: SGD={l1:.4f}  Mom={l2:.4f}  Adam={l3:.4f}")`,
        solution: `import torch
import torch.nn as nn
import torch.optim as optim

torch.manual_seed(42)

x = torch.randn(100, 1)
y = 3 * x + 2 + torch.randn(100, 1) * 0.1

loss_fn = nn.MSELoss()

model_sgd = nn.Linear(1, 1)
model_mom = nn.Linear(1, 1)
model_adam = nn.Linear(1, 1)

model_mom.load_state_dict(model_sgd.state_dict())
model_adam.load_state_dict(model_sgd.state_dict())

opt_sgd = optim.SGD(model_sgd.parameters(), lr=0.01)
opt_mom = optim.SGD(model_mom.parameters(), lr=0.01, momentum=0.9)
opt_adam = optim.Adam(model_adam.parameters(), lr=0.001)

for epoch in range(200):
    for model, opt, name in [(model_sgd, opt_sgd, 'SGD'),
                              (model_mom, opt_mom, 'Mom'),
                              (model_adam, opt_adam, 'Adam')]:
        loss = loss_fn(model(x), y)
        opt.zero_grad(); loss.backward(); opt.step()
    
    if (epoch+1) % 50 == 0:
        l1 = loss_fn(model_sgd(x), y).item()
        l2 = loss_fn(model_mom(x), y).item()
        l3 = loss_fn(model_adam(x), y).item()
        print(f"Epoch {epoch+1:3d}: SGD={l1:.4f}  Mom={l2:.4f}  Adam={l3:.4f}")`,
        hints: [
          'momentum=0.9 pour SGD avec Momentum',
          'lr=0.001 pour Adam (learning rate adaptatif, donc plus petit)',
        ],
        completed: false,
      },
      {
        id: 'gd-th1',
        title: 'ğŸ§  ThÃ©orie â€” Adam from scratch',
        instructions: 'ImplÃ©mentez l\'algorithme Adam manuellement (sans optim.Adam) et vÃ©rifiez la convergence sur une fonction simple f(x) = (x-3)Â².',
        starterCode: `import torch

# Minimiser f(x) = (x - 3)Â²
x = torch.tensor(10.0, requires_grad=True)

# HyperparamÃ¨tres Adam
alpha = 0.1
beta1, beta2, eps = 0.9, 0.999, 1e-8
m, v = 0.0, 0.0

print(f"{'t':>3} â”‚ {'x':>8} â”‚ {'grad':>8} â”‚ {'m_hat':>8} â”‚ {'v_hat':>10} â”‚ {'f(x)':>8}")
print("â”€" * 60)

for t in range(1, 51):
    # Forward
    f = (x - 3) ** 2
    f.backward()
    g = x.grad.item()
    
    # Adam update
    m = ___  # Î²â‚ * m + (1-Î²â‚) * g
    v = ___  # Î²â‚‚ * v + (1-Î²â‚‚) * gÂ²
    m_hat = m / (1 - beta1**t)  # correction biais
    v_hat = v / (1 - beta2**t)
    
    with torch.no_grad():
        x -= alpha * m_hat / (v_hat**0.5 + eps)
        x.grad.zero_()
    
    if t <= 5 or t % 10 == 0:
        print(f"{t:3d} â”‚ {x.item():8.4f} â”‚ {g:8.4f} â”‚ {m_hat:8.4f} â”‚ {v_hat:10.6f} â”‚ {(x.item()-3)**2:8.4f}")

print(f"\\nâœ“ x final = {x.item():.6f} (cible = 3.0)")`,
        solution: `import torch

x = torch.tensor(10.0, requires_grad=True)

alpha = 0.1
beta1, beta2, eps = 0.9, 0.999, 1e-8
m, v = 0.0, 0.0

print(f"{'t':>3} â”‚ {'x':>8} â”‚ {'grad':>8} â”‚ {'m_hat':>8} â”‚ {'v_hat':>10} â”‚ {'f(x)':>8}")
print("â”€" * 60)

for t in range(1, 51):
    f = (x - 3) ** 2
    f.backward()
    g = x.grad.item()
    
    m = beta1 * m + (1 - beta1) * g
    v = beta2 * v + (1 - beta2) * g ** 2
    m_hat = m / (1 - beta1**t)
    v_hat = v / (1 - beta2**t)
    
    with torch.no_grad():
        x -= alpha * m_hat / (v_hat**0.5 + eps)
        x.grad.zero_()
    
    if t <= 5 or t % 10 == 0:
        print(f"{t:3d} â”‚ {x.item():8.4f} â”‚ {g:8.4f} â”‚ {m_hat:8.4f} â”‚ {v_hat:10.6f} â”‚ {(x.item()-3)**2:8.4f}")

print(f"\\nâœ“ x final = {x.item():.6f} (cible = 3.0)")`,
        hints: [
          'm = Î²â‚ * m + (1 - Î²â‚) * g',
          'v = Î²â‚‚ * v + (1 - Î²â‚‚) * gÂ²',
          'La correction de biais divise par (1 - Î²áµ¢áµ—)',
        ],
        completed: false,
      },
      {
        id: 'gd-pr2',
        title: 'ğŸ’» Pratique â€” Mini-batch SGD avec DataLoader',
        instructions: 'Utilisez torch.utils.data.DataLoader pour crÃ©er des mini-batches et entraÃ®ner un rÃ©seau avec SGD.',
        starterCode: `import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

torch.manual_seed(42)

# Dataset
X = torch.randn(1000, 5)
W_true = torch.tensor([1.0, -2.0, 3.0, -1.0, 0.5])
y = X @ W_true + 0.1 * torch.randn(1000)

dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=___, shuffle=True)

model = nn.Linear(5, 1, bias=False)
opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
loss_fn = nn.MSELoss()

for epoch in range(20):
    epoch_loss = 0
    for batch_x, batch_y in loader:
        pred = model(batch_x).squeeze()
        loss = loss_fn(pred, batch_y)
        opt.zero_grad(); loss.backward(); opt.step()
        epoch_loss += loss.item()
    
    if (epoch+1) % 5 == 0:
        avg_loss = epoch_loss / len(loader)
        w = model.weight.data.squeeze()
        print(f"Epoch {epoch+1:2d}: loss={avg_loss:.4f}  w={w.tolist()}")

print(f"\\nAppris: {model.weight.data.squeeze().tolist()}")
print(f"RÃ©el:   {W_true.tolist()}")`,
        solution: `import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

torch.manual_seed(42)

X = torch.randn(1000, 5)
W_true = torch.tensor([1.0, -2.0, 3.0, -1.0, 0.5])
y = X @ W_true + 0.1 * torch.randn(1000)

dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

model = nn.Linear(5, 1, bias=False)
opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
loss_fn = nn.MSELoss()

for epoch in range(20):
    epoch_loss = 0
    for batch_x, batch_y in loader:
        pred = model(batch_x).squeeze()
        loss = loss_fn(pred, batch_y)
        opt.zero_grad(); loss.backward(); opt.step()
        epoch_loss += loss.item()
    
    if (epoch+1) % 5 == 0:
        avg_loss = epoch_loss / len(loader)
        w = model.weight.data.squeeze()
        print(f"Epoch {epoch+1:2d}: loss={avg_loss:.4f}  w={w.tolist()}")

print(f"\\nAppris: {model.weight.data.squeeze().tolist()}")
print(f"RÃ©el:   {W_true.tolist()}")`,
        hints: [
          'batch_size=32 est un bon dÃ©faut',
          'DataLoader gÃ¨re le shuffling et le batching automatiquement',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn
import torch.optim as optim

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Descente de Gradient â€” Ch. 6 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

torch.manual_seed(42)

x = torch.randn(200, 1)
y = 3 * x + 2 + torch.randn(200, 1) * 0.1

# â”€â”€ 1. Les 3 optimiseurs principaux â”€â”€
model = nn.Linear(1, 1)
print("â•â•â• Optimiseurs PyTorch â•â•â•")

optimizers = {
    'SGD':      optim.SGD(model.parameters(), lr=0.01),
    'Momentum': optim.SGD(model.parameters(), lr=0.01, momentum=0.9),
    'Adam':     optim.Adam(model.parameters(), lr=0.001),
}

for name, opt in optimizers.items():
    print(f"  {name}: {opt.__class__.__name__}")

# â”€â”€ 2. Boucle d'entraÃ®nement typique â”€â”€
print("\\nâ•â•â• EntraÃ®nement avec Adam â•â•â•")
model = nn.Linear(1, 1)
opt = optim.Adam(model.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

for epoch in range(200):
    pred = model(x)
    loss = loss_fn(pred, y)
    opt.zero_grad()
    loss.backward()
    opt.step()
    
    if (epoch + 1) % 40 == 0:
        w, b = model.weight.item(), model.bias.item()
        print(f"  Epoch {epoch+1:3d}: loss={loss.item():.4f}, y={w:.3f}x + {b:.3f}")

w, b = model.weight.item(), model.bias.item()
print(f"\\nâœ“ Appris : y = {w:.2f}x + {b:.2f}")
print(f"  RÃ©el  : y = 3.00x + 2.00")

# â”€â”€ 3. Learning Rate Schedule â”€â”€
print("\\nâ•â•â• Learning Rate Schedule â•â•â•")
model2 = nn.Linear(1, 1)
opt2 = optim.Adam(model2.parameters(), lr=0.01)
scheduler = optim.lr_scheduler.StepLR(opt2, step_size=50, gamma=0.5)
for epoch in range(200):
    loss = loss_fn(model2(x), y)
    opt2.zero_grad(); loss.backward(); opt2.step()
    scheduler.step()
    if (epoch+1) % 50 == 0:
        print(f"  Epoch {epoch+1}: lr = {scheduler.get_last_lr()[0]:.6f}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 7 â€” BACKPROPAGATION (Ch. 7)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'backprop',
    title: 'Backpropagation & Autograd',
    shortTitle: 'Backprop',
    description: 'RÃ¨gle de la chaÃ®ne, forward/backward pass, He initialization, vanishing gradients (Ch. 7 â€” UDL).',
    status: 'locked',
    progress: 0,
    dependencies: ['gradient-descent'],
    category: 'training',
    theory: [
      {
        type: 'text',
        content: `## 7.1 â€” Forward pass & Backward pass\n\nLa **backpropagation** calcule efficacement les gradients âˆ‚L/âˆ‚Ï• pour tous les paramÃ¨tres du rÃ©seau. Le processus :\n\n1. **Forward pass** : on calcule sÃ©quentiellement fâ‚€ â†’ hâ‚ â†’ fâ‚ â†’ hâ‚‚ â†’ ... â†’ fâ‚– â†’ perte â„“\n2. **Backward pass** : on propage âˆ‚â„“/âˆ‚fâ‚– de la sortie vers l'entrÃ©e via la **rÃ¨gle de la chaÃ®ne**`,
      },
      {
        type: 'diagram',
        content: `  FORWARD  â†’  â†’  â†’  â†’  â†’  â†’  â†’  â†’  â†’  â†’  â†’  â†’
  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  x â”€â”€â–¶ fâ‚€ â”€â”€â–¶ hâ‚ â”€â”€â–¶ fâ‚ â”€â”€â–¶ hâ‚‚ â”€â”€â–¶ fâ‚‚ â”€â”€â–¶ â„“
        Î²â‚€,Î©â‚€  ReLU   Î²â‚,Î©â‚  ReLU   Î²â‚‚,Î©â‚‚
  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  â†  â†  â†  â†  â†  â†  â†  â†  â†  â†  â†  â†  BACKWARD
  âˆ‚â„“/âˆ‚x  âˆ‚â„“/âˆ‚fâ‚€  âˆ‚â„“/âˆ‚hâ‚  âˆ‚â„“/âˆ‚fâ‚  âˆ‚â„“/âˆ‚hâ‚‚  âˆ‚â„“/âˆ‚fâ‚‚

  â†’ Chaque âˆ‚â„“/âˆ‚fâ‚– donne âˆ‚â„“/âˆ‚Î²â‚– et âˆ‚â„“/âˆ‚Î©â‚–`,
        label: 'Fig. 7.3 â€” Forward (â†’) et Backward (â†)',
      },
      {
        type: 'text',
        content: `## 7.2 â€” RÃ¨gle de la chaÃ®ne\n\nConsidÃ©rons un toy example :\n\ny = cos(sin(x) + exp(x))\n\nLe forward dÃ©compose en Ã©tapes : f = sin(x), g = exp(x), h = f + g, y = cos(h).\nLe backward applique dy/dx = dy/dh Â· dh/df Â· df/dx + dy/dh Â· dh/dg Â· dg/dx.`,
      },
      {
        type: 'equation',
        content: '\\frac{\\partial \\ell}{\\partial \\mathbf{f}_k} = \\underbrace{\\frac{\\partial \\mathbf{h}_{k+1}}{\\partial \\mathbf{f}_k}}_{\\text{Jacobien ReLU}} \\cdot \\underbrace{\\frac{\\partial \\mathbf{f}_{k+1}}{\\partial \\mathbf{h}_{k+1}}}_{= \\boldsymbol{\\Omega}_{k+1}} \\cdot \\underbrace{\\frac{\\partial \\ell}{\\partial \\mathbf{f}_{k+1}}}_{\\text{rÃ©cursion}}',
        label: 'Ã‰q. 7.13 â€” RÃ©cursion backward (couche k)',
        highlightVar: 'grad',
      },
      {
        type: 'text',
        content: `## 7.3 â€” Gradients par rapport aux paramÃ¨tres\n\nUne fois âˆ‚â„“/âˆ‚fâ‚– calculÃ© (via la rÃ©cursion backward), on obtient directement les gradients pour les poids et biais de la couche k :`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}_k} &= \\frac{\\partial \\ell}{\\partial \\mathbf{f}_k} \\\\[6pt] \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\Omega}_k} &= \\frac{\\partial \\ell}{\\partial \\mathbf{f}_k} \\cdot \\mathbf{h}_k^T \\end{aligned}',
        label: 'Ã‰q. 7.24â€“7.25 â€” Gradients des paramÃ¨tres',
        highlightVar: 'grad',
      },
      {
        type: 'text',
        content: `## 7.4 â€” DÃ©rivÃ©e du ReLU\n\nLa dÃ©rivÃ©e de ReLU est triviale : elle crÃ©e une matrice diagonale avec 1 pour les entrÃ©es positives et 0 pour les nÃ©gatives. C'est ce qui rend le backprop avec ReLU trÃ¨s rapide :`,
      },
      {
        type: 'equation',
        content: '\\frac{\\partial \\, \\text{ReLU}(z)}{\\partial z} = \\begin{cases} 0 & z < 0 \\\\ 1 & z > 0 \\end{cases} \\qquad \\Rightarrow \\qquad \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{f}} = \\text{diag}\\left[\\mathbb{I}[f_d > 0]\\right]',
        label: 'Ã‰q. 7.15 â€” Jacobien du ReLU',
      },
      {
        type: 'text',
        content: `## 7.5 â€” Algorithme complet de backpropagation\n\n**Forward pass** : stocker toutes les prÃ©-activations fâ‚– et activations hâ‚–\n**Backward pass** :\n1. Calculer âˆ‚â„“/âˆ‚fâ‚– pour K (derniÃ¨re couche)\n2. Pour k = Kâˆ’1, ..., 0 : propager âˆ‚â„“/âˆ‚fâ‚–\n3. Ã€ chaque couche : extraire âˆ‚â„“/âˆ‚Î²â‚– et âˆ‚â„“/âˆ‚Î©â‚–\n\n**ComplexitÃ©** : O(paramÃ¨tres) â€” identique au forward pass !`,
      },
      {
        type: 'callout',
        content: 'âš¡ **Autograd** de PyTorch implÃ©mente la **diffÃ©rentiation algorithmique** (reverse mode). Le graphe de calcul est construit automatiquement pendant le forward. Un seul appel Ã  \\`loss.backward()\\` calcule TOUS les gradients. C\'est la version automatique de la backpropagation.',
      },
      {
        type: 'text',
        content: `## 7.6 â€” Initialisation de He\n\nSi les poids sont trop grands â†’ les gradients **explosent**. Trop petits â†’ ils **s'Ã©vanouissent**. L'initialisation de **He** (2015) choisit la variance des poids pour que les activations ReLU gardent une variance constante :`,
      },
      {
        type: 'equation',
        content: '\\sigma^2 = \\frac{2}{D_h} \\qquad \\Rightarrow \\qquad \\omega_{ij} \\sim \\mathcal{N}\\!\\left(0, \\frac{2}{D_h}\\right)',
        label: 'Ã‰q. 7.40 â€” Initialisation de He (pour ReLU)',
        highlightVar: 'hidden',
      },
      {
        type: 'diagram',
        content: `  Sans bonne init                Avec He init
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Couche 1:  Ïƒ = 1.0            Couche 1:  Ïƒ = 0.71
  Couche 5:  Ïƒ = 0.001          Couche 5:  Ïƒ = 0.68
  Couche 10: Ïƒ = 0.000001       Couche 10: Ïƒ = 0.65
  â†’ Gradients Ã‰VANOUISSENT !    â†’ Gradients STABLES âœ“
  
  Formule : ÏƒÂ² = 2/Dâ‚• (facteur 2 pour ReLU qui clippe 50%)
  PyTorch : nn.init.kaiming_normal_(w, nonlinearity='relu')`,
        label: 'Fig. 7.8 â€” Variance des activations avec/sans He init',
      },
      {
        type: 'callout',
        content: 'ğŸ§  **RÃ©sumÃ© Ch. 7** :\n(1) Forward = calculer et stocker les activations couche par couche\n(2) Backward = propager âˆ‚â„“/âˆ‚f de la sortie vers l\'entrÃ©e via la rÃ¨gle de la chaÃ®ne\n(3) ComplexitÃ© backward = complexitÃ© forward (remarquable !)\n(4) PyTorch Autograd fait tout automatiquement\n(5) He init : ÏƒÂ² = 2/Dâ‚• pour stabiliser les gradients profonds',
      },
    ],
    exercises: [
      {
        id: 'bp-ex1',
        title: 'ğŸ’» Pratique â€” Autograd en action',
        instructions: 'Utilisez PyTorch Autograd pour calculer les gradients d\'une expression y = w*x + b, loss = (y-10)Â². VÃ©rifiez manuellement avec la rÃ¨gle de la chaÃ®ne.',
        starterCode: `import torch

x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(3.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

# Forward : y = w*x + b, loss = (y - 10)Â²
y = ___
loss = ___

print(f"y = {y.item():.2f}")
print(f"loss = {loss.item():.2f}")

# Backward
loss.backward()

print(f"\\nâˆ‚loss/âˆ‚w = {w.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚x = {x.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚b = {b.grad.item():.2f}")

# VÃ©rification manuelle : âˆ‚loss/âˆ‚w = 2(y-10) * âˆ‚y/âˆ‚w = 2(y-10) * x
print(f"\\nVÃ©rification: 2*(y-10)*x = {2*(y.item()-10)*x.item():.2f}")`,
        solution: `import torch

x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(3.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

y = w * x + b
loss = (y - 10) ** 2

print(f"y = {y.item():.2f}")
print(f"loss = {loss.item():.2f}")

loss.backward()

print(f"\\nâˆ‚loss/âˆ‚w = {w.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚x = {x.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚b = {b.grad.item():.2f}")

print(f"\\nVÃ©rification: 2*(y-10)*x = {2*(y.item()-10)*x.item():.2f}")`,
        hints: [
          'y = w * x + b',
          'loss = (y - 10) ** 2',
          'loss.backward() calcule tous les gradients automatiquement',
        ],
        completed: false,
      },
      {
        id: 'bp-th1',
        title: 'ğŸ§  ThÃ©orie â€” Backprop manuelle (toy example)',
        instructions: 'ImplÃ©mentez le forward et backward pass manuellement (sans autograd) pour un rÃ©seau Ã  1 couche cachÃ©e. VÃ©rifiez contre PyTorch.',
        starterCode: `import torch
import torch.nn as nn

torch.manual_seed(0)

# RÃ©seau : x(1) â†’ h(3) â†’ y(1), ReLU, loss = MSE
D_i, D_h, D_o = 1, 3, 1

# ParamÃ¨tres
W1 = torch.randn(D_h, D_i, requires_grad=True)  # (3,1)
b1 = torch.randn(D_h, requires_grad=True)        # (3,)
W2 = torch.randn(D_o, D_h, requires_grad=True)   # (1,3)
b2 = torch.randn(D_o, requires_grad=True)         # (1,)

x = torch.tensor([[1.5]])  # (1,1)
y_true = torch.tensor([[2.0]])

# â”€â”€ Forward pass (manuel) â”€â”€
f0 = (W1 @ x.T).squeeze() + b1   # prÃ©-activation
h1 = torch.relu(f0)               # activation
f1 = (W2 @ h1.unsqueeze(1)).squeeze() + b2  # sortie
loss = (f1 - y_true.squeeze()) ** 2

print(f"f0 = {f0.detach().tolist()}")
print(f"h1 = {h1.detach().tolist()}")
print(f"f1 = {f1.detach().item():.4f}")
print(f"loss = {loss.detach().item():.4f}")

# â”€â”€ Backward pass (manuel) â”€â”€
dl_df1 = 2 * (f1 - y_true.squeeze())        # âˆ‚L/âˆ‚fâ‚
dl_dW2 = ___  # dl_df1 * h1áµ€
dl_db2 = ___  # dl_df1
dl_dh1 = ___  # W2áµ€ * dl_df1
dl_df0 = dl_dh1.squeeze() * (f0 > 0).float()  # ReLU mask
dl_dW1 = dl_df0.unsqueeze(1) @ x  # âˆ‚L/âˆ‚Wâ‚
dl_db1 = dl_df0                    # âˆ‚L/âˆ‚bâ‚

# VÃ©rifier avec autograd
loss.backward()
print(f"\\n{'Param':>6} â”‚ {'Manuel':>10} â”‚ {'Autograd':>10} â”‚ {'Match':>5}")
print(f"{'â”€'*6}â”€â”¼â”€{'â”€'*10}â”€â”¼â”€{'â”€'*10}â”€â”¼â”€{'â”€'*5}")
for name, manual, auto in [('W2', dl_dW2, W2.grad),
                             ('b2', dl_db2, b2.grad),
                             ('W1', dl_dW1, W1.grad),
                             ('b1', dl_db1, b1.grad)]:
    m = manual.detach().flatten()
    a = auto.flatten()
    match = torch.allclose(m, a, atol=1e-5)
    print(f"{name:>6} â”‚ {m.tolist()} â”‚ {a.tolist()} â”‚ {'âœ“' if match else 'âœ—'}")`,
        solution: `import torch
import torch.nn as nn

torch.manual_seed(0)

D_i, D_h, D_o = 1, 3, 1

W1 = torch.randn(D_h, D_i, requires_grad=True)
b1 = torch.randn(D_h, requires_grad=True)
W2 = torch.randn(D_o, D_h, requires_grad=True)
b2 = torch.randn(D_o, requires_grad=True)

x = torch.tensor([[1.5]])
y_true = torch.tensor([[2.0]])

f0 = (W1 @ x.T).squeeze() + b1
h1 = torch.relu(f0)
f1 = (W2 @ h1.unsqueeze(1)).squeeze() + b2
loss = (f1 - y_true.squeeze()) ** 2

print(f"f0 = {f0.detach().tolist()}")
print(f"h1 = {h1.detach().tolist()}")
print(f"f1 = {f1.detach().item():.4f}")
print(f"loss = {loss.detach().item():.4f}")

dl_df1 = 2 * (f1 - y_true.squeeze())
dl_dW2 = dl_df1 * h1.unsqueeze(0)
dl_db2 = dl_df1
dl_dh1 = W2.T * dl_df1
dl_df0 = dl_dh1.squeeze() * (f0 > 0).float()
dl_dW1 = dl_df0.unsqueeze(1) @ x
dl_db1 = dl_df0

loss.backward()
print(f"\\n{'Param':>6} â”‚ {'Manuel':>10} â”‚ {'Autograd':>10} â”‚ {'Match':>5}")
print(f"{'â”€'*6}â”€â”¼â”€{'â”€'*10}â”€â”¼â”€{'â”€'*10}â”€â”¼â”€{'â”€'*5}")
for name, manual, auto in [('W2', dl_dW2, W2.grad),
                             ('b2', dl_db2, b2.grad),
                             ('W1', dl_dW1, W1.grad),
                             ('b1', dl_db1, b1.grad)]:
    m = manual.detach().flatten()
    a = auto.flatten()
    match = torch.allclose(m, a, atol=1e-5)
    print(f"{name:>6} â”‚ {m.tolist()} â”‚ {a.tolist()} â”‚ {'âœ“' if match else 'âœ—'}")`,
        hints: [
          'âˆ‚L/âˆ‚Wâ‚‚ = âˆ‚L/âˆ‚fâ‚ Â· hâ‚áµ€ (produit extÃ©rieur)',
          'âˆ‚L/âˆ‚hâ‚ = Wâ‚‚áµ€ Â· âˆ‚L/âˆ‚fâ‚',
          'âˆ‚L/âˆ‚fâ‚€ = âˆ‚L/âˆ‚hâ‚ Â· I[fâ‚€>0] (masque ReLU)',
        ],
        completed: false,
      },
      {
        id: 'bp-pr2',
        title: 'ğŸ’» Pratique â€” He init vs mauvaise init',
        instructions: 'Comparez l\'entraÃ®nement d\'un rÃ©seau profond (10 couches) avec initialisation standard vs He. Observez les gradients.',
        starterCode: `import torch
import torch.nn as nn

torch.manual_seed(42)

def make_deep_net(init_type='default'):
    layers = []
    dims = [1] + [50]*10 + [1]
    for i in range(len(dims)-1):
        layer = nn.Linear(dims[i], dims[i+1])
        if init_type == 'he':
            nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')
        elif init_type == 'small':
            nn.init.normal_(layer.weight, std=0.01)
        layers.append(layer)
        if i < len(dims) - 2:
            layers.append(nn.ReLU())
    return nn.Sequential(*layers)

x = torch.randn(32, 1)
y = torch.sin(x)

for init in ['small', 'default', 'he']:
    model = make_deep_net(init)
    loss = nn.MSELoss()(model(x), y)
    loss.backward()
    
    # Gradient de la PREMIÃˆRE couche
    grad_norm = model[0].weight.grad.norm().item()
    # Activation de la DERNIÃˆRE couche
    out_std = model(x).std().item()
    
    print(f"Init {init:>8s}: grad_norm_L0 = {grad_norm:.6f}, out_std = {out_std:.4f}")

print(f"\\nâ†’ 'he' garde les gradients ni trop grands ni trop petits !")`,
        solution: `import torch
import torch.nn as nn

torch.manual_seed(42)

def make_deep_net(init_type='default'):
    layers = []
    dims = [1] + [50]*10 + [1]
    for i in range(len(dims)-1):
        layer = nn.Linear(dims[i], dims[i+1])
        if init_type == 'he':
            nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')
        elif init_type == 'small':
            nn.init.normal_(layer.weight, std=0.01)
        layers.append(layer)
        if i < len(dims) - 2:
            layers.append(nn.ReLU())
    return nn.Sequential(*layers)

x = torch.randn(32, 1)
y = torch.sin(x)

for init in ['small', 'default', 'he']:
    model = make_deep_net(init)
    loss = nn.MSELoss()(model(x), y)
    loss.backward()
    
    grad_norm = model[0].weight.grad.norm().item()
    out_std = model(x).std().item()
    
    print(f"Init {init:>8s}: grad_norm_L0 = {grad_norm:.6f}, out_std = {out_std:.4f}")

print(f"\\nâ†’ 'he' garde les gradients ni trop grands ni trop petits !")`,
        hints: [
          'nn.init.kaiming_normal_ implÃ©mente He init',
          'Avec std=0.01 les gradients seront quasi-nuls (vanishing)',
          'He init utilise ÏƒÂ² = 2/fan_in pour ReLU',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Backpropagation â€” Ch. 7 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. Autograd simple â”€â”€
print("â•â•â• Autograd â•â•â•")
x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(3.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

y = w * x + b
loss = (y - 10) ** 2

loss.backward()
print(f"  y = {y.item():.2f}, loss = {loss.item():.2f}")
print(f"  âˆ‚loss/âˆ‚w = {w.grad.item():.2f}")
print(f"  âˆ‚loss/âˆ‚x = {x.grad.item():.2f}")
print(f"  âˆ‚loss/âˆ‚b = {b.grad.item():.2f}")

# â”€â”€ 2. Graphe de calcul complexe â”€â”€
print("\\nâ•â•â• Graphe complexe â•â•â•")
a = torch.tensor(1.5, requires_grad=True)
b2 = torch.tensor(2.0, requires_grad=True)

c = a * b2
d = torch.relu(c - 2.5)
e = d ** 2

e.backward()
print(f"  e = {e.item():.4f}")
print(f"  âˆ‚e/âˆ‚a = {a.grad.item():.4f}")
print(f"  âˆ‚e/âˆ‚b = {b2.grad.item():.4f}")

# â”€â”€ 3. He Initialization â”€â”€
print("\\nâ•â•â• He Init â•â•â•")
D = 256
layer = nn.Linear(D, D)
nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')
print(f"  E[wÂ²] = {(layer.weight**2).mean().item():.6f}")
print(f"  2/D   = {2/D:.6f}")
print(f"  â†’ {'âœ“ Match !' if abs((layer.weight**2).mean().item() - 2/D) < 0.01 else 'âœ—'}")

# â”€â”€ 4. Gradient flow dans un rÃ©seau profond â”€â”€
print("\\nâ•â•â• Gradient Flow (10 couches) â•â•â•")
model = nn.Sequential(*[
    layer for D in [1] + [50]*10 + [1]
    for layer in [nn.Linear(D, 50), nn.ReLU()]
][:-1])
# He init
for m in model.modules():
    if isinstance(m, nn.Linear):
        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')

x = torch.randn(32, 1)
out = model(x)
loss = out.mean()
loss.backward()

for i, m in enumerate(model):
    if isinstance(m, nn.Linear):
        print(f"  Layer {i}: grad_norm = {m.weight.grad.norm().item():.6f}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 8 â€” RÃ‰GULARISATION (Ch. 9)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'regularization',
    title: 'RÃ©gularisation & GÃ©nÃ©ralisation',
    shortTitle: 'RÃ©gular.',
    description: 'Biais-Variance, L2/Weight Decay, Dropout, BatchNorm, Early Stopping, Data Augmentation (Ch. 9 â€” UDL).',
    status: 'locked',
    progress: 0,
    dependencies: ['backprop'],
    category: 'training',
    theory: [
      {
        type: 'text',
        content: `## 9.1 â€” Biais, Variance et Bruit\n\nL'erreur de gÃ©nÃ©ralisation se dÃ©compose en trois termes :\n\n- **Biais** : erreur due Ã  un modÃ¨le trop simple (under-fitting)\n- **Variance** : sensibilitÃ© aux donnÃ©es d'entraÃ®nement (over-fitting)\n- **Bruit** : erreur irrÃ©ductible dans les donnÃ©es\n\nUn modÃ¨le trop simple a un **biais Ã©levÃ©**. Un modÃ¨le trop complexe a une **variance Ã©levÃ©e**. Le but est de trouver le juste milieu.`,
      },
      {
        type: 'diagram',
        content: `  Erreur â†‘
    â”‚\\
    â”‚ \\  Biais
    â”‚  \\          â•± Variance
    â”‚   \\       â•±
    â”‚    \\    â•±
    â”‚     \\â•±â”€â”€â”€â”€ Erreur totale
    â”‚     â•±\\
    â”‚   â•±   \\
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ComplexitÃ© du modÃ¨le â†’
      Simple              Complexe
    (underfitting)      (overfitting)
                   â†‘
            Sweet spot !`,
        label: 'Fig. 9.2 â€” Compromis Biais-Variance',
      },
      {
        type: 'text',
        content: `## 9.2 â€” RÃ©gularisation L2 (Weight Decay)\n\nOn ajoute un terme qui pÃ©nalise les **poids trop grands**. Cela force le modÃ¨le Ã  utiliser des poids plus petits â†’ solutions plus lisses â†’ meilleure gÃ©nÃ©ralisation. En pratique, c'est le paramÃ¨tre \`weight_decay\` de l'optimiseur :`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}_{\\text{reg}} = \\underbrace{\\mathcal{L}_{\\text{data}}}_{\\text{MSE ou CE}} + \\underbrace{\\frac{\\lambda}{2} \\| \\boldsymbol{\\phi} \\|_2^2}_{\\text{pÃ©nalitÃ© L2}}',
        label: 'Ã‰q. 9.3 â€” RÃ©gularisation L2',
        highlightVar: 'loss',
      },
      {
        type: 'text',
        content: `## 9.3 â€” Dropout\n\nPendant l'entraÃ®nement, on **dÃ©sactive alÃ©atoirement** une fraction p des neurones. Cela force le rÃ©seau Ã  ne pas dÃ©pendre d'un neurone unique â†’ Ã©quivalent approximatif d'un **ensemble** de sous-rÃ©seaux.\n\nÃ€ l'infÃ©rence (\`model.eval()\`), tous les neurones sont actifs mais multipliÃ©s par (1âˆ’p) pour compenser.`,
      },
      {
        type: 'equation',
        content: '\\tilde{h}_d = h_d \\cdot m_d \\quad \\text{oÃ¹ } m_d \\sim \\text{Bernoulli}(1-p)',
        label: 'Ã‰q. 9.14 â€” Dropout mask',
      },
      {
        type: 'text',
        content: `## 9.4 â€” Batch Normalization\n\nNormalise les activations de chaque couche pour avoir **Î¼=0, Ïƒ=1** sur le mini-batch, puis applique un rescaling appris Î³,Î². Agit comme rÃ©gularisateur ET accÃ©lÃ©rateur :`,
      },
      {
        type: 'equation',
        content: '\\hat{h}_d = \\gamma_d \\cdot \\frac{h_d - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^2 + \\epsilon}} + \\beta_d',
        label: 'Ã‰q. â€” Batch Normalization',
      },
      {
        type: 'text',
        content: `## 9.5 â€” Autres techniques\n\n- **Early Stopping** : surveiller la loss de validation, arrÃªter quand elle augmente\n- **Data Augmentation** : crÃ©er des exemples artificiels (rotations, flips, crops, color jitter)\n- **Label Smoothing** : remplacer les labels one-hot par des soft labels (1 â†’ 0.9, 0 â†’ 0.1/K)\n- **Weight Noise / Gradient Noise** : ajouter du bruit pendant l'entraÃ®nement`,
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ **Recette anti-overfitting** :\n1. Plus de **donnÃ©es** (data augmentation)\n2. **Weight Decay** (Î» = 1e-4 ou 1e-5)\n3. **Dropout** (p = 0.1 Ã  0.5)\n4. **Early Stopping** (patience 5-10 epochs)\n5. **Batch Norm** (accÃ©lÃ¨re + rÃ©gularise)\n6. RÃ©duire la **taille du modÃ¨le** (dernier recours)',
      },
    ],
    exercises: [
      {
        id: 'reg-ex1',
        title: 'ğŸ’» Pratique â€” Dropout train vs eval',
        instructions: 'Montrez la diffÃ©rence entre model.train() et model.eval() avec Dropout. VÃ©rifiez que les sorties sont stochastiques en train mode et dÃ©terministes en eval mode.',
        starterCode: `import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(10, 50), nn.ReLU(), nn.Dropout(0.5),
    nn.Linear(50, 50), nn.ReLU(), nn.Dropout(0.5),
    nn.Linear(50, 1)
)

x = torch.randn(1, 10)

# Train mode â†’ dropout actif â†’ sorties diffÃ©rentes
model.train()
out1 = model(x).item()
out2 = model(x).item()
print(f"Train mode: {out1:.4f} vs {out2:.4f} â†’ {'DiffÃ©rents âœ“' if abs(out1-out2) > 1e-6 else 'ERREUR'}")

# Eval mode â†’ dropout dÃ©sactivÃ© â†’ sorties identiques
model.___()  # passer en eval
out3 = model(x).item()
out4 = model(x).item()
print(f"Eval mode:  {out3:.4f} vs {out4:.4f} â†’ {'Identiques âœ“' if abs(out3-out4) < 1e-6 else 'ERREUR'}")`,
        solution: `import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(10, 50), nn.ReLU(), nn.Dropout(0.5),
    nn.Linear(50, 50), nn.ReLU(), nn.Dropout(0.5),
    nn.Linear(50, 1)
)

x = torch.randn(1, 10)

model.train()
out1 = model(x).item()
out2 = model(x).item()
print(f"Train mode: {out1:.4f} vs {out2:.4f} â†’ {'DiffÃ©rents âœ“' if abs(out1-out2) > 1e-6 else 'ERREUR'}")

model.eval()
out3 = model(x).item()
out4 = model(x).item()
print(f"Eval mode:  {out3:.4f} vs {out4:.4f} â†’ {'Identiques âœ“' if abs(out3-out4) < 1e-6 else 'ERREUR'}")`,
        hints: [
          'model.eval() dÃ©sactive le dropout et la batch norm',
          'model.train() les rÃ©active',
        ],
        completed: false,
      },
      {
        id: 'reg-ex2',
        title: 'ğŸ’» Pratique â€” Overfitting vs RÃ©gularisation',
        instructions: 'EntraÃ®nez un modÃ¨le sur peu de donnÃ©es avec et sans rÃ©gularisation. Observez l\'overfitting et l\'effet de weight decay + dropout.',
        starterCode: `import torch
import torch.nn as nn
import torch.optim as optim

torch.manual_seed(42)

# Peu de donnÃ©es (overfitting garanti !)
x_train = torch.randn(20, 1)
y_train = torch.sin(x_train) + 0.1 * torch.randn(20, 1)
x_test = torch.linspace(-4, 4, 100).unsqueeze(1)
y_test = torch.sin(x_test)

# ModÃ¨le SANS rÃ©gularisation
model_noreg = nn.Sequential(
    nn.Linear(1, 100), nn.ReLU(),
    nn.Linear(100, 100), nn.ReLU(),
    nn.Linear(100, 1)
)

# ModÃ¨le AVEC rÃ©gularisation
model_reg = nn.Sequential(
    nn.Linear(1, 100), nn.ReLU(), nn.Dropout(0.3),
    nn.Linear(100, 100), nn.ReLU(), nn.Dropout(0.3),
    nn.Linear(100, 1)
)

opt_noreg = optim.Adam(model_noreg.parameters(), lr=0.01)
opt_reg = optim.Adam(model_reg.parameters(), lr=0.01, weight_decay=___)

loss_fn = nn.MSELoss()

for epoch in range(500):
    for model, opt in [(model_noreg, opt_noreg), (model_reg, opt_reg)]:
        model.train()
        loss = loss_fn(model(x_train), y_train)
        opt.zero_grad(); loss.backward(); opt.step()

# Ã‰valuation
model_noreg.eval(); model_reg.eval()
test_loss_noreg = loss_fn(model_noreg(x_test), y_test).item()
test_loss_reg = loss_fn(model_reg(x_test), y_test).item()
train_loss_noreg = loss_fn(model_noreg(x_train), y_train).item()
train_loss_reg = loss_fn(model_reg(x_train), y_train).item()

print(f"{'':>10} â”‚ {'Train':>8} â”‚ {'Test':>8} â”‚ {'Gap':>8}")
print(f"{'â”€'*10}â”€â”¼â”€{'â”€'*8}â”€â”¼â”€{'â”€'*8}â”€â”¼â”€{'â”€'*8}")
print(f"{'No reg':>10} â”‚ {train_loss_noreg:8.4f} â”‚ {test_loss_noreg:8.4f} â”‚ {test_loss_noreg-train_loss_noreg:8.4f}")
print(f"{'Dropout+WD':>10} â”‚ {train_loss_reg:8.4f} â”‚ {test_loss_reg:8.4f} â”‚ {test_loss_reg-train_loss_reg:8.4f}")`,
        solution: `import torch
import torch.nn as nn
import torch.optim as optim

torch.manual_seed(42)

x_train = torch.randn(20, 1)
y_train = torch.sin(x_train) + 0.1 * torch.randn(20, 1)
x_test = torch.linspace(-4, 4, 100).unsqueeze(1)
y_test = torch.sin(x_test)

model_noreg = nn.Sequential(
    nn.Linear(1, 100), nn.ReLU(),
    nn.Linear(100, 100), nn.ReLU(),
    nn.Linear(100, 1)
)

model_reg = nn.Sequential(
    nn.Linear(1, 100), nn.ReLU(), nn.Dropout(0.3),
    nn.Linear(100, 100), nn.ReLU(), nn.Dropout(0.3),
    nn.Linear(100, 1)
)

opt_noreg = optim.Adam(model_noreg.parameters(), lr=0.01)
opt_reg = optim.Adam(model_reg.parameters(), lr=0.01, weight_decay=1e-4)

loss_fn = nn.MSELoss()

for epoch in range(500):
    for model, opt in [(model_noreg, opt_noreg), (model_reg, opt_reg)]:
        model.train()
        loss = loss_fn(model(x_train), y_train)
        opt.zero_grad(); loss.backward(); opt.step()

model_noreg.eval(); model_reg.eval()
test_loss_noreg = loss_fn(model_noreg(x_test), y_test).item()
test_loss_reg = loss_fn(model_reg(x_test), y_test).item()
train_loss_noreg = loss_fn(model_noreg(x_train), y_train).item()
train_loss_reg = loss_fn(model_reg(x_train), y_train).item()

print(f"{'':>10} â”‚ {'Train':>8} â”‚ {'Test':>8} â”‚ {'Gap':>8}")
print(f"{'â”€'*10}â”€â”¼â”€{'â”€'*8}â”€â”¼â”€{'â”€'*8}â”€â”¼â”€{'â”€'*8}")
print(f"{'No reg':>10} â”‚ {train_loss_noreg:8.4f} â”‚ {test_loss_noreg:8.4f} â”‚ {test_loss_noreg-train_loss_noreg:8.4f}")
print(f"{'Dropout+WD':>10} â”‚ {train_loss_reg:8.4f} â”‚ {test_loss_reg:8.4f} â”‚ {test_loss_reg-train_loss_reg:8.4f}")`,
        hints: [
          'weight_decay=1e-4 est une bonne valeur par dÃ©faut',
          'Le "gap" train-test mesure l\'overfitting',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RÃ©gularisation â€” Ch. 9 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. Weight Decay (L2) â”€â”€
model = nn.Sequential(nn.Linear(10, 50), nn.ReLU(), nn.Linear(50, 1))
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)
print("âœ“ Weight decay Î»=0.01")

# â”€â”€ 2. Dropout â”€â”€
model_drop = nn.Sequential(
    nn.Linear(10, 50), nn.ReLU(), nn.Dropout(p=0.3),
    nn.Linear(50, 50), nn.ReLU(), nn.Dropout(p=0.3),
    nn.Linear(50, 1)
)

x = torch.randn(1, 10)
model_drop.train()
print(f"\\nTrain: {model_drop(x).item():.4f} vs {model_drop(x).item():.4f} (diffÃ©rents)")
model_drop.eval()
print(f"Eval:  {model_drop(x).item():.4f} vs {model_drop(x).item():.4f} (identiques)")

# â”€â”€ 3. Batch Normalization â”€â”€
bn_model = nn.Sequential(
    nn.Linear(10, 50), nn.BatchNorm1d(50), nn.ReLU(),
    nn.Linear(50, 1)
)
print(f"\\nâœ“ BatchNorm model: {sum(p.numel() for p in bn_model.parameters())} params")

# â”€â”€ 4. Ensemble des techniques â”€â”€
full_model = nn.Sequential(
    nn.Linear(10, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.2),
    nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.2),
    nn.Linear(64, 1)
)
print(f"âœ“ Full model: {sum(p.numel() for p in full_model.parameters())} params")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 9 â€” CNN (Ch. 10)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'cnn',
    title: 'RÃ©seaux Convolutifs (CNN)',
    shortTitle: 'CNN',
    description: 'Convolutions 2D, invariance/Ã©quivariance, pooling, architectures classiques (Ch. 10 â€” UDL).',
    status: 'locked',
    progress: 0,
    dependencies: ['regularization'],
    category: 'architectures',
    theory: [
      {
        type: 'text',
        content: `## 10.1 â€” Invariance et Ã‰quivariance\n\nLes images ont une structure spatiale : les mÃªmes patterns (bords, textures) apparaissent partout. Deux concepts formalisent cette idÃ©e :\n\n- **Invariance** : f[t[x]] = f[x] â€” la sortie ne change pas (classification : "c'est un chat" peu importe la position)\n- **Ã‰quivariance** : f[t[x]] = t[f[x]] â€” la sortie se transforme identiquement (dÃ©tection d'objets : bouger l'objet â†’ bouger la boÃ®te)`,
      },
      {
        type: 'text',
        content: `## 10.2 â€” Convolution 1D et 2D\n\nLa **convolution** applique un filtre (kernel) qui **glisse** sur l'entrÃ©e. Les mÃªmes poids sont partagÃ©s partout â†’ **Ã©quivariance Ã  la translation** et rÃ©duction massive du nombre de paramÃ¨tres. La convolution 2D est le cÅ“ur des CNN :`,
      },
      {
        type: 'equation',
        content: 'z_{ij} = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} \\omega_{mn} \\cdot x_{i+m, \\, j+n} + b',
        label: 'Ã‰q. 10.3 â€” Convolution 2D (kernel MÃ—N)',
      },
      {
        type: 'diagram',
        content: `  Input (5Ã—5)          Kernel (3Ã—3)       Output (3Ã—3)
  â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”         â”Œâ”€â”¬â”€â”¬â”€â”
  â”‚Â·â”‚Â·â”‚Â·â”‚ â”‚ â”‚         â”‚1â”‚0â”‚1â”‚           â”Œâ”€â”¬â”€â”¬â”€â”
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤    âˆ—    â”œâ”€â”¼â”€â”¼â”€â”¤    =     â”‚ â”‚ â”‚ â”‚
  â”‚Â·â”‚Â·â”‚Â·â”‚ â”‚ â”‚         â”‚0â”‚1â”‚0â”‚           â”œâ”€â”¼â”€â”¼â”€â”¤
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤         â”œâ”€â”¼â”€â”¼â”€â”¤           â”‚ â”‚ â”‚ â”‚
  â”‚Â·â”‚Â·â”‚Â·â”‚ â”‚ â”‚         â”‚1â”‚0â”‚1â”‚           â”œâ”€â”¼â”€â”¼â”€â”¤
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤         â””â”€â”´â”€â”´â”€â”˜           â”‚ â”‚ â”‚ â”‚
  â”‚ â”‚ â”‚ â”‚ â”‚ â”‚    â† 9 poids partagÃ©s     â””â”€â”´â”€â”´â”€â”˜
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤      partout !
  â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
  â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜    padding='same' â†’ taille conservÃ©e`,
        label: 'Fig. 10.5 â€” Convolution 2D sliding window',
      },
      {
        type: 'text',
        content: `## 10.3 â€” Canaux, Stride, Padding\n\n- **Canaux d'entrÃ©e Cáµ¢â‚™** : image RGB â†’ 3 canaux. Le kernel est 3D : (Cáµ¢â‚™ Ã— K Ã— K)\n- **Canaux de sortie Câ‚’áµ¤â‚œ** : nombre de filtres â†’ feature maps. ParamÃ¨tres = Câ‚’áµ¤â‚œ Ã— Cáµ¢â‚™ Ã— K Ã— K\n- **Stride** : pas du glissement (stride=2 â†’ divise la taille par 2)\n- **Padding** : ajout de zÃ©ros autour pour contrÃ´ler la taille de sortie`,
      },
      {
        type: 'equation',
        content: 'H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2p - k}{s} \\right\\rfloor + 1',
        label: 'Taille de sortie d\'une convolution',
      },
      {
        type: 'text',
        content: `## 10.4 â€” Pooling\n\n**Max Pooling** : prend le maximum dans chaque fenÃªtre â†’ rÃ©duit la rÃ©solution + ajoute une petite **invariance Ã  la translation**.\n**Average Pooling** : prend la moyenne.\n**Global Average Pooling** : une seule valeur par channel â†’ remplace le Flatten+FC final.`,
      },
      {
        type: 'text',
        content: `## 10.5 â€” Architecture CNN typique\n\nEmpiler : [Conv â†’ BatchNorm â†’ ReLU â†’ Pool]Ã—N â†’ Flatten â†’ FC â†’ Sortie\n\n- Les premiÃ¨res couches dÃ©tectent des **features bas-niveau** (bords, coins)\n- Les couches profondes combinent en **features haut-niveau** (textures, objets)\n- En augmentant les canaux et rÃ©duisant la rÃ©solution spatiale :`,
      },
      {
        type: 'diagram',
        content: `  Input    Conv1+Pool  Conv2+Pool  Conv3+Pool  FC
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  28Ã—28Ã—1  â†’ 14Ã—14Ã—16  â†’ 7Ã—7Ã—32   â†’ 3Ã—3Ã—64   â†’ 10
  
  RÃ©solution: â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“
  Channels:   â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘
  
  HiÃ©rarchie : bords â†’ textures â†’ parties â†’ objets`,
        label: 'Fig. â€” Progression spatiale dans un CNN',
      },
      {
        type: 'callout',
        content: 'âš¡ **Architectures cÃ©lÃ¨bres** :\nâ€¢ **LeNet-5** (1998) : 2 conv, 60K params â†’ MNIST\nâ€¢ **AlexNet** (2012) : 5 conv, 60M params â†’ ImageNet revolution\nâ€¢ **VGG** (2014) : 16-19 couches, pattern 3Ã—3 â†’ simple et profond\nâ€¢ **GoogLeNet** (2014) : Inception modules â†’ parallÃ©lisme\nâ€¢ **ResNet** (2015) : Skip connections â†’ 152+ couches',
      },
    ],
    exercises: [
      {
        id: 'cnn-ex1',
        title: 'ğŸ’» Pratique â€” CNN pour MNIST',
        instructions: 'CrÃ©ez un CNN avec 2 couches conv pour classifier des images 28Ã—28 (1 canal). Calculez les dimensions de chaque couche.',
        starterCode: `import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # Conv1: 1â†’16 channels, kernel 3, padding 1 â†’ 28Ã—28Ã—16
        # MaxPool: â†’ 14Ã—14Ã—16
        self.conv1 = ___
        # Conv2: 16â†’32 channels, kernel 3, padding 1 â†’ 14Ã—14Ã—32
        # MaxPool: â†’ 7Ã—7Ã—32
        self.conv2 = ___
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = ___   # 32*7*7 â†’ 10
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))  # 28â†’14
        x = self.pool(self.relu(self.conv2(x)))  # 14â†’7
        x = x.view(x.size(0), -1)  # flatten
        return self.fc(x)

model = SimpleCNN()
x = torch.randn(4, 1, 28, 28)
out = model(x)
print(f"Input:  {x.shape}")
print(f"Output: {out.shape}")
print(f"Params: {sum(p.numel() for p in model.parameters()):,}")`,
        solution: `import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = nn.Linear(32 * 7 * 7, 10)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        return self.fc(x)

model = SimpleCNN()
x = torch.randn(4, 1, 28, 28)
out = model(x)
print(f"Input:  {x.shape}")
print(f"Output: {out.shape}")
print(f"Params: {sum(p.numel() for p in model.parameters()):,}")`,
        hints: [
          'nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)',
          'AprÃ¨s 2 MaxPool(2,2) sur 28Ã—28 â†’ 7Ã—7',
          'nn.Linear(32 * 7 * 7, 10)',
        ],
        completed: false,
      },
      {
        id: 'cnn-th1',
        title: 'ğŸ§  ThÃ©orie â€” Comptage des paramÃ¨tres conv',
        instructions: 'Calculez le nombre de paramÃ¨tres de chaque couche conv et comparez avec un rÃ©seau fully-connected Ã©quivalent.',
        starterCode: `import torch
import torch.nn as nn

# CNN
conv1 = nn.Conv2d(3, 16, 3, padding=1)   # RGB input
conv2 = nn.Conv2d(16, 32, 3, padding=1)
conv3 = nn.Conv2d(32, 64, 3, padding=1)

# Fully-connected Ã©quivalent (32Ã—32 RGB image â†’ mÃªme features)
fc1 = nn.Linear(3 * 32 * 32, 16 * 32 * 32)  # mÃªme "capacitÃ©"

print("â•â•â• CNN vs FC â€” Nombre de paramÃ¨tres â•â•â•")
print(f"Conv1 (3â†’16, 3Ã—3):  {sum(p.numel() for p in conv1.parameters()):>10,}")
print(f"Conv2 (16â†’32, 3Ã—3): {sum(p.numel() for p in conv2.parameters()):>10,}")
print(f"Conv3 (32â†’64, 3Ã—3): {sum(p.numel() for p in conv3.parameters()):>10,}")
total_conv = sum(sum(p.numel() for p in m.parameters()) for m in [conv1, conv2, conv3])
print(f"Total CNN:           {total_conv:>10,}")
print(f"\\nFC1 (3*32*32 â†’ 16*32*32): {sum(p.numel() for p in fc1.parameters()):>10,}")
print(f"\\nâ†’ Le FC a {sum(p.numel() for p in fc1.parameters()) // total_conv}Ã— plus de paramÃ¨tres !")
print(f"   grÃ¢ce au partage de poids (weight sharing) des convolutions.")`,
        solution: `import torch
import torch.nn as nn

conv1 = nn.Conv2d(3, 16, 3, padding=1)
conv2 = nn.Conv2d(16, 32, 3, padding=1)
conv3 = nn.Conv2d(32, 64, 3, padding=1)

fc1 = nn.Linear(3 * 32 * 32, 16 * 32 * 32)

print("â•â•â• CNN vs FC â€” Nombre de paramÃ¨tres â•â•â•")
print(f"Conv1 (3â†’16, 3Ã—3):  {sum(p.numel() for p in conv1.parameters()):>10,}")
print(f"Conv2 (16â†’32, 3Ã—3): {sum(p.numel() for p in conv2.parameters()):>10,}")
print(f"Conv3 (32â†’64, 3Ã—3): {sum(p.numel() for p in conv3.parameters()):>10,}")
total_conv = sum(sum(p.numel() for p in m.parameters()) for m in [conv1, conv2, conv3])
print(f"Total CNN:           {total_conv:>10,}")
print(f"\\nFC1 (3*32*32 â†’ 16*32*32): {sum(p.numel() for p in fc1.parameters()):>10,}")
print(f"\\nâ†’ Le FC a {sum(p.numel() for p in fc1.parameters()) // total_conv}Ã— plus de paramÃ¨tres !")
print(f"   grÃ¢ce au partage de poids (weight sharing) des convolutions.")`,
        hints: [
          'Conv2d params = Cout Ã— (Cin Ã— K Ã— K + 1_bias)',
          'Le weight sharing rÃ©duit drastiquement les paramÃ¨tres',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RÃ©seaux Convolutifs â€” Ch. 10 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. Convolution 2D â”€â”€
conv = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)
x = torch.randn(1, 1, 28, 28)
out = conv(x)
print(f"Conv2d: {x.shape} â†’ {out.shape}")
print(f"Kernel: {conv.weight.shape} = {conv.weight.numel()} poids + {conv.bias.numel()} biais")

# â”€â”€ 2. CNN complet â”€â”€
class MNISTNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.classifier = nn.Sequential(
            nn.Flatten(), nn.Linear(32*7*7, 128), nn.ReLU(),
            nn.Dropout(0.3), nn.Linear(128, 10)
        )
    
    def forward(self, x):
        return self.classifier(self.features(x))

model = MNISTNet()
out = model(torch.randn(4, 1, 28, 28))
print(f"\\nMNISTNet: batch=4 â†’ {out.shape}")
print(f"ParamÃ¨tres: {sum(p.numel() for p in model.parameters()):,}")

# â”€â”€ 3. Feature maps â”€â”€
print("\\nâ•â•â• Feature maps par couche â•â•â•")
x = torch.randn(1, 1, 28, 28)
for name, layer in model.features.named_children():
    x = layer(x)
    print(f"  Layer {name}: {x.shape}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 10 â€” RESIDUAL NETWORKS (Ch. 11)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'resnet',
    title: 'RÃ©seaux RÃ©siduels (ResNet)',
    shortTitle: 'ResNet',
    description: 'Skip connections, blocs rÃ©siduels, BatchNorm, entraÃ®ner des rÃ©seaux de 100+ couches (Ch. 11 â€” UDL).',
    status: 'locked',
    progress: 0,
    dependencies: ['cnn'],
    category: 'architectures',
    theory: [
      {
        type: 'text',
        content: `## 11.1 â€” Le problÃ¨me de la profondeur\n\nLes rÃ©seaux trÃ¨s profonds (>20 couches) souffrent du **degradation problem** : la performance se dÃ©grade avec la profondeur, mÃªme sur le jeu d'entraÃ®nement ! Ce n'est pas de l'overfitting â€” c'est un problÃ¨me d'**optimisation** dÃ» aux gradients Ã©vanescents.`,
      },
      {
        type: 'text',
        content: `## 11.2 â€” Connexion rÃ©siduelle\n\nL'idÃ©e gÃ©niale de He et al. (2015) : au lieu d'apprendre la transformation complÃ¨te h â†’ h', le rÃ©seau apprend le **rÃ©sidu** f(h) = h' âˆ’ h. La sortie est simplement h + f(h). Si le rÃ©sidu est proche de zÃ©ro, le gradient circule librement via le "highway" de la skip connection :`,
      },
      {
        type: 'equation',
        content: '\\mathbf{h}_{k+1} = \\mathbf{h}_k + f_k(\\mathbf{h}_k) \\qquad \\text{(rÃ©siduel)}',
        label: 'Ã‰q. 11.1 â€” Skip connection',
        highlightVar: 'hidden',
      },
      {
        type: 'diagram',
        content: `  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚         Skip Connection              â”‚
  â”‚              â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®       â”‚
  â”‚    x  â”€â”€â”€â”€â”€â”€â–¶â”‚               â”‚â”€â”€â–¶ +  â”‚â”€â”€â–¶ output
  â”‚    â”‚         â”‚  Conv-BN-ReLU â”‚       â–²
  â”‚    â”‚         â”‚  Conv-BN      â”‚       â”‚
  â”‚    â”‚         â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯       â”‚
  â”‚    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
  â”‚         x directement additionnÃ© !
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  Si f(x) â†’ 0, alors output â‰ˆ x (identitÃ©)
  â†’ Le rÃ©seau peut toujours "copier" l'entrÃ©e`,
        label: 'Fig. 11.1 â€” Bloc rÃ©siduel avec skip connection',
      },
      {
        type: 'text',
        content: `## 11.3 â€” Bloc rÃ©siduel prÃ©-activation\n\nDeux variantes :\n- **Post-activation** (ResNet v1) : Conv â†’ BN â†’ ReLU â†’ Conv â†’ BN â†’ + â†’ ReLU\n- **PrÃ©-activation** (ResNet v2, meilleur) : BN â†’ ReLU â†’ Conv â†’ BN â†’ ReLU â†’ Conv â†’ +\n\nLa version prÃ©-activation garde le chemin rÃ©siduel **propre** (pas de non-linÃ©aritÃ© sur le shortcut).`,
      },
      {
        type: 'text',
        content: `## 11.4 â€” Changement de dimensions\n\nQuand les dimensions changent (doubler les channels, rÃ©duire la rÃ©solution), la skip connection utilise une **convolution 1Ã—1** avec stride 2 pour adapter les dimensions. Cela garde l'addition h + f(h) valide.`,
      },
      {
        type: 'equation',
        content: '\\frac{\\partial \\ell}{\\partial \\mathbf{h}_k} = \\frac{\\partial \\ell}{\\partial \\mathbf{h}_{k+1}} \\cdot \\left( \\mathbf{I} + \\frac{\\partial f_k}{\\partial \\mathbf{h}_k} \\right)',
        label: 'Gradient : le terme I empÃªche le vanishing !',
        highlightVar: 'grad',
      },
      {
        type: 'callout',
        content: 'ğŸ§  **Pourquoi Ã§a marche ?** Le gradient de la skip connection contient un terme **identitÃ© I**. MÃªme si âˆ‚f/âˆ‚h est petit, le gradient circule via I. Cela crÃ©e un "highway" pour le gradient, permettant d\'entraÃ®ner des rÃ©seaux de **152, 1000+** couches !',
      },
      {
        type: 'text',
        content: `## 11.5 â€” Architectures ResNet\n\n- **ResNet-18/34** : blocs basiques (2 convolutions 3Ã—3)\n- **ResNet-50/101/152** : blocs bottleneck (1Ã—1 â†’ 3Ã—3 â†’ 1Ã—1, rÃ©duit les calculs)\n- **WideResNet** : plus large au lieu de plus profond\n- **ResNeXt** : blocs parallÃ¨les avec cardinality\n- **DenseNet** : chaque couche connectÃ©e Ã  TOUTES les prÃ©cÃ©dentes`,
      },
    ],
    exercises: [
      {
        id: 'resnet-ex1',
        title: 'ğŸ’» Pratique â€” Bloc rÃ©siduel',
        instructions: 'ImplÃ©mentez un bloc rÃ©siduel (Residual Block) et un SimpleResNet avec 3 blocs. VÃ©rifiez que les gradients circulent bien.',
        starterCode: `import torch
import torch.nn as nn

class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.block = nn.Sequential(
            nn.BatchNorm2d(channels),
            nn.ReLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.BatchNorm2d(channels),
            nn.ReLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
        )
    
    def forward(self, x):
        return ___  # skip connection !

class SimpleResNet(nn.Module):
    def __init__(self, num_blocks=3):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.blocks = nn.Sequential(*[ResidualBlock(32) for _ in range(num_blocks)])
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(32, 10)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.blocks(x)
        x = self.pool(x).flatten(1)
        return self.fc(x)

model = SimpleResNet(num_blocks=5)
x = torch.randn(2, 1, 28, 28)
out = model(x)
loss = out.sum()
loss.backward()

print(f"Output: {out.shape}")
print(f"Params: {sum(p.numel() for p in model.parameters()):,}")
print(f"Conv1 grad norm: {model.conv1.weight.grad.norm().item():.6f}")
print(f"â†’ Le gradient circule bien malgrÃ© 11 couches conv !")`,
        solution: `import torch
import torch.nn as nn

class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.block = nn.Sequential(
            nn.BatchNorm2d(channels),
            nn.ReLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.BatchNorm2d(channels),
            nn.ReLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
        )
    
    def forward(self, x):
        return x + self.block(x)

class SimpleResNet(nn.Module):
    def __init__(self, num_blocks=3):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.blocks = nn.Sequential(*[ResidualBlock(32) for _ in range(num_blocks)])
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(32, 10)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.blocks(x)
        x = self.pool(x).flatten(1)
        return self.fc(x)

model = SimpleResNet(num_blocks=5)
x = torch.randn(2, 1, 28, 28)
out = model(x)
loss = out.sum()
loss.backward()

print(f"Output: {out.shape}")
print(f"Params: {sum(p.numel() for p in model.parameters()):,}")
print(f"Conv1 grad norm: {model.conv1.weight.grad.norm().item():.6f}")
print(f"â†’ Le gradient circule bien malgrÃ© 11 couches conv !")`,
        hints: [
          'return x + self.block(x)  â€” c\'est la skip connection',
          'Le + additionne l\'entrÃ©e et la sortie du bloc',
        ],
        completed: false,
      },
      {
        id: 'resnet-th1',
        title: 'ğŸ§  ThÃ©orie â€” ResNet vs PlainNet (gradient flow)',
        instructions: 'Comparez le flux de gradient dans un rÃ©seau "plain" (sans skip) vs ResNet de mÃªme profondeur. Montrez que ResNet prÃ©serve les gradients.',
        starterCode: `import torch
import torch.nn as nn

torch.manual_seed(42)

def make_plain_net(depth=20, ch=32):
    layers = [nn.Conv2d(1, ch, 3, padding=1)]
    for _ in range(depth):
        layers.extend([nn.Conv2d(ch, ch, 3, padding=1), nn.ReLU()])
    layers.append(nn.AdaptiveAvgPool2d(1))
    return nn.Sequential(*layers)

class ResBlock(nn.Module):
    def __init__(self, ch):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(ch, ch, 3, padding=1), nn.ReLU(),
        )
    def forward(self, x):
        return x + self.net(x)

def make_resnet(depth=20, ch=32):
    layers = [nn.Conv2d(1, ch, 3, padding=1)]
    for _ in range(depth):
        layers.append(ResBlock(ch))
    layers.append(nn.AdaptiveAvgPool2d(1))
    return nn.Sequential(*layers)

x = torch.randn(1, 1, 28, 28)

for name, model_fn in [('Plain', make_plain_net), ('ResNet', make_resnet)]:
    model = model_fn(depth=20)
    out = model(x)
    out.sum().backward()
    grad = model[0].weight.grad.norm().item()
    print(f"{name:>6}: grad_norm_layer0 = {grad:.8f}")

print(f"\\nâ†’ Le gradient du PlainNet est beaucoup plus petit (vanishing) !")`,
        solution: `import torch
import torch.nn as nn

torch.manual_seed(42)

def make_plain_net(depth=20, ch=32):
    layers = [nn.Conv2d(1, ch, 3, padding=1)]
    for _ in range(depth):
        layers.extend([nn.Conv2d(ch, ch, 3, padding=1), nn.ReLU()])
    layers.append(nn.AdaptiveAvgPool2d(1))
    return nn.Sequential(*layers)

class ResBlock(nn.Module):
    def __init__(self, ch):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(ch, ch, 3, padding=1), nn.ReLU(),
        )
    def forward(self, x):
        return x + self.net(x)

def make_resnet(depth=20, ch=32):
    layers = [nn.Conv2d(1, ch, 3, padding=1)]
    for _ in range(depth):
        layers.append(ResBlock(ch))
    layers.append(nn.AdaptiveAvgPool2d(1))
    return nn.Sequential(*layers)

x = torch.randn(1, 1, 28, 28)

for name, model_fn in [('Plain', make_plain_net), ('ResNet', make_resnet)]:
    model = model_fn(depth=20)
    out = model(x)
    out.sum().backward()
    grad = model[0].weight.grad.norm().item()
    print(f"{name:>6}: grad_norm_layer0 = {grad:.8f}")

print(f"\\nâ†’ Le gradient du PlainNet est beaucoup plus petit (vanishing) !")`,
        hints: [
          'Le PlainNet perd le gradient car chaque couche le multiplie par < 1',
          'Le ResNet prÃ©serve le gradient grÃ¢ce au terme identitÃ© I',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RÃ©seaux RÃ©siduels â€” Ch. 11 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.block = nn.Sequential(
            nn.BatchNorm2d(channels), nn.ReLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.BatchNorm2d(channels), nn.ReLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
        )
    
    def forward(self, x):
        return x + self.block(x)

class SimpleResNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.res1 = ResidualBlock(32)
        self.res2 = ResidualBlock(32)
        self.res3 = ResidualBlock(32)
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(32, 10)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.res1(x)
        x = self.res2(x)
        x = self.res3(x)
        x = self.pool(x).flatten(1)
        return self.fc(x)

model = SimpleResNet()
x = torch.randn(2, 1, 28, 28)
out = model(x)
print(f"SimpleResNet: {x.shape} â†’ {out.shape}")
print(f"ParamÃ¨tres: {sum(p.numel() for p in model.parameters()):,}")
print(f"Profondeur: 1 + 6 = 7 couches conv (3 blocs Ã— 2)")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 11 â€” RNN/LSTM (Ch. 12)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'rnn',
    title: 'RÃ©seaux RÃ©currents (RNN/LSTM)',
    shortTitle: 'RNN',
    description: 'Traitement sÃ©quentiel, Ã©tat cachÃ©, vanishing gradient temporel, portes LSTM/GRU (Ch. 12 â€” UDL).',
    status: 'locked',
    progress: 0,
    dependencies: ['regularization'],
    category: 'architectures',
    theory: [
      {
        type: 'text',
        content: `## 12.1 â€” Pourquoi les sÃ©quences sont spÃ©ciales\n\nLes donnÃ©es sÃ©quentielles (texte, audio, sÃ©ries temporelles) ont des **dÃ©pendances temporelles** : chaque Ã©lÃ©ment dÃ©pend des prÃ©cÃ©dents. Un rÃ©seau feedforward traite chaque entrÃ©e indÃ©pendamment â€” il ignore l'ordre. Le **RNN** rÃ©sout cela avec un **Ã©tat cachÃ©** qui accumule l'information au fil du temps.`,
      },
      {
        type: 'text',
        content: `## 12.2 â€” RNN (Recurrent Neural Network)\n\nÃ€ chaque pas de temps t, le RNN reÃ§oit l'entrÃ©e xâ‚œ et l'Ã©tat prÃ©cÃ©dent hâ‚œâ‚‹â‚, et produit un nouvel Ã©tat hâ‚œ. Les mÃªmes poids W sont **partagÃ©s** Ã  chaque pas de temps :`,
      },
      {
        type: 'equation',
        content: '\\mathbf{h}_t = \\tanh\\!\\left(\\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{W}_{xh} \\mathbf{x}_t + \\mathbf{b}_h\\right)',
        label: 'Ã‰q. 12.1 â€” RNN : Ã‰tat cachÃ©',
      },
      {
        type: 'diagram',
        content: `  Unfolded RNN (3 pas de temps)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      xâ‚         xâ‚‚         xâ‚ƒ
      â”‚          â”‚          â”‚
      â–¼          â–¼          â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”
  â”‚ RNN   â”‚â†’â”‚ RNN   â”‚â†’â”‚ RNN   â”‚â†’ hâ‚œ
  â”‚ cell  â”‚ â”‚ cell  â”‚ â”‚ cell  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜
  hâ‚€    hâ‚       hâ‚‚       hâ‚ƒ
  
  MÃŠMES POIDS (Whh, Wxh, bh) partagÃ©s Ã  chaque t
  â†’ "DÃ©rouler" le RNN = rÃ©seau profond de T couches`,
        label: 'Fig. 12.2 â€” RNN dÃ©roulÃ© dans le temps',
      },
      {
        type: 'text',
        content: `## 12.3 â€” Vanishing gradient temporel\n\nQuand on dÃ©roule le RNN sur T pas de temps, le backprop traverse T copies de W. Si les valeurs propres de W < 1, le gradient **s'Ã©vanouit** exponentiellement. Si > 1, il **explose**. C'est le problÃ¨me des **long-range dependencies**.`,
      },
      {
        type: 'text',
        content: `## 12.4 â€” LSTM (Long Short-Term Memory)\n\nLe LSTM rÃ©sout le vanishing gradient avec une **mÃ©moire Ã  long terme** (cell state câ‚œ) protÃ©gÃ©e par 3 **portes** (gates) apprenables :`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} \\mathbf{f}_t &= \\sigma(\\mathbf{W}_f [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_f) & \\text{(forget gate)} \\\\ \\mathbf{i}_t &= \\sigma(\\mathbf{W}_i [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_i) & \\text{(input gate)} \\\\ \\tilde{\\mathbf{c}}_t &= \\tanh(\\mathbf{W}_c [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_c) & \\text{(candidate)} \\\\ \\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t & \\text{(cell update)} \\\\ \\mathbf{o}_t &= \\sigma(\\mathbf{W}_o [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_o) & \\text{(output gate)} \\\\ \\mathbf{h}_t &= \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t) & \\text{(hidden state)} \\end{aligned}',
        label: 'Ã‰q. 12.8â€“12.13 â€” LSTM complet',
      },
      {
        type: 'diagram',
        content: `  LSTM Cell
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  câ‚œâ‚‹â‚ â”€â”€â–¶ Ã—(fâ‚œ) â”€â”€â–¶ + â”€â”€â–¶ câ‚œ        â•‘
  â•‘                     â–²                â•‘
  â•‘                 Ã—(iâ‚œ)                â•‘
  â•‘                     â–²                â•‘
  â•‘                 tanh(cÌƒâ‚œ)             â•‘
  â•‘                                      â•‘
  â•‘  hâ‚œâ‚‹â‚ â”€â”¬â”€â”€â–¶ [fâ‚œ, iâ‚œ, cÌƒâ‚œ, oâ‚œ]       â•‘
  â•‘  xâ‚œ   â”€â”˜                            â•‘
  â•‘                                      â•‘
  â•‘  hâ‚œ = oâ‚œ Ã— tanh(câ‚œ)                  â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  fâ‚œ : forget gate  â†’ quoi effacer de câ‚œâ‚‹â‚
  iâ‚œ : input gate   â†’ quoi ajouter
  oâ‚œ : output gate  â†’ quoi exposer`,
        label: 'Fig. 12.6 â€” Architecture LSTM',
      },
      {
        type: 'text',
        content: `## 12.5 â€” GRU (Gated Recurrent Unit)\n\nVersion simplifiÃ©e du LSTM avec seulement **2 portes** (reset r et update z). Moins de paramÃ¨tres, performances souvent comparables. Pas de cell state sÃ©parÃ©.`,
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ **En pratique** :\nâ€¢ \\`nn.RNN\\` : simple mais vanishing gradient â†’ rarement utilisÃ©\nâ€¢ \\`nn.LSTM\\` : robuste, gÃ¨re les longues sÃ©quences\nâ€¢ \\`nn.GRU\\` : lÃ©ger, bon pour les petits datasets\nâ€¢ Les **Transformers** ont largement remplacÃ© les RNN pour le NLP, mais les RNN restent utiles pour le streaming et les sÃ©quences trÃ¨s longues.',
      },
    ],
    exercises: [
      {
        id: 'rnn-ex1',
        title: 'ğŸ’» Pratique â€” LSTM pour sÃ©quences',
        instructions: 'Construisez un classifieur de sÃ©quences avec LSTM. Utilisez le dernier Ã©tat cachÃ© pour la classification.',
        starterCode: `import torch
import torch.nn as nn

class LSTMClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim,
                            num_layers=num_layers,
                            batch_first=True,
                            dropout=0.2)
        self.fc = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x):
        # x: (batch, seq_len, input_dim)
        output, (h_n, c_n) = self.lstm(x)
        # h_n: (num_layers, batch, hidden_dim)
        last_hidden = ___  # dernier layer, dernier Ã©tat
        return self.fc(last_hidden)

model = LSTMClassifier(input_dim=10, hidden_dim=64, num_classes=5)
x = torch.randn(8, 20, 10)  # batch=8, seq_len=20, features=10
out = model(x)
print(f"Input:  {x.shape}")
print(f"Output: {out.shape}")
print(f"Params: {sum(p.numel() for p in model.parameters()):,}")`,
        solution: `import torch
import torch.nn as nn

class LSTMClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim,
                            num_layers=num_layers,
                            batch_first=True,
                            dropout=0.2)
        self.fc = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x):
        output, (h_n, c_n) = self.lstm(x)
        last_hidden = h_n[-1]
        return self.fc(last_hidden)

model = LSTMClassifier(input_dim=10, hidden_dim=64, num_classes=5)
x = torch.randn(8, 20, 10)
out = model(x)
print(f"Input:  {x.shape}")
print(f"Output: {out.shape}")
print(f"Params: {sum(p.numel() for p in model.parameters()):,}")`,
        hints: [
          'h_n[-1] donne le dernier layer du dernier timestep',
          'h_n shape = (num_layers, batch, hidden_dim)',
        ],
        completed: false,
      },
      {
        id: 'rnn-th1',
        title: 'ğŸ§  ThÃ©orie â€” RNN vs LSTM gradient flow',
        instructions: 'Comparez la norme des gradients sur des sÃ©quences de longueur croissante pour un RNN simple vs LSTM.',
        starterCode: `import torch
import torch.nn as nn

torch.manual_seed(42)

def test_gradient_flow(model_class, seq_lengths, input_dim=5, hidden_dim=32):
    results = []
    for T in seq_lengths:
        model = model_class(input_dim, hidden_dim, batch_first=True)
        x = torch.randn(1, T, input_dim, requires_grad=True)
        
        output, _ = model(x)
        loss = output[:, -1, :].sum()  # utiliser le dernier output
        loss.backward()
        
        grad_norm = x.grad[:, 0, :].norm().item()  # gradient au PREMIER timestep
        results.append(grad_norm)
    return results

seq_lengths = [5, 10, 20, 50, 100]
rnn_grads = test_gradient_flow(nn.RNN, seq_lengths)
lstm_grads = test_gradient_flow(nn.LSTM, seq_lengths)

print(f"{'T':>5} â”‚ {'RNN grad':>12} â”‚ {'LSTM grad':>12}")
print(f"{'â”€'*5}â”€â”¼â”€{'â”€'*12}â”€â”¼â”€{'â”€'*12}")
for T, rg, lg in zip(seq_lengths, rnn_grads, lstm_grads):
    print(f"{T:5d} â”‚ {rg:12.6f} â”‚ {lg:12.6f}")
print(f"\\nâ†’ Le gradient RNN s'Ã©vanouit, le LSTM le prÃ©serve !")`,
        solution: `import torch
import torch.nn as nn

torch.manual_seed(42)

def test_gradient_flow(model_class, seq_lengths, input_dim=5, hidden_dim=32):
    results = []
    for T in seq_lengths:
        model = model_class(input_dim, hidden_dim, batch_first=True)
        x = torch.randn(1, T, input_dim, requires_grad=True)
        
        output, _ = model(x)
        loss = output[:, -1, :].sum()
        loss.backward()
        
        grad_norm = x.grad[:, 0, :].norm().item()
        results.append(grad_norm)
    return results

seq_lengths = [5, 10, 20, 50, 100]
rnn_grads = test_gradient_flow(nn.RNN, seq_lengths)
lstm_grads = test_gradient_flow(nn.LSTM, seq_lengths)

print(f"{'T':>5} â”‚ {'RNN grad':>12} â”‚ {'LSTM grad':>12}")
print(f"{'â”€'*5}â”€â”¼â”€{'â”€'*12}â”€â”¼â”€{'â”€'*12}")
for T, rg, lg in zip(seq_lengths, rnn_grads, lstm_grads):
    print(f"{T:5d} â”‚ {rg:12.6f} â”‚ {lg:12.6f}")
print(f"\\nâ†’ Le gradient RNN s'Ã©vanouit, le LSTM le prÃ©serve !")`,
        hints: [
          'Le gradient au premier timestep mesure les long-range dependencies',
          'Le cell state du LSTM crÃ©e un "highway" pour le gradient',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RÃ©seaux RÃ©currents â€” Ch. 12 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. RNN simple â”€â”€
print("â•â•â• RNN â•â•â•")
rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
x = torch.randn(1, 5, 10)
output, h_n = rnn(x)
print(f"Output: {output.shape}  (batch, seq_len, hidden)")
print(f"Hidden: {h_n.shape}    (layers, batch, hidden)")

# â”€â”€ 2. LSTM â”€â”€
print("\\nâ•â•â• LSTM â•â•â•")
lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
output, (h_n, c_n) = lstm(x)
print(f"Output: {output.shape}")
print(f"Hidden: {h_n.shape}, Cell: {c_n.shape}")

# â”€â”€ 3. GRU â”€â”€
print("\\nâ•â•â• GRU â•â•â•")
gru = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
output, h_n = gru(x)
print(f"Output: {output.shape}")

# â”€â”€ 4. Comparaison paramÃ¨tres â”€â”€
print("\\nâ•â•â• ParamÃ¨tres â•â•â•")
for name, m in [('RNN', rnn), ('LSTM', lstm), ('GRU', gru)]:
    p = sum(p.numel() for p in m.parameters())
    print(f"  {name:>4s}: {p:,} params")
print("  LSTM â‰ˆ 4Ã— RNN (4 portes), GRU â‰ˆ 3Ã— RNN (3 gates)")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 12 â€” TRANSFORMERS (Ch. 12-13)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'attention',
    title: 'Attention & Transformers',
    shortTitle: 'Transformer',
    description: 'Self-Attention, Multi-Head Attention, Positional Encoding, blocs Transformer encoder/decoder (Ch. 12-13 â€” UDL).',
    status: 'locked',
    progress: 0,
    dependencies: ['rnn'],
    category: 'advanced',
    theory: [
      {
        type: 'text',
        content: `## 12.1 â€” Du RNN au Transformer\n\nLes RNN traitent les tokens **sÃ©quentiellement** â€” impossible de parallÃ©liser, et l'information se dÃ©grade sur les longues sÃ©quences. Le **Transformer** (Vaswani et al., 2017) remplace la rÃ©currence par le **self-attention** : chaque token consulte directement tous les autres en parallÃ¨le.\n\nRÃ©sultat : parallÃ©lisme massif + pas de vanishing gradient temporel.`,
      },
      {
        type: 'text',
        content: `## 12.2 â€” Dot-Product Self-Attention\n\nPour chaque entrÃ©e xâ‚˜, on projette vers 3 vecteurs :\n- **Query** q = WqÂ·x : "quelle information je cherche"\n- **Key** k = WkÂ·x : "quelle information j'offre"\n- **Value** v = WvÂ·x : "le contenu Ã  transmettre"\n\nLa sortie de l'attention est une somme pondÃ©rÃ©e des values, avec des poids calculÃ©s par compatibilitÃ© query-key :`,
      },
      {
        type: 'equation',
        content: '\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\!\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\right) \\mathbf{V}',
        label: 'Ã‰q. 12.14 â€” Scaled Dot-Product Attention',
        highlightVar: 'attention',
      },
      {
        type: 'diagram',
        content: `  Scaled Dot-Product Attention
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      Q    K    V
      â”‚    â”‚    â”‚
      â””â”€â”€â”¬â”€â”˜    â”‚
         â”‚      â”‚
     QÂ·K^T/âˆšdk  â”‚
         â”‚      â”‚
      softmax   â”‚
         â”‚      â”‚
         â””â”€â”€â”¬â”€â”€â”€â”˜
            â”‚
        Attn @ V
            â”‚
         Output
  
  ComplexitÃ© : O(NÂ² Â· d)  â€” quadratique en longueur de sÃ©quence`,
        label: 'Fig. 12.3 â€” Attention Pipeline',
      },
      {
        type: 'text',
        content: `## 12.3 â€” Multi-Head Attention\n\nAu lieu d'un seul mÃ©canisme d'attention, on exÃ©cute **H tÃªtes** en parallÃ¨le. Chaque tÃªte utilise des projections diffÃ©rentes (Wq_h, Wk_h, Wv_h) et apprend des types de relations distincts : syntaxe, corÃ©fÃ©rence, sÃ©mantique, etc.\n\nLes sorties des H tÃªtes sont concatÃ©nÃ©es puis projetÃ©es par Wo :`,
      },
      {
        type: 'equation',
        content: '\\text{MultiHead}(\\mathbf{X}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_H) \\, \\mathbf{W}_O \\quad \\text{oÃ¹}\\; \\text{head}_h = \\text{Attn}(\\mathbf{X}\\mathbf{W}_h^Q, \\mathbf{X}\\mathbf{W}_h^K, \\mathbf{X}\\mathbf{W}_h^V)',
        label: 'Ã‰q. 12.18 â€” Multi-Head Attention',
      },
      {
        type: 'text',
        content: `## 12.4 â€” Positional Encoding\n\nLe self-attention est **permutation-invariant** â€” il ne connaÃ®t pas l'ordre des tokens ! On ajoute un **positional encoding** pour injecter la notion de position :\n\n**Sinusoidal** (Vaswani) : PE(pos, 2i) = sin(pos / 10000^(2i/d)), PE(pos, 2i+1) = cos(pos / 10000^(2i/d))\n\n**Appris** (BERT, GPT) : une matrice (max_len Ã— d_model) entraÃ®nable.`,
      },
      {
        type: 'text',
        content: `## 12.5 â€” Bloc Transformer\n\nUn bloc Transformer empile :\n1. **Multi-Head Self-Attention** + rÃ©siduel + LayerNorm\n2. **Feed-Forward Network** (2 couches, ReLU/GELU) + rÃ©siduel + LayerNorm\n\nOn empile L blocs identiques. BERT-base : L=12, d=768, H=12. GPT-3 : L=96, d=12288, H=96.`,
      },
      {
        type: 'diagram',
        content: `  Transformer Block (Pre-Norm variant)
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  Input x                         â•‘
  â•‘    â”‚                              â•‘
  â•‘    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â•‘
  â•‘    â–¼                   â”‚          â•‘
  â•‘  LayerNorm             â”‚          â•‘
  â•‘    â–¼                   â”‚          â•‘
  â•‘  Multi-Head Attention  â”‚          â•‘
  â•‘    â–¼                   â”‚          â•‘
  â•‘    + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (rÃ©siduel)â•‘
  â•‘    â”‚                              â•‘
  â•‘    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â•‘
  â•‘    â–¼                   â”‚          â•‘
  â•‘  LayerNorm             â”‚          â•‘
  â•‘    â–¼                   â”‚          â•‘
  â•‘  FFN (Linearâ†’GELUâ†’Linear)        â•‘
  â•‘    â–¼                   â”‚          â•‘
  â•‘    + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (rÃ©siduel)â•‘
  â•‘    â”‚                              â•‘
  â•‘  Output                          â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`,
        label: 'Fig. 12.7 â€” Bloc Transformer',
      },
      {
        type: 'text',
        content: `## 12.6 â€” Encoder vs Decoder\n\n- **Encoder** (BERT) : self-attention **bidirectionnelle** â€” chaque token voit tous les autres. Pour la comprÃ©hension (classification, NER).\n- **Decoder** (GPT) : self-attention **causale** â€” un masque triangulaire empÃªche de voir les tokens futurs. Pour la gÃ©nÃ©ration.\n- **Encoder-Decoder** (T5, traduction) : le decoder utilise le **cross-attention** pour consulter l'encoder.`,
      },
      {
        type: 'text',
        content: `## 12.7 â€” Masque Causal\n\nPour la gÃ©nÃ©ration autoregressive (GPT), on applique un masque **triangulaire infÃ©rieur** avant le softmax. Les positions futures sont remplies de âˆ’âˆ, ce qui donne un poids d'attention de 0 aprÃ¨s softmax. Le token n ne peut voir que les tokens 1..n.`,
      },
      {
        type: 'callout',
        content: 'ğŸ§  **Hypernetwork** : le self-attention est un rÃ©seau dont les poids (attention scores) sont eux-mÃªmes calculÃ©s par le rÃ©seau. C\'est ce qui le rend si flexible â€” les connexions **dÃ©pendent des donnÃ©es**.\n\nğŸ“Š **Ã‰chelle** : BERT-base = 110M params, GPT-2 = 1.5B, GPT-3 = 175B, GPT-4 ~ 1.8T (estimÃ©). La loi d\'Ã©chelle montre que la performance s\'amÃ©liore en power-law avec la taille.',
      },
    ],
    exercises: [
      {
        id: 'attn-ex1',
        title: 'ğŸ’» Pratique â€” Self-Attention from scratch',
        instructions: 'ImplÃ©mentez le scaled dot-product attention manuellement (sans nn.MultiheadAttention), puis ajoutez le masque causal.',
        starterCode: `import torch
import torch.nn as nn
import math

d_model = 64
seq_len = 5
batch = 1

W_q = nn.Linear(d_model, d_model)
W_k = nn.Linear(d_model, d_model)
W_v = nn.Linear(d_model, d_model)

x = torch.randn(batch, seq_len, d_model)

Q = W_q(x)
K = W_k(x)
V = W_v(x)

# â”€â”€ 1. Attention bidirectionnelle â”€â”€
scores = ___  # Q @ K^T / sqrt(d_k)
weights = ___  # softmax(scores)
output = ___   # weights @ V
print(f"Bidirectional output: {output.shape}")

# â”€â”€ 2. Attention causale (masque triangulaire) â”€â”€
mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
scores_causal = scores.masked_fill(mask, float('-inf'))
weights_causal = torch.softmax(scores_causal, dim=-1)
output_causal = weights_causal @ V
print(f"Causal output: {output_causal.shape}")
print(f"Causal weights row 0: {weights_causal[0, 0]}")  # seul le 1er token a un poids`,
        solution: `import torch
import torch.nn as nn
import math

d_model = 64
seq_len = 5
batch = 1

W_q = nn.Linear(d_model, d_model)
W_k = nn.Linear(d_model, d_model)
W_v = nn.Linear(d_model, d_model)

x = torch.randn(batch, seq_len, d_model)

Q = W_q(x)
K = W_k(x)
V = W_v(x)

# â”€â”€ 1. Attention bidirectionnelle â”€â”€
scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_model)
weights = torch.softmax(scores, dim=-1)
output = weights @ V
print(f"Bidirectional output: {output.shape}")

# â”€â”€ 2. Attention causale â”€â”€
mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
scores_causal = scores.masked_fill(mask, float('-inf'))
weights_causal = torch.softmax(scores_causal, dim=-1)
output_causal = weights_causal @ V
print(f"Causal output: {output_causal.shape}")
print(f"Causal weights row 0: {weights_causal[0, 0]}")`,
        hints: [
          'scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_model)',
          'Le masque causal met -inf aux positions futures, le softmax les transforme en 0',
        ],
        completed: false,
      },
      {
        id: 'attn-ex2',
        title: 'ğŸ’» Pratique â€” Transformer Block complet',
        instructions: 'Construisez un bloc Transformer complet avec Multi-Head Attention, FFN, rÃ©siduel et LayerNorm.',
        starterCode: `import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_k = d_model // n_heads
        self.n_heads = n_heads
        self.W_qkv = nn.Linear(d_model, 3 * d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, x, mask=None):
        B, T, C = x.shape
        qkv = self.W_qkv(x).reshape(B, T, 3, self.n_heads, self.d_k)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, H, T, dk)
        Q, K, V = qkv[0], qkv[1], qkv[2]
        
        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask, float('-inf'))
        attn = torch.softmax(scores, dim=-1)
        out = (attn @ V).transpose(1, 2).contiguous().view(B, T, C)
        return self.W_o(out)

class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff=None):
        super().__init__()
        d_ff = d_ff or 4 * d_model
        self.attn = MultiHeadAttention(d_model, n_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Linear(d_ff, d_model),
        )
    
    def forward(self, x, mask=None):
        # Pre-norm: x + Attn(LN(x))
        x = x + self.attn(self.norm1(x), mask)
        x = x + self.ffn(self.norm2(x))
        return x

# Test
block = TransformerBlock(d_model=128, n_heads=8)
x = torch.randn(2, 10, 128)
out = block(x)
print(f"Input:  {x.shape}")
print(f"Output: {out.shape}")
print(f"Params: {sum(p.numel() for p in block.parameters()):,}")
print(f"  - Attn: {sum(p.numel() for p in block.attn.parameters()):,}")
print(f"  - FFN:  {sum(p.numel() for p in block.ffn.parameters()):,}")`,
        solution: `import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_k = d_model // n_heads
        self.n_heads = n_heads
        self.W_qkv = nn.Linear(d_model, 3 * d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, x, mask=None):
        B, T, C = x.shape
        qkv = self.W_qkv(x).reshape(B, T, 3, self.n_heads, self.d_k)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        Q, K, V = qkv[0], qkv[1], qkv[2]
        
        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask, float('-inf'))
        attn = torch.softmax(scores, dim=-1)
        out = (attn @ V).transpose(1, 2).contiguous().view(B, T, C)
        return self.W_o(out)

class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff=None):
        super().__init__()
        d_ff = d_ff or 4 * d_model
        self.attn = MultiHeadAttention(d_model, n_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Linear(d_ff, d_model),
        )
    
    def forward(self, x, mask=None):
        x = x + self.attn(self.norm1(x), mask)
        x = x + self.ffn(self.norm2(x))
        return x

block = TransformerBlock(d_model=128, n_heads=8)
x = torch.randn(2, 10, 128)
out = block(x)
print(f"Input:  {x.shape}")
print(f"Output: {out.shape}")
print(f"Params: {sum(p.numel() for p in block.parameters()):,}")
print(f"  - Attn: {sum(p.numel() for p in block.attn.parameters()):,}")
print(f"  - FFN:  {sum(p.numel() for p in block.ffn.parameters()):,}")`,
        hints: [
          'Pre-norm : LN avant attention et FFN, pas aprÃ¨s',
          'W_qkv projette vers 3Ã—d_model, puis on split en Q, K, V',
          'd_ff = 4 Ã— d_model est le standard',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn
import math

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Attention & Transformers â€” Ch. 12-13 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. Scaled Dot-Product Attention â”€â”€
print("â•â•â• Scaled Dot-Product Attention â•â•â•")
d_model = 64
seq_len = 8
x = torch.randn(1, seq_len, d_model)

W_q = nn.Linear(d_model, d_model)
W_k = nn.Linear(d_model, d_model)
W_v = nn.Linear(d_model, d_model)

Q, K, V = W_q(x), W_k(x), W_v(x)
scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_model)
attn_weights = torch.softmax(scores, dim=-1)
output = attn_weights @ V
print(f"Attention weights: {attn_weights.shape}")
print(f"Output: {output.shape}")

# â”€â”€ 2. Masque causal (GPT-style) â”€â”€
print("\\nâ•â•â• Causal Mask â•â•â•")
causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
scores_masked = scores.masked_fill(causal_mask, float('-inf'))
attn_causal = torch.softmax(scores_masked, dim=-1)
print(f"Causal attn row 0: {attn_causal[0, 0].tolist()}")
print(f"Causal attn row 4: {attn_causal[0, 4].tolist()}")

# â”€â”€ 3. Multi-Head Attention â”€â”€
print("\\nâ•â•â• Multi-Head Attention â•â•â•")
mha = nn.MultiheadAttention(embed_dim=64, num_heads=8, batch_first=True)
out, weights = mha(x, x, x)
print(f"MHA output: {out.shape}")
print(f"MHA weights: {weights.shape}")

# â”€â”€ 4. Positional Encoding â”€â”€
print("\\nâ•â•â• Positional Encoding â•â•â•")
max_len = 100
pe = torch.zeros(max_len, d_model)
pos = torch.arange(0, max_len).unsqueeze(1).float()
div = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))
pe[:, 0::2] = torch.sin(pos * div)
pe[:, 1::2] = torch.cos(pos * div)
print(f"PE shape: {pe.shape}")
print(f"PE[0, :8]: {pe[0, :8].tolist()}")
print(f"PE[1, :8]: {pe[1, :8].tolist()}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 13 â€” GANs (Ch. 15)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'gan',
    title: 'Generative Adversarial Networks (GAN)',
    shortTitle: 'GAN',
    description: 'Objectif minimax, entraÃ®nement adversarial, mode collapse, WGAN, et astuces pratiques (Ch. 15 â€” UDL).',
    status: 'locked',
    progress: 0,
    dependencies: ['attention'],
    category: 'advanced',
    theory: [
      {
        type: 'text',
        content: `## 15.1 â€” Le principe adversarial\n\nUn **GAN** met en compÃ©tition deux rÃ©seaux :\n\n- **GÃ©nÃ©rateur G(z)** : transforme du bruit z ~ N(0,I) en donnÃ©es synthÃ©tiques\n- **Discriminateur D(x)** : estime P(x est rÃ©el)\n\nG cherche Ã  **tromper** D. D cherche Ã  **ne pas Ãªtre trompÃ©**. Ce **jeu minimax** produit un Ã©quilibre de Nash oÃ¹ G gÃ©nÃ¨re des donnÃ©es indistinguables des rÃ©elles.`,
      },
      {
        type: 'equation',
        content: '\\min_G \\max_D \\; V(D,G) = \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}}[\\log D(\\mathbf{x})] + \\mathbb{E}_{\\mathbf{z} \\sim p_z}[\\log(1 - D(G(\\mathbf{z}))))]',
        label: 'Ã‰q. 15.1 â€” Objectif Minimax du GAN',
      },
      {
        type: 'text',
        content: `## 15.2 â€” Algorithme d'entraÃ®nement\n\nÃ€ chaque itÃ©ration :\n\n**Ã‰tape 1 â€” EntraÃ®ner D** (k pas) :\n- Ã‰chantillonner un mini-batch rÃ©el x et un bruit z\n- Calculer loss_D = âˆ’[log D(x) + log(1 âˆ’ D(G(z)))]\n- Mettre Ã  jour D par gradient ascent\n\n**Ã‰tape 2 â€” EntraÃ®ner G** (1 pas) :\n- Ã‰chantillonner un bruit z\n- Calculer loss_G = âˆ’log D(G(z))  â† "non-saturating" trick\n- Mettre Ã  jour G par gradient descent\n\nâš ï¸ En pratique, on minimise âˆ’log D(G(z)) au lieu de log(1âˆ’D(G(z))) pour Ã©viter le vanishing gradient quand G est mauvais.`,
      },
      {
        type: 'diagram',
        content: `  Architecture GAN
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       z ~ N(0,I)      x ~ p_data
          â”‚                â”‚
          â–¼                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
    â”‚ GÃ©nÃ©rateurâ”‚          â”‚
    â”‚     G     â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
          â”‚                â”‚
       G(z)               x
          â”‚                â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚Discriminateurâ”‚
          â”‚      D       â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
          D(Â·) âˆˆ [0,1]
          0 = faux, 1 = vrai`,
        label: 'Fig. 15.2 â€” Architecture GAN',
      },
      {
        type: 'text',
        content: `## 15.3 â€” Mode Collapse\n\nLe problÃ¨me le plus courant des GANs : G apprend Ã  gÃ©nÃ©rer seulement **quelques modes** de la distribution au lieu de la distribution complÃ¨te. Par exemple, G ne produit que des "7" au lieu de tous les chiffres.\n\n**Causes** : G trouve un point fixe qui trompe D systÃ©matiquement, donc il n'a pas d'incitation Ã  diversifier.\n\n**Solutions** : mini-batch discrimination, unrolled GAN, feature matching.`,
      },
      {
        type: 'text',
        content: `## 15.4 â€” WGAN (Wasserstein GAN)\n\nRemplace la divergence JS par la **distance de Wasserstein** (Earth Mover's Distance), ce qui donne des gradients plus stables et un signal mÃªme quand les distributions ne se chevauchent pas.\n\nLe discriminateur devient un **critique** (pas de sigmoid) avec contrainte de Lipschitz :\n- **WGAN** : weight clipping\n- **WGAN-GP** : gradient penalty (Î»Â·(||âˆ‡D(xÌ‚)||â‚‚ âˆ’ 1)Â²)`,
      },
      {
        type: 'equation',
        content: '\\min_G \\max_{D \\in \\mathcal{D}_L} \\; \\mathbb{E}_{\\mathbf{x}}[D(\\mathbf{x})] - \\mathbb{E}_{\\mathbf{z}}[D(G(\\mathbf{z}))]',
        label: 'Ã‰q. 15.8 â€” Objectif WGAN',
      },
      {
        type: 'callout',
        content: 'âš¡ **Astuces d\'entraÃ®nement GAN** :\nâ€¢ Utiliser \\`LeakyReLU(0.2)\\` dans D (pas ReLU)\nâ€¢ \\`BatchNorm\\` dans G (pas dans D â€” ou spectral norm)\nâ€¢ Labels lissÃ©s : 0.9 au lieu de 1.0 pour les vrais\nâ€¢ Adam avec lr=0.0002, betas=(0.5, 0.999)\nâ€¢ EntraÃ®ner D plus souvent que G (k=5 pour WGAN)\nâ€¢ Architectures notables : DCGAN, StyleGAN, StyleGAN2, StyleGAN3',
      },
    ],
    exercises: [
      {
        id: 'gan-ex1',
        title: 'ğŸ’» Pratique â€” GAN simple sur 2D',
        instructions: 'ImplÃ©mentez un GAN qui apprend Ã  gÃ©nÃ©rer des points sur un cercle (distribution 2D simple).',
        starterCode: `import torch
import torch.nn as nn

torch.manual_seed(42)

# â”€â”€ Distribution cible : points sur un cercle â”€â”€
def sample_circle(n, noise=0.05):
    theta = torch.rand(n) * 2 * 3.14159
    x = torch.stack([torch.cos(theta), torch.sin(theta)], dim=1)
    return x + torch.randn_like(x) * noise

# â”€â”€ GÃ©nÃ©rateur â”€â”€
G = nn.Sequential(
    nn.Linear(2, 64),
    nn.ReLU(),
    nn.Linear(64, 64),
    nn.ReLU(),
    nn.Linear(64, 2),
)

# â”€â”€ Discriminateur â”€â”€
D = nn.Sequential(
    nn.Linear(2, 64),
    nn.LeakyReLU(0.2),
    nn.Linear(64, 64),
    nn.LeakyReLU(0.2),
    nn.Linear(64, 1),
    nn.Sigmoid(),
)

opt_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))
opt_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))
criterion = nn.BCELoss()

# â”€â”€ EntraÃ®nement â”€â”€
for epoch in range(2000):
    # 1. EntraÃ®ner D
    real = sample_circle(128)
    z = torch.randn(128, 2)
    fake = G(z).detach()
    
    loss_D = criterion(D(real), torch.ones(128, 1)) + \\
             criterion(D(fake), torch.zeros(128, 1))
    opt_D.zero_grad()
    loss_D.backward()
    opt_D.step()
    
    # 2. EntraÃ®ner G
    z = torch.randn(128, 2)
    fake = G(z)
    loss_G = criterion(D(fake), torch.ones(128, 1))  # tromper D
    opt_G.zero_grad()
    loss_G.backward()
    opt_G.step()
    
    if (epoch + 1) % 500 == 0:
        print(f"Epoch {epoch+1:4d} | D loss: {loss_D.item():.4f} | G loss: {loss_G.item():.4f}")

# â”€â”€ RÃ©sultat â”€â”€
z = torch.randn(500, 2)
generated = G(z).detach()
print(f"\\nMoyenne rayon gÃ©nÃ©rÃ©: {generated.norm(dim=1).mean():.3f} (cible â‰ˆ 1.0)")`,
        solution: `import torch
import torch.nn as nn

torch.manual_seed(42)

def sample_circle(n, noise=0.05):
    theta = torch.rand(n) * 2 * 3.14159
    x = torch.stack([torch.cos(theta), torch.sin(theta)], dim=1)
    return x + torch.randn_like(x) * noise

G = nn.Sequential(
    nn.Linear(2, 64), nn.ReLU(),
    nn.Linear(64, 64), nn.ReLU(),
    nn.Linear(64, 2),
)

D = nn.Sequential(
    nn.Linear(2, 64), nn.LeakyReLU(0.2),
    nn.Linear(64, 64), nn.LeakyReLU(0.2),
    nn.Linear(64, 1), nn.Sigmoid(),
)

opt_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))
opt_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))
criterion = nn.BCELoss()

for epoch in range(2000):
    real = sample_circle(128)
    z = torch.randn(128, 2)
    fake = G(z).detach()
    
    loss_D = criterion(D(real), torch.ones(128, 1)) + \\
             criterion(D(fake), torch.zeros(128, 1))
    opt_D.zero_grad()
    loss_D.backward()
    opt_D.step()
    
    z = torch.randn(128, 2)
    fake = G(z)
    loss_G = criterion(D(fake), torch.ones(128, 1))
    opt_G.zero_grad()
    loss_G.backward()
    opt_G.step()
    
    if (epoch + 1) % 500 == 0:
        print(f"Epoch {epoch+1:4d} | D loss: {loss_D.item():.4f} | G loss: {loss_G.item():.4f}")

z = torch.randn(500, 2)
generated = G(z).detach()
print(f"\\nMoyenne rayon gÃ©nÃ©rÃ©: {generated.norm(dim=1).mean():.3f} (cible â‰ˆ 1.0)")`,
        hints: [
          'G(z).detach() empÃªche les gradients de G de fuiter dans D',
          'loss_G utilise torch.ones â€” on veut que D dise "vrai" pour les faux',
        ],
        completed: false,
      },
      {
        id: 'gan-th1',
        title: 'ğŸ§  ThÃ©orie â€” Comparer GAN vs WGAN',
        instructions: 'ImplÃ©mentez un WGAN-GP (Wasserstein GAN with Gradient Penalty) et comparez la stabilitÃ© avec un GAN classique.',
        starterCode: `import torch
import torch.nn as nn

# â”€â”€ Gradient Penalty (WGAN-GP) â”€â”€
def gradient_penalty(D, real, fake, lambda_gp=10.0):
    """Calcule la pÃ©nalitÃ© de gradient pour WGAN-GP"""
    alpha = torch.rand(real.size(0), 1)
    interpolated = (alpha * real + (1 - alpha) * fake).requires_grad_(True)
    d_out = D(interpolated)
    
    gradients = torch.autograd.grad(
        outputs=d_out,
        inputs=interpolated,
        grad_outputs=torch.ones_like(d_out),
        create_graph=True,
        retain_graph=True,
    )[0]
    
    gp = lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    return gp

print("ğŸ’¡ WGAN-GP : le critique (D sans sigmoid) est contraint")
print("   Ã  Ãªtre 1-Lipschitz via la pÃ©nalitÃ© de gradient")
print("   â†’ gradients stables, pas de mode collapse")`,
        solution: `import torch
import torch.nn as nn

def gradient_penalty(D, real, fake, lambda_gp=10.0):
    alpha = torch.rand(real.size(0), 1)
    interpolated = (alpha * real + (1 - alpha) * fake).requires_grad_(True)
    d_out = D(interpolated)
    
    gradients = torch.autograd.grad(
        outputs=d_out,
        inputs=interpolated,
        grad_outputs=torch.ones_like(d_out),
        create_graph=True,
        retain_graph=True,
    )[0]
    
    gp = lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    return gp

print("WGAN-GP : le critique est contraint Ã  Ãªtre 1-Lipschitz via gradient penalty")`,
        hints: [
          'Le critique WGAN n\'a PAS de sigmoid â€” sortie non bornÃ©e',
          'La gradient penalty interpole entre vrais et faux pour contraindre ||âˆ‡D|| â‰ˆ 1',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Generative Adversarial Networks â€” Ch. 15 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. Architecture GAN de base â”€â”€
print("â•â•â• GAN Architecture â•â•â•")

class Generator(nn.Module):
    def __init__(self, latent_dim=100, img_dim=784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, img_dim),
            nn.Tanh()  # sortie dans [-1, 1]
        )
    
    def forward(self, z):
        return self.net(z)

class Discriminator(nn.Module):
    def __init__(self, img_dim=784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(img_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()  # P(rÃ©el)
        )
    
    def forward(self, x):
        return self.net(x)

G = Generator()
D = Discriminator()

z = torch.randn(4, 100)
fake = G(z)
score = D(fake)

print(f"Bruit z:        {z.shape}")
print(f"Image gÃ©nÃ©rÃ©e:  {fake.shape}")
print(f"Score D(G(z)):  {score.squeeze().tolist()}")
print(f"G params: {sum(p.numel() for p in G.parameters()):,}")
print(f"D params: {sum(p.numel() for p in D.parameters()):,}")

# â”€â”€ 2. Losses â”€â”€
print("\\nâ•â•â• GAN Losses â•â•â•")
criterion = nn.BCELoss()
real_data = torch.randn(4, 784)

# D loss
loss_real = criterion(D(real_data), torch.ones(4, 1))
loss_fake = criterion(D(G(z).detach()), torch.zeros(4, 1))
loss_D = loss_real + loss_fake
print(f"D loss: {loss_D.item():.4f}")

# G loss (non-saturating)
loss_G = criterion(D(G(z)), torch.ones(4, 1))
print(f"G loss: {loss_G.item():.4f}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 14 â€” DIFFUSION MODELS (Ch. 18)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'diffusion',
    title: 'ModÃ¨les de Diffusion',
    shortTitle: 'Diffusion',
    description: 'Forward/reverse process, DDPM, reparameterization trick, U-Net denoiser, classifier-free guidance (Ch. 18 â€” UDL).',
    status: 'locked',
    progress: 0,
    dependencies: ['attention'],
    category: 'advanced',
    theory: [
      {
        type: 'text',
        content: `## 18.1 â€” L'idÃ©e de la diffusion\n\nLes **modÃ¨les de diffusion** apprennent Ã  gÃ©nÃ©rer des donnÃ©es en inversant un processus de bruitage progressif en T Ã©tapes (typiquement T=1000).\n\n- **Forward process** q(xâ‚œ|xâ‚œâ‚‹â‚) : ajouter un petit peu de bruit gaussien Ã  chaque Ã©tape. Au bout de T Ã©tapes, x_T â‰ˆ bruit pur N(0,I).\n- **Reverse process** p_Î¸(xâ‚œâ‚‹â‚|xâ‚œ) : un rÃ©seau neuronal apprend Ã  **dÃ©bruiter** â€” retirer le bruit Ã©tape par Ã©tape pour reconstruire l'image.`,
      },
      {
        type: 'equation',
        content: 'q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}\\big(\\mathbf{x}_t;\\; \\sqrt{1-\\beta_t}\\,\\mathbf{x}_{t-1},\\; \\beta_t \\mathbf{I}\\big)',
        label: 'Ã‰q. 18.1 â€” Forward Process (1 Ã©tape)',
      },
      {
        type: 'text',
        content: `## 18.2 â€” Reparameterization trick\n\nGrÃ¢ce Ã  la propriÃ©tÃ© d'additivitÃ© des gaussiennes, on peut **sauter directement** Ã  n'importe quel timestep t sans itÃ©rer :\n\nEn posant Î±â‚œ = 1âˆ’Î²â‚œ et á¾±â‚œ = âˆâ‚›â‚Œâ‚áµ— Î±â‚›, on obtient :`,
      },
      {
        type: 'equation',
        content: '\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t}\\,\\mathbf{x}_0 + \\sqrt{1-\\bar{\\alpha}_t}\\,\\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})',
        label: 'Ã‰q. 18.5 â€” Closed-form sampling',
        highlightVar: 'x_t',
      },
      {
        type: 'diagram',
        content: `  Processus de Diffusion (DDPM)
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  Forward (bruitage progressif) â†’
  â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”
  â”‚ xâ‚€  â”‚â”€â”€â–¶â”‚ xâ‚  â”‚â”€â”€â–¶â”‚ xâ‚‚  â”‚â”€â”€â–¶Â·Â·Â·â”‚ x_T â”‚
  â”‚imageâ”‚   â”‚     â”‚   â”‚     â”‚       â”‚bruitâ”‚
  â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”˜
                                        â”‚
  â† Reverse (dÃ©bruitage appris)         â”‚
  â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”´â”€â”€â”
  â”‚ xâ‚€  â”‚â—€â”€â”€â”‚ xâ‚  â”‚â—€â”€â”€â”‚ xâ‚‚  â”‚â—€â”€â”€Â·Â·Â·â”‚ x_T â”‚
  â”‚imageâ”‚   â”‚     â”‚   â”‚     â”‚       â”‚bruitâ”‚
  â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”˜
     â–²          â–²          â–²
     â””â”€ ÎµÎ¸(xâ‚œ,t) prÃ©dit le bruit Ã  chaque Ã©tape`,
        label: 'Fig. 18.1 â€” Forward et Reverse Process',
      },
      {
        type: 'text',
        content: `## 18.3 â€” Objectif d'entraÃ®nement (DDPM)\n\nLe rÃ©seau ÎµÎ¸ apprend Ã  **prÃ©dire le bruit** Îµ ajoutÃ© Ã  xâ‚€ pour obtenir xâ‚œ. L'objectif simplifiÃ© de Ho et al. (2020) est une simple MSE :`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{t \\sim U(1,T),\\; \\mathbf{x}_0,\\; \\boldsymbol{\\epsilon}} \\Big[ \\big\\| \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\big\\|^2 \\Big]',
        label: 'Ã‰q. 18.10 â€” Objectif simplifiÃ© DDPM',
        highlightVar: 'loss',
      },
      {
        type: 'text',
        content: `## 18.4 â€” Architecture U-Net\n\nLe denoiser ÎµÎ¸ est typiquement un **U-Net** : un rÃ©seau encodeur-dÃ©codeur avec des **skip connections** entre couches de mÃªme rÃ©solution. Le timestep t est injectÃ© via un **embedding sinusoidal** (comme le positional encoding des Transformers).\n\n**Stable Diffusion** ajoute du **cross-attention** dans le U-Net pour conditionner la gÃ©nÃ©ration sur du texte (CLIP embeddings).`,
      },
      {
        type: 'text',
        content: `## 18.5 â€” Sampling (gÃ©nÃ©ration)\n\n**Algorithme DDPM Sampling** :\n1. Ã‰chantillonner x_T ~ N(0, I)\n2. Pour t = T, Tâˆ’1, ..., 1 :\n   a. PrÃ©dire le bruit : ÎµÌ‚ = ÎµÎ¸(xâ‚œ, t)\n   b. Calculer xâ‚œâ‚‹â‚ = (1/âˆšÎ±â‚œ)(xâ‚œ âˆ’ (Î²â‚œ/âˆš(1âˆ’á¾±â‚œ))ÎµÌ‚) + Ïƒâ‚œz\n3. Retourner xâ‚€\n\nâš ï¸ 1000 Ã©tapes de dÃ©bruitage = lent â†’ **DDIM** (50 Ã©tapes), **DPM-Solver** (20 Ã©tapes).`,
      },
      {
        type: 'text',
        content: `## 18.6 â€” Classifier-Free Guidance\n\nPour contrÃ´ler la gÃ©nÃ©ration avec un **prompt texte**, on entraÃ®ne le mÃªme modÃ¨le avec et sans condition (en dropout du prompt avec probabilitÃ© p=0.1). Ã€ l'infÃ©rence, on amplifie la direction conditionnelle :\n\nÎµÌ‚ = ÎµÎ¸(xâ‚œ, âˆ…) + w Â· (ÎµÎ¸(xâ‚œ, c) âˆ’ ÎµÎ¸(xâ‚œ, âˆ…))\n\nw > 1 (typiquement 7.5) renforce l'adhÃ©rence au prompt au dÃ©triment de la diversitÃ©.`,
      },
      {
        type: 'callout',
        content: 'ğŸ§  **ModÃ¨les notables** :\nâ€¢ **DDPM** (Ho 2020) : l\'article fondateur\nâ€¢ **DALL-E 2** (OpenAI) : diffusion + CLIP\nâ€¢ **Stable Diffusion** (Stability AI) : diffusion dans l\'espace latent (LDM)\nâ€¢ **Midjourney** : variante propriÃ©taire\nâ€¢ **Imagen** (Google) : cascaded diffusion\nâ€¢ **FLUX, SD3** : architectures DiT (Diffusion Transformer) â€” remplacent le U-Net par des Transformers',
      },
    ],
    exercises: [
      {
        id: 'diff-ex1',
        title: 'ğŸ’» Pratique â€” Forward Process DDPM',
        instructions: 'ImplÃ©mentez le forward process et visualisez comment une image se dÃ©grade progressivement avec le bruit.',
        starterCode: `import torch
import torch.nn as nn

torch.manual_seed(42)

# â”€â”€ Schedule de bruit â”€â”€
T = 1000
betas = torch.linspace(0.0001, 0.02, T)
alphas = 1 - betas
alpha_bar = torch.cumprod(alphas, dim=0)

# â”€â”€ Forward process : sauter directement au timestep t â”€â”€
def forward_diffusion(x0, t, noise=None):
    """Ajoute du bruit au timestep t (closed-form)"""
    if noise is None:
        noise = torch.randn_like(x0)
    sqrt_ab = torch.sqrt(alpha_bar[t]).view(-1, 1)
    sqrt_1_ab = torch.sqrt(1 - alpha_bar[t]).view(-1, 1)
    return sqrt_ab * x0 + sqrt_1_ab * noise, noise

# â”€â”€ Test sur un "signal" 1D â”€â”€
x0 = torch.sin(torch.linspace(0, 4 * 3.14159, 100)).unsqueeze(0)  # signal sinusoÃ¯dal

print(f"{'t':>6} â”‚ {'á¾±â‚œ':>8} â”‚ {'SNR (dB)':>10} â”‚ {'x_t std':>8}")
print(f"{'â”€'*6}â”€â”¼â”€{'â”€'*8}â”€â”¼â”€{'â”€'*10}â”€â”¼â”€{'â”€'*8}")

for t_val in [0, 50, 100, 250, 500, 750, 999]:
    t = torch.tensor([t_val])
    x_t, eps = forward_diffusion(x0, t)
    ab = alpha_bar[t_val].item()
    snr = 10 * torch.log10(torch.tensor(ab / (1 - ab))).item()
    print(f"{t_val:6d} â”‚ {ab:8.4f} â”‚ {snr:10.2f} â”‚ {x_t.std():8.4f}")

print(f"\\nâ†’ Ã€ t=0, signal intact (á¾±â‰ˆ1)")
print(f"â†’ Ã€ t=999, bruit pur (á¾±â‰ˆ0)")`,
        solution: `import torch
import torch.nn as nn

torch.manual_seed(42)

T = 1000
betas = torch.linspace(0.0001, 0.02, T)
alphas = 1 - betas
alpha_bar = torch.cumprod(alphas, dim=0)

def forward_diffusion(x0, t, noise=None):
    if noise is None:
        noise = torch.randn_like(x0)
    sqrt_ab = torch.sqrt(alpha_bar[t]).view(-1, 1)
    sqrt_1_ab = torch.sqrt(1 - alpha_bar[t]).view(-1, 1)
    return sqrt_ab * x0 + sqrt_1_ab * noise, noise

x0 = torch.sin(torch.linspace(0, 4 * 3.14159, 100)).unsqueeze(0)

print(f"{'t':>6} â”‚ {'á¾±â‚œ':>8} â”‚ {'SNR (dB)':>10} â”‚ {'x_t std':>8}")
print(f"{'â”€'*6}â”€â”¼â”€{'â”€'*8}â”€â”¼â”€{'â”€'*10}â”€â”¼â”€{'â”€'*8}")

for t_val in [0, 50, 100, 250, 500, 750, 999]:
    t = torch.tensor([t_val])
    x_t, eps = forward_diffusion(x0, t)
    ab = alpha_bar[t_val].item()
    snr = 10 * torch.log10(torch.tensor(ab / (1 - ab))).item()
    print(f"{t_val:6d} â”‚ {ab:8.4f} â”‚ {snr:10.2f} â”‚ {x_t.std():8.4f}")

print(f"\\nâ†’ Ã€ t=0, signal intact (á¾±â‰ˆ1)")
print(f"â†’ Ã€ t=999, bruit pur (á¾±â‰ˆ0)")`,
        hints: [
          'alpha_bar = cumprod des (1-beta)',
          'x_t = sqrt(á¾±â‚œ)Â·xâ‚€ + sqrt(1âˆ’á¾±â‚œ)Â·Îµ',
          'Le SNR dÃ©croÃ®t monotoniquement avec t',
        ],
        completed: false,
      },
      {
        id: 'diff-ex2',
        title: 'ğŸ’» Pratique â€” Denoiser simple + training loop',
        instructions: 'EntraÃ®nez un petit denoiser MLP sur des donnÃ©es 1D pour comprendre la boucle DDPM.',
        starterCode: `import torch
import torch.nn as nn

torch.manual_seed(42)

# â”€â”€ Schedule â”€â”€
T = 200
betas = torch.linspace(0.001, 0.02, T)
alphas = 1 - betas
alpha_bar = torch.cumprod(alphas, dim=0)

# â”€â”€ DonnÃ©es : distribution bimodale 1D â”€â”€
def sample_data(n):
    mix = torch.rand(n) > 0.5
    return mix.float() * 2 - 1 + torch.randn(n) * 0.1  # pics Ã  -1 et +1

# â”€â”€ Denoiser â”€â”€
class Denoiser(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 128),   # x_t + t_embed
            nn.SiLU(),
            nn.Linear(128, 128),
            nn.SiLU(),
            nn.Linear(128, 1),
        )
    
    def forward(self, x_t, t_norm):
        inp = torch.stack([x_t, t_norm], dim=-1)
        return self.net(inp).squeeze(-1)

model = Denoiser()
opt = torch.optim.Adam(model.parameters(), lr=1e-3)

# â”€â”€ Training â”€â”€
for step in range(3000):
    x0 = sample_data(256)
    t = torch.randint(0, T, (256,))
    eps = torch.randn_like(x0)
    
    x_t = torch.sqrt(alpha_bar[t]) * x0 + torch.sqrt(1 - alpha_bar[t]) * eps
    eps_pred = model(x_t, t.float() / T)
    
    loss = nn.MSELoss()(eps_pred, eps)
    opt.zero_grad()
    loss.backward()
    opt.step()
    
    if (step + 1) % 1000 == 0:
        print(f"Step {step+1:4d} | Loss: {loss.item():.4f}")

# â”€â”€ Sampling â”€â”€
print("\\nâ•â•â• Sampling â•â•â•")
x = torch.randn(1000)
for t in reversed(range(T)):
    t_batch = torch.full((1000,), t)
    eps_pred = model(x, t_batch.float() / T)
    
    alpha_t = alphas[t]
    alpha_bar_t = alpha_bar[t]
    x = (1 / torch.sqrt(alpha_t)) * (x - (betas[t] / torch.sqrt(1 - alpha_bar_t)) * eps_pred)
    if t > 0:
        x = x + torch.sqrt(betas[t]) * torch.randn_like(x)

print(f"Ã‰chantillons gÃ©nÃ©rÃ©s â€” mean: {x.mean():.3f}, std: {x.std():.3f}")
print(f"Modes attendus: -1 et +1")
print(f"Fraction < 0: {(x < 0).float().mean():.2f} (attendu â‰ˆ 0.50)")`,
        solution: `import torch
import torch.nn as nn

torch.manual_seed(42)

T = 200
betas = torch.linspace(0.001, 0.02, T)
alphas = 1 - betas
alpha_bar = torch.cumprod(alphas, dim=0)

def sample_data(n):
    mix = torch.rand(n) > 0.5
    return mix.float() * 2 - 1 + torch.randn(n) * 0.1

class Denoiser(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 128), nn.SiLU(),
            nn.Linear(128, 128), nn.SiLU(),
            nn.Linear(128, 1),
        )
    
    def forward(self, x_t, t_norm):
        inp = torch.stack([x_t, t_norm], dim=-1)
        return self.net(inp).squeeze(-1)

model = Denoiser()
opt = torch.optim.Adam(model.parameters(), lr=1e-3)

for step in range(3000):
    x0 = sample_data(256)
    t = torch.randint(0, T, (256,))
    eps = torch.randn_like(x0)
    
    x_t = torch.sqrt(alpha_bar[t]) * x0 + torch.sqrt(1 - alpha_bar[t]) * eps
    eps_pred = model(x_t, t.float() / T)
    
    loss = nn.MSELoss()(eps_pred, eps)
    opt.zero_grad()
    loss.backward()
    opt.step()
    
    if (step + 1) % 1000 == 0:
        print(f"Step {step+1:4d} | Loss: {loss.item():.4f}")

print("\\nâ•â•â• Sampling â•â•â•")
x = torch.randn(1000)
for t in reversed(range(T)):
    t_batch = torch.full((1000,), t)
    eps_pred = model(x, t_batch.float() / T)
    
    alpha_t = alphas[t]
    alpha_bar_t = alpha_bar[t]
    x = (1 / torch.sqrt(alpha_t)) * (x - (betas[t] / torch.sqrt(1 - alpha_bar_t)) * eps_pred)
    if t > 0:
        x = x + torch.sqrt(betas[t]) * torch.randn_like(x)

print(f"Ã‰chantillons gÃ©nÃ©rÃ©s â€” mean: {x.mean():.3f}, std: {x.std():.3f}")
print(f"Modes attendus: -1 et +1")
print(f"Fraction < 0: {(x < 0).float().mean():.2f} (attendu â‰ˆ 0.50)")`,
        hints: [
          'x_t = sqrt(á¾±â‚œ)Â·xâ‚€ + sqrt(1âˆ’á¾±â‚œ)Â·Îµ â€” le forward process',
          'Le sampling inverse la formule Ã©tape par Ã©tape',
          'Le denoiser reÃ§oit x_t et t normalisÃ© comme entrÃ©es',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ModÃ¨les de Diffusion â€” Ch. 18 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. Noise Schedule â”€â”€
print("â•â•â• Noise Schedule DDPM â•â•â•")
T = 1000
betas = torch.linspace(0.0001, 0.02, T)
alphas = 1 - betas
alpha_bar = torch.cumprod(alphas, dim=0)

print(f"Î²_1 = {betas[0]:.4f}, Î²_T = {betas[-1]:.4f}")
print(f"á¾±_1 = {alpha_bar[0]:.4f}, á¾±_T = {alpha_bar[-1]:.6f}")

# â”€â”€ 2. Forward Process â”€â”€
print("\\nâ•â•â• Forward Process â•â•â•")
x0 = torch.randn(1, 784)  # image "propre"

def add_noise(x0, t, noise=None):
    if noise is None:
        noise = torch.randn_like(x0)
    sqrt_ab = torch.sqrt(alpha_bar[t]).view(-1, 1)
    sqrt_1_ab = torch.sqrt(1 - alpha_bar[t]).view(-1, 1)
    return sqrt_ab * x0 + sqrt_1_ab * noise, noise

for t_val in [0, 100, 500, 999]:
    t = torch.tensor([t_val])
    x_t, _ = add_noise(x0, t)
    print(f"  t={t_val:4d}: á¾±={alpha_bar[t_val]:.4f}, ||x_t||={x_t.norm():.2f}")

# â”€â”€ 3. Simple Denoiser â”€â”€
print("\\nâ•â•â• Denoiser Architecture â•â•â•")
class SimpleDenoiser(nn.Module):
    def __init__(self, dim=784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim + 1, 512),
            nn.SiLU(),
            nn.Linear(512, 512),
            nn.SiLU(),
            nn.Linear(512, dim)
        )
    
    def forward(self, x_noisy, t_norm):
        inp = torch.cat([x_noisy, t_norm.unsqueeze(-1)], dim=-1)
        return self.net(inp)

model = SimpleDenoiser()
print(f"Params: {sum(p.numel() for p in model.parameters()):,}")

# â”€â”€ 4. Training step â”€â”€
print("\\nâ•â•â• Training Step â•â•â•")
t = torch.randint(0, T, (4,))
eps = torch.randn(4, 784)
x_t = torch.sqrt(alpha_bar[t]).unsqueeze(1) * x0 + torch.sqrt(1 - alpha_bar[t]).unsqueeze(1) * eps
eps_pred = model(x_t, t.float() / T)
loss = nn.MSELoss()(eps_pred, eps)
print(f"Loss (untrained): {loss.item():.4f}")
print(f"Objectif: prÃ©dire le bruit Îµ ajoutÃ© Ã  xâ‚€")
`,
  },
];

// â”€â”€ Graph positions for Roadmap visualization â”€â”€
export const nodePositions: Record<string, { x: number; y: number }> = {
  // Row 1 â€” Fundamentals
  'supervised-learning': { x: 100, y: 80 },
  'tensors':             { x: 350, y: 80 },
  'shallow-networks':    { x: 600, y: 80 },
  'deep-networks':       { x: 850, y: 80 },
  // Row 2 â€” Training
  'loss-functions':      { x: 100, y: 260 },
  'gradient-descent':    { x: 350, y: 260 },
  'backprop':            { x: 600, y: 260 },
  'regularization':      { x: 850, y: 260 },
  // Row 3 â€” Architectures
  'cnn':                 { x: 100, y: 440 },
  'resnet':              { x: 350, y: 440 },
  'rnn':                 { x: 600, y: 440 },
  // Row 4 â€” Advanced
  'attention':           { x: 350, y: 620 },
  'gan':                 { x: 600, y: 620 },
  'diffusion':           { x: 850, y: 620 },
};

export function getTotalProgress(nodes: CourseNode[]): number {
  const completed = nodes.filter(n => n.status === 'completed').length;
  return Math.round((completed / nodes.length) * 100);
}

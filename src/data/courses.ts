/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   Course Data Model & Sample Content
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

export interface CourseNode {
  id: string;
  title: string;
  shortTitle: string;
  description: string;
  status: 'locked' | 'available' | 'in-progress' | 'completed';
  progress: number; // 0-100
  dependencies: string[];
  category: 'fundamentals' | 'architectures' | 'training' | 'advanced';
  exercises: Exercise[];
  theory: TheoryBlock[];
  codeTemplate: string;
  expectedOutput?: string;
}

export interface Exercise {
  id: string;
  title: string;
  instructions: string;
  starterCode: string;
  solution: string;
  hints: string[];
  completed: boolean;
}

export interface TheoryBlock {
  type: 'text' | 'equation' | 'diagram' | 'callout';
  content: string;
  label?: string;
  highlightVar?: string; // variable name that links to code
}

// â”€â”€ Sample Course Data â”€â”€
// Content enriched from "Understanding Deep Learning" by Simon J.D. Prince (MIT Press)
export const courseNodes: CourseNode[] = [
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 1 â€” SUPERVISED LEARNING INTRO
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'supervised-learning',
    title: 'Introduction Ã  l\'Apprentissage SupervisÃ©',
    shortTitle: 'SupervisÃ©',
    description: 'Les bases de l\'apprentissage supervisÃ© : modÃ¨les, paramÃ¨tres, fonctions de perte et entraÃ®nement.',
    status: 'available',
    progress: 0,
    dependencies: [],
    category: 'fundamentals',
    theory: [
      {
        type: 'text',
        content: `En **apprentissage supervisÃ©**, on construit un modÃ¨le **f[x, Ï•]** qui prend une entrÃ©e **x** et produit une prÃ©diction **y**. Le modÃ¨le est une Ã©quation mathÃ©matique de forme fixe contenant des **paramÃ¨tres Ï•** qui dÃ©terminent la relation entre entrÃ©e et sortie.`,
      },
      {
        type: 'equation',
        content: 'y = f[\\mathbf{x}, \\boldsymbol{\\phi}]',
        label: 'ModÃ¨le de prÃ©diction',
        highlightVar: 'y',
      },
      {
        type: 'text',
        content: `**EntraÃ®ner** un modÃ¨le consiste Ã  trouver les paramÃ¨tres **Ï•** qui minimisent une **fonction de perte L[Ï•]**. Cette perte quantifie l'Ã©cart entre les prÃ©dictions du modÃ¨le et les sorties rÃ©elles du jeu de donnÃ©es d'entraÃ®nement {xáµ¢, yáµ¢}.`,
      },
      {
        type: 'equation',
        content: '\\hat{\\boldsymbol{\\phi}} = \\underset{\\boldsymbol{\\phi}}{\\text{argmin}} \\; \\mathcal{L}[\\boldsymbol{\\phi}]',
        label: 'Minimisation de la perte',
        highlightVar: 'loss',
      },
      {
        type: 'text',
        content: `L'exemple le plus simple est la **rÃ©gression linÃ©aire 1D** : le modÃ¨le dÃ©crit une droite y = Ï•â‚€ + Ï•â‚x, oÃ¹ Ï•â‚€ est l'ordonnÃ©e Ã  l'origine et Ï•â‚ la pente. La perte des **moindres carrÃ©s** mesure la somme des carrÃ©s des Ã©carts :`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}[\\boldsymbol{\\phi}] = \\sum_{i=1}^{I} (f[x_i, \\boldsymbol{\\phi}] - y_i)^2',
        label: 'Perte des moindres carrÃ©s (Least Squares)',
        highlightVar: 'loss',
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ AprÃ¨s l\'entraÃ®nement, on Ã©value le modÃ¨le sur des **donnÃ©es de test** sÃ©parÃ©es pour mesurer sa capacitÃ© de **gÃ©nÃ©ralisation** â€” c\'est-Ã -dire sa performance sur des exemples qu\'il n\'a jamais vus.',
      },
    ],
    exercises: [
      {
        id: 'sl-ex1',
        title: 'RÃ©gression linÃ©aire depuis zÃ©ro',
        instructions: 'ImplÃ©mentez une rÃ©gression linÃ©aire simple y = Ï•â‚€ + Ï•â‚x et calculez la perte des moindres carrÃ©s.',
        starterCode: `import torch

# DonnÃ©es d'entraÃ®nement
x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
y = torch.tensor([2.1, 3.9, 6.2, 7.8, 10.1])

# ParamÃ¨tres du modÃ¨le (Ã  initialiser)
phi_0 = ___  # ordonnÃ©e Ã  l'origine
phi_1 = ___  # pente

# PrÃ©diction : y = phi_0 + phi_1 * x
y_pred = ___

# Perte des moindres carrÃ©s
loss = ___

print(f"PrÃ©dictions: {y_pred}")
print(f"Perte: {loss.item():.4f}")`,
        solution: `import torch

# DonnÃ©es d'entraÃ®nement
x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
y = torch.tensor([2.1, 3.9, 6.2, 7.8, 10.1])

# ParamÃ¨tres du modÃ¨le
phi_0 = torch.tensor(0.0)  # ordonnÃ©e Ã  l'origine
phi_1 = torch.tensor(2.0)  # pente

# PrÃ©diction : y = phi_0 + phi_1 * x
y_pred = phi_0 + phi_1 * x

# Perte des moindres carrÃ©s
loss = torch.sum((y_pred - y) ** 2)

print(f"PrÃ©dictions: {y_pred}")
print(f"Perte: {loss.item():.4f}")`,
        hints: [
          'Initialisez phi_0 et phi_1 avec torch.tensor(valeur)',
          'y_pred = phi_0 + phi_1 * x',
          'loss = torch.sum((y_pred - y) ** 2)',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch

# â•â• Apprentissage SupervisÃ© â€” RÃ©gression LinÃ©aire â•â•

# DonnÃ©es d'entraÃ®nement (entrÃ©e â†’ sortie)
x_train = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
y_train = torch.tensor([2.1, 3.9, 6.2, 7.8, 10.1])

# ModÃ¨le : y = phi_0 + phi_1 * x
phi_0 = torch.tensor(0.0, requires_grad=True)
phi_1 = torch.tensor(0.0, requires_grad=True)

# EntraÃ®nement par descente de gradient
learning_rate = 0.01
for epoch in range(100):
    # Forward pass
    y_pred = phi_0 + phi_1 * x_train
    
    # Perte des moindres carrÃ©s
    loss = torch.sum((y_pred - y_train) ** 2)
    
    # Backward pass
    loss.backward()
    
    # Mise Ã  jour des paramÃ¨tres
    with torch.no_grad():
        phi_0 -= learning_rate * phi_0.grad
        phi_1 -= learning_rate * phi_1.grad
        phi_0.grad.zero_()
        phi_1.grad.zero_()
    
    if (epoch + 1) % 20 == 0:
        print(f"Epoch {epoch+1}: loss={loss.item():.4f}, y={phi_1.item():.2f}x + {phi_0.item():.2f}")

print(f"\\nModÃ¨le final: y = {phi_1.item():.2f}x + {phi_0.item():.2f}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 2 â€” TENSEURS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'tensors',
    title: 'Tenseurs & OpÃ©rations',
    shortTitle: 'Tenseurs',
    description: 'Comprendre les tenseurs â€” la structure de donnÃ©es fondamentale du Deep Learning â€” et leurs opÃ©rations.',
    status: 'available',
    progress: 0,
    dependencies: ['supervised-learning'],
    category: 'fundamentals',
    theory: [
      {
        type: 'text',
        content: `Un **tenseur** est la structure de donnÃ©es fondamentale du Deep Learning. C'est une gÃ©nÃ©ralisation des matrices Ã  N dimensions. En Deep Learning, toutes les donnÃ©es (images, texte, audio) et tous les paramÃ¨tres des modÃ¨les sont reprÃ©sentÃ©s comme des tenseurs.\n\n- **Scalaire** : tenseur de rang 0 (un seul nombre)\n- **Vecteur** : tenseur de rang 1 (liste de nombres)\n- **Matrice** : tenseur de rang 2 (grille 2D)\n- **Tenseur 3D+** : rang 3+ (ex: image RGB = 3Ã—HÃ—W)`,
      },
      {
        type: 'equation',
        content: '\\mathbf{T} \\in \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_n}',
        label: 'Forme d\'un tenseur de rang n',
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ En PyTorch, `torch.tensor()` crÃ©e un tenseur depuis des donnÃ©es Python. Utilisez `.shape` pour inspecter ses dimensions et `.dtype` pour son type.',
      },
      {
        type: 'text',
        content: `La **multiplication matricielle** est l'opÃ©ration la plus fondamentale en Deep Learning. C'est elle qui implÃ©mente les transformations linÃ©aires au cÅ“ur de chaque couche neuronale. Pour que AÃ—B soit dÃ©fini, le nombre de colonnes de A doit Ã©galer le nombre de lignes de B.`,
      },
      {
        type: 'equation',
        content: '\\mathbf{C} = \\mathbf{A} \\mathbf{B} \\quad \\text{oÃ¹ } C_{ij} = \\sum_{k} A_{ik} B_{kj}',
        label: 'Multiplication matricielle',
        highlightVar: 'result',
      },
      {
        type: 'text',
        content: `Le **broadcasting** permet d'effectuer des opÃ©rations entre tenseurs de tailles diffÃ©rentes. PyTorch aligne automatiquement les dimensions en partant de la droite et expand les dimensions de taille 1.`,
      },
    ],
    exercises: [
      {
        id: 'tensors-ex1',
        title: 'CrÃ©er et manipuler des tenseurs',
        instructions: 'CrÃ©ez un tenseur 3Ã—4 rempli de uns, multipliez-le par un tenseur 4Ã—2 alÃ©atoire, et affichez les formes.',
        starterCode: `import torch

# CrÃ©ez un tenseur 3x4 rempli de uns
A = ___

# CrÃ©ez un tenseur 4x2 alÃ©atoire (normal)
B = ___

# Multiplication matricielle
result = ___

print(f"Shape de A: {A.shape}")
print(f"Shape de B: {B.shape}")
print(f"Shape du rÃ©sultat: {result.shape}")
print(f"RÃ©sultat:\\n{result}")`,
        solution: `import torch

# CrÃ©ez un tenseur 3x4 rempli de uns
A = torch.ones(3, 4)

# CrÃ©ez un tenseur 4x2 alÃ©atoire (normal)
B = torch.randn(4, 2)

# Multiplication matricielle
result = torch.matmul(A, B)

print(f"Shape de A: {A.shape}")
print(f"Shape de B: {B.shape}")
print(f"Shape du rÃ©sultat: {result.shape}")
print(f"RÃ©sultat:\\n{result}")`,
        hints: [
          'torch.ones(rows, cols) crÃ©e un tenseur de uns',
          'torch.randn(rows, cols) gÃ©nÃ¨re des valeurs alÃ©atoires normales',
          'torch.matmul(A, B) ou A @ B pour la multiplication matricielle',
        ],
        completed: false,
      },
      {
        id: 'tensors-ex2',
        title: 'Broadcasting et opÃ©rations',
        instructions: 'CrÃ©ez un tenseur 3Ã—1 et un tenseur 1Ã—4. Additionnez-les pour obtenir un tenseur 3Ã—4 grÃ¢ce au broadcasting.',
        starterCode: `import torch

# Tenseur colonne 3x1
col = ___

# Tenseur ligne 1x4
row = ___

# Broadcasting : 3x1 + 1x4 â†’ 3x4
result = ___

print(f"col shape: {col.shape}")
print(f"row shape: {row.shape}")
print(f"result shape: {result.shape}")
print(f"result:\\n{result}")`,
        solution: `import torch

# Tenseur colonne 3x1
col = torch.tensor([[1.0], [2.0], [3.0]])

# Tenseur ligne 1x4
row = torch.tensor([[10.0, 20.0, 30.0, 40.0]])

# Broadcasting : 3x1 + 1x4 â†’ 3x4
result = col + row

print(f"col shape: {col.shape}")
print(f"row shape: {row.shape}")
print(f"result shape: {result.shape}")
print(f"result:\\n{result}")`,
        hints: [
          'Utilisez torch.tensor([[1.0], [2.0], [3.0]]) pour une colonne 3x1',
          'Le broadcasting aligne les dimensions depuis la droite',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch

# â•â• Exploration des Tenseurs â•â•

# Scalaire (rang 0)
scalar = torch.tensor(42.0)
print(f"Scalaire: {scalar}, shape: {scalar.shape}, ndim: {scalar.ndim}")

# Vecteur (rang 1)
vector = torch.tensor([1.0, 2.0, 3.0])
print(f"Vecteur: {vector}, shape: {vector.shape}")

# Matrice (rang 2)
matrix = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float32)
print(f"Matrice shape: {matrix.shape}")

# Tenseur 3D (rang 3) â€” comme une image RGB
image = torch.randn(3, 224, 224)
print(f"Image tensor shape: {image.shape}")

# â”€â”€ OpÃ©rations fondamentales â”€â”€
A = torch.ones(3, 4)
B = torch.randn(4, 2)
result = A @ B  # multiplication matricielle
print(f"\\n{A.shape} Ã— {B.shape} = {result.shape}")

# Broadcasting
col = torch.tensor([[1.0], [2.0], [3.0]])  # 3x1
row = torch.tensor([[10.0, 20.0, 30.0]])    # 1x3
print(f"\\nBroadcasting: {col.shape} + {row.shape} = {(col + row).shape}")
print(col + row)
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 3 â€” RÃ‰SEAUX SUPERFICIELS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'shallow-networks',
    title: 'RÃ©seaux de Neurones Superficiels',
    shortTitle: 'Shallow Net',
    description: 'Comprendre les rÃ©seaux Ã  une couche cachÃ©e, ReLU, et le thÃ©orÃ¨me d\'approximation universelle.',
    status: 'available',
    progress: 0,
    dependencies: ['tensors'],
    category: 'fundamentals',
    theory: [
      {
        type: 'text',
        content: `Un **rÃ©seau de neurones superficiel** (shallow network) est une fonction **y = f[x, Ï•]** avec une seule couche cachÃ©e. Il prend une entrÃ©e, calcule des **unitÃ©s cachÃ©es** (hidden units) via une activation, puis combine linÃ©airement ces unitÃ©s pour produire la sortie.\n\nLe rÃ©seau se dÃ©compose en trois Ã©tapes :\n1. Calculer des fonctions linÃ©aires de l'entrÃ©e\n2. Appliquer une **fonction d'activation** a[â€¢]\n3. Combiner linÃ©airement les rÃ©sultats`,
      },
      {
        type: 'equation',
        content: 'y = \\phi_0 + \\phi_1 \\, a[\\theta_{10} + \\theta_{11} x] + \\phi_2 \\, a[\\theta_{20} + \\theta_{21} x] + \\phi_3 \\, a[\\theta_{30} + \\theta_{31} x]',
        label: 'RÃ©seau superficiel (Shallow Network)',
        highlightVar: 'output',
      },
      {
        type: 'text',
        content: `La fonction d'activation la plus utilisÃ©e est le **ReLU** (Rectified Linear Unit). Elle retourne l'entrÃ©e si elle est positive, et zÃ©ro sinon. Cette simplicitÃ© rend le calcul efficace et produit des **fonctions linÃ©aires par morceaux**.`,
      },
      {
        type: 'equation',
        content: 'a[z] = \\text{ReLU}[z] = \\begin{cases} 0 & \\text{si } z < 0 \\\\ z & \\text{si } z \\geq 0 \\end{cases}',
        label: 'Rectified Linear Unit (ReLU)',
        highlightVar: 'relu',
      },
      {
        type: 'callout',
        content: 'ğŸ§  **ThÃ©orÃ¨me d\'approximation universelle** : un rÃ©seau superficiel avec suffisamment d\'unitÃ©s cachÃ©es peut approximer n\'importe quelle fonction continue sur un compact. Chaque unitÃ© cachÃ©e contribue un "joint" Ã  la fonction, crÃ©ant des rÃ©gions linÃ©aires supplÃ©mentaires.',
      },
      {
        type: 'text',
        content: `Les **unitÃ©s cachÃ©es** hâ‚, hâ‚‚, hâ‚ƒ sont des rÃ©sultats intermÃ©diaires. Chaque unitÃ© contient une fonction linÃ©aire de l'entrÃ©e, clippÃ©e par ReLU. La sortie finale est une combinaison linÃ©aire de ces unitÃ©s :\n\n**y = Ï•â‚€ + Ï•â‚hâ‚ + Ï•â‚‚hâ‚‚ + Ï•â‚ƒhâ‚ƒ**\n\nAvec D unitÃ©s cachÃ©es, on obtient un maximum de D+1 rÃ©gions linÃ©aires.`,
      },
    ],
    exercises: [
      {
        id: 'shallow-ex1',
        title: 'Un rÃ©seau Ã  une couche cachÃ©e',
        instructions: 'ImplÃ©mentez un rÃ©seau superficiel avec 3 unitÃ©s cachÃ©es et activation ReLU. Calculez la sortie pour une entrÃ©e x = 1.5.',
        starterCode: `import torch

def relu(z):
    return ___

x = torch.tensor(1.5)

# ParamÃ¨tres de la couche cachÃ©e
theta = torch.tensor([[0.5, -1.0],   # theta_10, theta_11
                       [-0.3, 0.8],   # theta_20, theta_21
                       [0.1, 1.2]])   # theta_30, theta_31

# ParamÃ¨tres de sortie
phi = torch.tensor([0.2, 0.5, -0.3, 0.7])  # phi_0, phi_1, phi_2, phi_3

# Calcul des unitÃ©s cachÃ©es
h1 = relu(___)
h2 = relu(___)
h3 = relu(___)

# Sortie
y = ___

print(f"h1={h1.item():.4f}, h2={h2.item():.4f}, h3={h3.item():.4f}")
print(f"y = {y.item():.4f}")`,
        solution: `import torch

def relu(z):
    return torch.clamp(z, min=0)

x = torch.tensor(1.5)

# ParamÃ¨tres de la couche cachÃ©e
theta = torch.tensor([[0.5, -1.0],
                       [-0.3, 0.8],
                       [0.1, 1.2]])

# ParamÃ¨tres de sortie
phi = torch.tensor([0.2, 0.5, -0.3, 0.7])

# Calcul des unitÃ©s cachÃ©es
h1 = relu(theta[0, 0] + theta[0, 1] * x)
h2 = relu(theta[1, 0] + theta[1, 1] * x)
h3 = relu(theta[2, 0] + theta[2, 1] * x)

# Sortie
y = phi[0] + phi[1] * h1 + phi[2] * h2 + phi[3] * h3

print(f"h1={h1.item():.4f}, h2={h2.item():.4f}, h3={h3.item():.4f}")
print(f"y = {y.item():.4f}")`,
        hints: [
          'relu(z) = torch.clamp(z, min=0) ou torch.max(z, torch.tensor(0.0))',
          'h1 = relu(theta[0,0] + theta[0,1] * x)',
          'y = phi[0] + phi[1]*h1 + phi[2]*h2 + phi[3]*h3',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• RÃ©seau Superficiel (Shallow Network) â•â•
# ImplÃ©mentation selon le livre "Understanding Deep Learning"

def shallow_network(x, theta, phi):
    """
    RÃ©seau Ã  1 couche cachÃ©e avec 3 unitÃ©s + ReLU
    x: entrÃ©e scalaire
    theta: paramÃ¨tres couche cachÃ©e (3x2)
    phi: paramÃ¨tres sortie (4,)
    """
    # UnitÃ©s cachÃ©es avec ReLU
    h1 = torch.relu(theta[0, 0] + theta[0, 1] * x)
    h2 = torch.relu(theta[1, 0] + theta[1, 1] * x)
    h3 = torch.relu(theta[2, 0] + theta[2, 1] * x)
    
    # Combinaison linÃ©aire
    y = phi[0] + phi[1] * h1 + phi[2] * h2 + phi[3] * h3
    return y

# ParamÃ¨tres
theta = torch.tensor([[0.5, -1.0], [-0.3, 0.8], [0.1, 1.2]])
phi = torch.tensor([0.2, 0.5, -0.3, 0.7])

# Test sur plusieurs entrÃ©es
for x_val in [-2.0, -1.0, 0.0, 1.0, 2.0]:
    x = torch.tensor(x_val)
    y = shallow_network(x, theta, phi)
    print(f"f({x_val:+.1f}) = {y.item():.4f}")

# â”€â”€ Avec PyTorch nn.Module â”€â”€
model = nn.Sequential(
    nn.Linear(1, 10),    # 10 hidden units
    nn.ReLU(),
    nn.Linear(10, 1)
)
print(f"\\nParamÃ¨tres: {sum(p.numel() for p in model.parameters())}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 4 â€” RÃ‰SEAUX PROFONDS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'deep-networks',
    title: 'RÃ©seaux de Neurones Profonds',
    shortTitle: 'Deep Nets',
    description: 'Composition de rÃ©seaux, profondeur vs largeur, et notation matricielle.',
    status: 'locked',
    progress: 0,
    dependencies: ['shallow-networks'],
    category: 'fundamentals',
    theory: [
      {
        type: 'text',
        content: `Un **rÃ©seau profond** est obtenu en **composant** plusieurs rÃ©seaux superficiels : la sortie du premier devient l'entrÃ©e du second, et ainsi de suite. Cette composition crÃ©e des fonctions beaucoup plus complexes.\n\nAvec ReLU, un rÃ©seau profond de K couches de D unitÃ©s cachÃ©es chacune peut crÃ©er jusqu'Ã  **(D+1)^K** rÃ©gions linÃ©aires, contre D+1 pour un rÃ©seau superficiel.`,
      },
      {
        type: 'equation',
        content: '\\mathbf{h}_k = a[\\boldsymbol{\\beta}_k + \\boldsymbol{\\Omega}_k \\mathbf{h}_{k-1}]',
        label: 'Couche k du rÃ©seau profond',
        highlightVar: 'hidden',
      },
      {
        type: 'text',
        content: `En notation matricielle, chaque couche applique une transformation affine (multiplication par la matrice de poids **Î©k** + biais **Î²k**) suivie d'une activation. Le rÃ©seau complet est :\n\n- Forward pass : on calcule sÃ©quentiellement hâ‚, hâ‚‚, ..., hK\n- La sortie finale fâ‚ƒ est le rÃ©sultat de la derniÃ¨re couche`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} \\mathbf{f}_0 &= \\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x} \\\\ \\mathbf{h}_k &= a[\\mathbf{f}_{k-1}] \\\\ \\mathbf{f}_k &= \\boldsymbol{\\beta}_k + \\boldsymbol{\\Omega}_k \\mathbf{h}_k \\end{aligned}',
        label: 'Forward pass complet',
        highlightVar: 'hidden',
      },
      {
        type: 'callout',
        content: 'âš¡ **Profondeur vs Largeur** : un rÃ©seau profond avec le mÃªme nombre total de paramÃ¨tres qu\'un rÃ©seau superficiel large peut reprÃ©senter des fonctions exponentiellement plus complexes. C\'est pourquoi le "deep" learning est si puissant.',
      },
    ],
    exercises: [
      {
        id: 'deep-ex1',
        title: 'Construire un rÃ©seau Ã  3 couches',
        instructions: 'CrÃ©ez un rÃ©seau nn.Module avec 3 couches cachÃ©es (784â†’256â†’128â†’64â†’10). Comptez les paramÃ¨tres.',
        starterCode: `import torch
import torch.nn as nn

class DeepNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = ___
        self.layer2 = ___
        self.layer3 = ___
        self.output = ___
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.relu(self.layer3(x))
        return self.output(x)

model = DeepNet()
print(model)

total = sum(p.numel() for p in model.parameters())
print(f"\\nTotal paramÃ¨tres: {total:,}")`,
        solution: `import torch
import torch.nn as nn

class DeepNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(784, 256)
        self.layer2 = nn.Linear(256, 128)
        self.layer3 = nn.Linear(128, 64)
        self.output = nn.Linear(64, 10)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.relu(self.layer3(x))
        return self.output(x)

model = DeepNet()
print(model)

total = sum(p.numel() for p in model.parameters())
print(f"\\nTotal paramÃ¨tres: {total:,}")`,
        hints: [
          'nn.Linear(in_features, out_features) crÃ©e une couche dense',
          'Couche 1: 784â†’256, Couche 2: 256â†’128, Couche 3: 128â†’64, Sortie: 64â†’10',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• RÃ©seaux Profonds vs Superficiels â•â•

# RÃ©seau SUPERFICIEL (1 couche cachÃ©e large)
shallow = nn.Sequential(
    nn.Linear(1, 100),
    nn.ReLU(),
    nn.Linear(100, 1)
)

# RÃ©seau PROFOND (3 couches cachÃ©es Ã©troites)
deep = nn.Sequential(
    nn.Linear(1, 20),
    nn.ReLU(),
    nn.Linear(20, 20),
    nn.ReLU(),
    nn.Linear(20, 20),
    nn.ReLU(),
    nn.Linear(20, 1)
)

shallow_params = sum(p.numel() for p in shallow.parameters())
deep_params = sum(p.numel() for p in deep.parameters())

print(f"Shallow: {shallow_params} paramÃ¨tres")
print(f"Deep:    {deep_params} paramÃ¨tres")

# Les deux ont ~300 paramÃ¨tres mais le deep peut
# reprÃ©senter des fonctions beaucoup plus complexes !

# Forward pass
x = torch.randn(5, 1)
print(f"\\nShallow output: {shallow(x).squeeze().tolist()}")
print(f"Deep output:    {deep(x).squeeze().tolist()}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 5 â€” FONCTIONS DE PERTE
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'loss-functions',
    title: 'Fonctions de Perte (Loss Functions)',
    shortTitle: 'Loss',
    description: 'Maximum de vraisemblance, MSE, Cross-Entropy â€” mesurer l\'erreur du modÃ¨le.',
    status: 'locked',
    progress: 0,
    dependencies: ['deep-networks'],
    category: 'training',
    theory: [
      {
        type: 'text',
        content: `La **fonction de perte** mesure Ã  quel point les prÃ©dictions sont Ã©loignÃ©es de la rÃ©alitÃ©. En Deep Learning, on construit les fonctions de perte via le **maximum de vraisemblance** : le modÃ¨le prÃ©dit une distribution de probabilitÃ© Pr(y|x), et on cherche les paramÃ¨tres qui maximisent la probabilitÃ© des donnÃ©es observÃ©es.`,
      },
      {
        type: 'equation',
        content: '\\hat{\\boldsymbol{\\phi}} = \\underset{\\boldsymbol{\\phi}}{\\text{argmax}} \\prod_{i=1}^{I} Pr(y_i | x_i)',
        label: 'Maximum de vraisemblance',
      },
      {
        type: 'text',
        content: `En prenant le logarithme nÃ©gatif (pour transformer le produit en somme et la maximisation en minimisation), on obtient la **perte de log-vraisemblance nÃ©gative**. Pour la rÃ©gression avec bruit gaussien, cela donne le **MSE** (Mean Squared Error) :`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}_{MSE} = \\frac{1}{I}\\sum_{i=1}^{I}(y_i - f[x_i, \\boldsymbol{\\phi}])^2',
        label: 'Mean Squared Error (RÃ©gression)',
        highlightVar: 'loss',
      },
      {
        type: 'text',
        content: `Pour la **classification binaire**, le modÃ¨le prÃ©dit une probabilitÃ© via sigmoid, et la perte est la **cross-entropie binaire**. Pour la classification **multi-classe** avec K classes, on utilise softmax + cross-entropie :`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}_{CE} = -\\sum_{i=1}^{I} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{y}_{ik})',
        label: 'Cross-Entropy Loss (Classification)',
        highlightVar: 'loss',
      },
      {
        type: 'callout',
        content: 'âš¡ La **cross-entropie** mesure la "distance" entre la distribution prÃ©dite et la distribution rÃ©elle. Elle est toujours â‰¥ 0 et vaut 0 seulement quand la prÃ©diction est parfaite.',
      },
    ],
    exercises: [
      {
        id: 'loss-ex1',
        title: 'Comparer MSE et Cross-Entropy',
        instructions: 'Calculez la perte MSE pour un problÃ¨me de rÃ©gression et la perte Cross-Entropy pour un problÃ¨me de classification.',
        starterCode: `import torch
import torch.nn as nn

# â”€â”€ RÃ©gression : MSE â”€â”€
predictions = torch.tensor([2.5, 3.2, 4.1])
targets = torch.tensor([3.0, 3.0, 4.0])

mse = nn.MSELoss()
loss_mse = ___

# â”€â”€ Classification : Cross-Entropy â”€â”€
logits = torch.tensor([[2.0, 1.0, 0.1],
                        [0.5, 2.5, 0.3]])
labels = torch.tensor([0, 1])

ce = nn.CrossEntropyLoss()
loss_ce = ___

print(f"MSE Loss: {loss_mse.item():.4f}")
print(f"CE Loss:  {loss_ce.item():.4f}")`,
        solution: `import torch
import torch.nn as nn

# â”€â”€ RÃ©gression : MSE â”€â”€
predictions = torch.tensor([2.5, 3.2, 4.1])
targets = torch.tensor([3.0, 3.0, 4.0])

mse = nn.MSELoss()
loss_mse = mse(predictions, targets)

# â”€â”€ Classification : Cross-Entropy â”€â”€
logits = torch.tensor([[2.0, 1.0, 0.1],
                        [0.5, 2.5, 0.3]])
labels = torch.tensor([0, 1])

ce = nn.CrossEntropyLoss()
loss_ce = ce(logits, labels)

print(f"MSE Loss: {loss_mse.item():.4f}")
print(f"CE Loss:  {loss_ce.item():.4f}")`,
        hints: [
          'loss_mse = mse(predictions, targets)',
          'CrossEntropyLoss prend des logits (avant softmax) et des labels entiers',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• Fonctions de Perte â€” Depuis le Maximum de Vraisemblance â•â•

# â”€â”€ 1. MSE pour la rÃ©gression â”€â”€
pred = torch.tensor([2.5, 3.2, 4.1, 1.8])
target = torch.tensor([3.0, 3.0, 4.0, 2.0])

mse = nn.MSELoss()
print(f"MSE Loss: {mse(pred, target).item():.4f}")

# Calcul manuel
manual_mse = torch.mean((pred - target) ** 2)
print(f"MSE manuelle: {manual_mse.item():.4f}")

# â”€â”€ 2. Binary Cross-Entropy â”€â”€
# Pour classification binaire (sortie sigmoid)
pred_prob = torch.tensor([0.9, 0.2, 0.8])
target_bin = torch.tensor([1.0, 0.0, 1.0])

bce = nn.BCELoss()
print(f"\\nBCE Loss: {bce(pred_prob, target_bin).item():.4f}")

# â”€â”€ 3. Cross-Entropy pour multi-classe â”€â”€
logits = torch.tensor([[2.0, 1.0, 0.1],
                        [0.5, 2.5, 0.3],
                        [0.1, 0.3, 3.0]])
labels = torch.tensor([0, 1, 2])  # classes correctes

ce = nn.CrossEntropyLoss()
print(f"CE Loss: {ce(logits, labels).item():.4f}")

# VÃ©rifier les probabilitÃ©s avec softmax
probs = torch.softmax(logits, dim=1)
print(f"\\nProbabilitÃ©s prÃ©dites:\\n{probs}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 6 â€” DESCENTE DE GRADIENT
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'gradient-descent',
    title: 'Descente de Gradient & Optimisation',
    shortTitle: 'Gradient',
    description: 'Gradient descent, SGD, Momentum et Adam â€” comment entraÃ®ner un rÃ©seau.',
    status: 'locked',
    progress: 0,
    dependencies: ['loss-functions'],
    category: 'training',
    theory: [
      {
        type: 'text',
        content: `La **descente de gradient** est l'algorithme itÃ©ratif standard pour entraÃ®ner les rÃ©seaux de neurones. On part de paramÃ¨tres initiaux alÃ©atoires, puis on rÃ©pÃ¨te deux Ã©tapes :\n\n1. **Calculer le gradient** âˆ‚L/âˆ‚Ï• de la perte par rapport aux paramÃ¨tres\n2. **Mettre Ã  jour** les paramÃ¨tres dans la direction opposÃ©e au gradient`,
      },
      {
        type: 'equation',
        content: '\\boldsymbol{\\phi} \\leftarrow \\boldsymbol{\\phi} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\phi}}',
        label: 'RÃ¨gle de mise Ã  jour (Gradient Descent)',
        highlightVar: 'grad',
      },
      {
        type: 'text',
        content: `Le **SGD** (Stochastic Gradient Descent) utilise un **mini-batch** alÃ©atoire au lieu de tout le dataset, ce qui est bien plus rapide. Le **Momentum** ajoute une "inertie" qui lisse les mises Ã  jour. **Adam** combine momentum + adaptation du learning rate par paramÃ¨tre.`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} \\mathbf{m}_t &= \\beta_1 \\mathbf{m}_{t-1} + (1-\\beta_1) \\mathbf{g}_t \\\\ \\mathbf{v}_t &= \\beta_2 \\mathbf{v}_{t-1} + (1-\\beta_2) \\mathbf{g}_t^2 \\\\ \\boldsymbol{\\phi}_t &= \\boldsymbol{\\phi}_{t-1} - \\alpha \\frac{\\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon} \\end{aligned}',
        label: 'Algorithme Adam',
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ Le **learning rate Î±** est l\'hyperparamÃ¨tre le plus important. Trop grand â†’ la perte diverge. Trop petit â†’ l\'entraÃ®nement est trop lent. Adam (Î± â‰ˆ 0.001) est le choix par dÃ©faut en pratique.',
      },
    ],
    exercises: [
      {
        id: 'gd-ex1',
        title: 'SGD vs Adam',
        instructions: 'EntraÃ®nez le mÃªme modÃ¨le avec SGD et Adam. Comparez la vitesse de convergence.',
        starterCode: `import torch
import torch.nn as nn
import torch.optim as optim

torch.manual_seed(42)

# DonnÃ©es : y = 3x + 2
x = torch.randn(100, 1)
y = 3 * x + 2 + torch.randn(100, 1) * 0.1

# ModÃ¨le avec SGD
model_sgd = nn.Linear(1, 1)
opt_sgd = ___

# ModÃ¨le avec Adam
model_adam = nn.Linear(1, 1)
opt_adam = ___

loss_fn = nn.MSELoss()

for epoch in range(200):
    # SGD step
    loss_sgd = loss_fn(model_sgd(x), y)
    opt_sgd.zero_grad()
    loss_sgd.backward()
    opt_sgd.step()
    
    # Adam step
    loss_adam = loss_fn(model_adam(x), y)
    opt_adam.zero_grad()
    loss_adam.backward()
    opt_adam.step()
    
    if (epoch+1) % 50 == 0:
        print(f"Epoch {epoch+1}: SGD={loss_sgd.item():.4f}, Adam={loss_adam.item():.4f}")`,
        solution: `import torch
import torch.nn as nn
import torch.optim as optim

torch.manual_seed(42)

x = torch.randn(100, 1)
y = 3 * x + 2 + torch.randn(100, 1) * 0.1

model_sgd = nn.Linear(1, 1)
opt_sgd = optim.SGD(model_sgd.parameters(), lr=0.01)

model_adam = nn.Linear(1, 1)
opt_adam = optim.Adam(model_adam.parameters(), lr=0.01)

loss_fn = nn.MSELoss()

for epoch in range(200):
    loss_sgd = loss_fn(model_sgd(x), y)
    opt_sgd.zero_grad()
    loss_sgd.backward()
    opt_sgd.step()
    
    loss_adam = loss_fn(model_adam(x), y)
    opt_adam.zero_grad()
    loss_adam.backward()
    opt_adam.step()
    
    if (epoch+1) % 50 == 0:
        print(f"Epoch {epoch+1}: SGD={loss_sgd.item():.4f}, Adam={loss_adam.item():.4f}")`,
        hints: [
          'optim.SGD(model.parameters(), lr=0.01)',
          'optim.Adam(model.parameters(), lr=0.01)',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn
import torch.optim as optim

# â•â• Descente de Gradient â€” SGD, Momentum, Adam â•â•

torch.manual_seed(42)

# DonnÃ©es synthÃ©tiques : y = 3x + 2 + bruit
x = torch.randn(200, 1)
y = 3 * x + 2 + torch.randn(200, 1) * 0.1

model = nn.Linear(1, 1)
optimizer = optim.Adam(model.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

# Boucle d'entraÃ®nement
for epoch in range(200):
    # Mini-batch (ici on utilise tout le dataset)
    pred = model(x)
    loss = loss_fn(pred, y)
    
    # Gradient â†’ mise Ã  jour
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 40 == 0:
        w, b = model.weight.item(), model.bias.item()
        print(f"Epoch {epoch+1:3d}: loss={loss.item():.4f}, y={w:.3f}x + {b:.3f}")

w, b = model.weight.item(), model.bias.item()
print(f"\\nâœ“ Appris : y = {w:.2f}x + {b:.2f}")
print(f"  RÃ©el  : y = 3.00x + 2.00")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 7 â€” BACKPROPAGATION
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'backprop',
    title: 'Backpropagation & Autograd',
    shortTitle: 'Backprop',
    description: 'Propagation arriÃ¨re du gradient â€” le cÅ“ur de l\'apprentissage.',
    status: 'locked',
    progress: 0,
    dependencies: ['gradient-descent'],
    category: 'training',
    theory: [
      {
        type: 'text',
        content: `La **backpropagation** est l'algorithme qui calcule efficacement les gradients dans un rÃ©seau profond. Elle utilise la **rÃ¨gle de la chaÃ®ne** pour propager le gradient de la perte depuis la sortie jusqu'aux entrÃ©es, couche par couche.\n\nLe processus se dÃ©roule en deux passes :\n1. **Forward pass** : calcul de toutes les activations hâ‚, hâ‚‚, ... et de la perte\n2. **Backward pass** : propagation des gradients de la sortie vers l'entrÃ©e`,
      },
      {
        type: 'equation',
        content: '\\frac{\\partial \\ell}{\\partial \\mathbf{f}_k} = \\frac{\\partial \\mathbf{h}_{k+1}}{\\partial \\mathbf{f}_k} \\cdot \\frac{\\partial \\mathbf{f}_{k+1}}{\\partial \\mathbf{h}_{k+1}} \\cdot \\frac{\\partial \\ell}{\\partial \\mathbf{f}_{k+1}}',
        label: 'RÃ¨gle de la chaÃ®ne (Backprop)',
        highlightVar: 'grad',
      },
      {
        type: 'text',
        content: `Pour chaque couche k, on calcule les gradients par rapport aux poids **Î©k** et biais **Î²k** :\n\n- âˆ‚â„“/âˆ‚Î©k = âˆ‚â„“/âˆ‚fk Â· hkáµ€\n- âˆ‚â„“/âˆ‚Î²k = âˆ‚â„“/âˆ‚fk\n\nLa dÃ©rivÃ©e de ReLU est simple : elle vaut 1 si l'entrÃ©e > 0, et 0 sinon.`,
      },
      {
        type: 'equation',
        content: '\\frac{\\partial \\, \\text{ReLU}(z)}{\\partial z} = \\begin{cases} 0 & z < 0 \\\\ 1 & z > 0 \\end{cases}',
        label: 'DÃ©rivÃ©e du ReLU',
      },
      {
        type: 'callout',
        content: 'âš¡ PyTorch implÃ©mente la backpropagation automatiquement via **Autograd**. Il suffit d\'appeler `loss.backward()` et les gradients sont calculÃ©s pour tous les paramÃ¨tres avec `requires_grad=True`.',
      },
      {
        type: 'text',
        content: `L'**initialisation des paramÃ¨tres** est cruciale. Si les poids sont trop grands, les gradients explosent. Trop petits, ils s'Ã©vanouissent. L'initialisation de **He** (pour ReLU) choisit les poids avec une variance de 2/n, oÃ¹ n est le nombre d'entrÃ©es.`,
      },
    ],
    exercises: [
      {
        id: 'bp-ex1',
        title: 'Autograd en action',
        instructions: 'Utilisez PyTorch Autograd pour calculer les gradients d\'une expression. VÃ©rifiez les rÃ©sultats manuellement.',
        starterCode: `import torch

# Variables avec suivi du gradient
x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(3.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

# Forward : y = w*x + b, loss = (y - 10)Â²
y = ___
loss = ___

print(f"y = {y.item():.2f}")
print(f"loss = {loss.item():.2f}")

# Backward
loss.backward()

print(f"\\nâˆ‚loss/âˆ‚w = {w.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚x = {x.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚b = {b.grad.item():.2f}")

# VÃ©rification manuelle :
# âˆ‚loss/âˆ‚y = 2(y-10), âˆ‚y/âˆ‚w = x â†’ âˆ‚loss/âˆ‚w = 2(y-10)*x
print(f"\\nVÃ©rification: 2*(y-10)*x = {2*(y.item()-10)*x.item():.2f}")`,
        solution: `import torch

x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(3.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

y = w * x + b
loss = (y - 10) ** 2

print(f"y = {y.item():.2f}")
print(f"loss = {loss.item():.2f}")

loss.backward()

print(f"\\nâˆ‚loss/âˆ‚w = {w.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚x = {x.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚b = {b.grad.item():.2f}")

print(f"\\nVÃ©rification: 2*(y-10)*x = {2*(y.item()-10)*x.item():.2f}")`,
        hints: [
          'y = w * x + b',
          'loss = (y - 10) ** 2',
          'loss.backward() calcule tous les gradients automatiquement',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch

# â•â• Backpropagation â€” Autograd â•â•

# â”€â”€ 1. Calcul simple â”€â”€
x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(3.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

y = w * x + b        # Forward
loss = (y - 10) ** 2  # Perte

print(f"y = wÂ·x + b = {y.item():.2f}")
print(f"loss = (y - 10)Â² = {loss.item():.2f}")

loss.backward()       # Backward

print(f"\\nâˆ‚loss/âˆ‚w = {w.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚x = {x.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚b = {b.grad.item():.2f}")

# â”€â”€ 2. Graphe de calcul plus complexe â”€â”€
a = torch.tensor(1.5, requires_grad=True)
b = torch.tensor(2.0, requires_grad=True)

c = a * b           # c = 3.0
d = torch.relu(c - 2.5)  # d = ReLU(0.5) = 0.5
e = d ** 2           # e = 0.25

e.backward()
print(f"\\nâ”€â”€ Graphe complexe â”€â”€")
print(f"âˆ‚e/âˆ‚a = {a.grad.item():.4f}")
print(f"âˆ‚e/âˆ‚b = {b.grad.item():.4f}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 8 â€” RÃ‰GULARISATION
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'regularization',
    title: 'RÃ©gularisation & GÃ©nÃ©ralisation',
    shortTitle: 'RÃ©gular.',
    description: 'L2, Dropout, Data Augmentation, Early Stopping â€” Ã©viter le surapprentissage.',
    status: 'locked',
    progress: 0,
    dependencies: ['backprop'],
    category: 'training',
    theory: [
      {
        type: 'text',
        content: `Le **surapprentissage** (overfitting) se produit quand le modÃ¨le mÃ©morise les donnÃ©es d'entraÃ®nement au lieu d'apprendre des patterns gÃ©nÃ©raux. La **rÃ©gularisation** ajoute des contraintes pour favoriser la gÃ©nÃ©ralisation.\n\nSources d'erreur :\n- **Biais** : le modÃ¨le est trop simple (underfitting)\n- **Variance** : le modÃ¨le est trop sensible aux donnÃ©es (overfitting)\n- **Bruit** : erreur irrÃ©ductible dans les donnÃ©es`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}_{reg} = \\mathcal{L}_{data} + \\lambda \\| \\boldsymbol{\\phi} \\|_2^2',
        label: 'RÃ©gularisation L2 (Weight Decay)',
        highlightVar: 'loss',
      },
      {
        type: 'text',
        content: `Le **Dropout** dÃ©sactive alÃ©atoirement une fraction p des neurones pendant l'entraÃ®nement, forÃ§ant le rÃ©seau Ã  ne pas dÃ©pendre d'un seul neurone. Ã€ l'infÃ©rence, tous les neurones sont actifs mais leurs sorties sont multipliÃ©es par (1-p).\n\n**Early Stopping** : on surveille la perte de validation et on arrÃªte l'entraÃ®nement quand elle commence Ã  augmenter.\n\n**Data Augmentation** : on augmente artificiellement le dataset (rotations, flips, crops pour les images).`,
      },
      {
        type: 'equation',
        content: '\\tilde{h}_k = h_k \\cdot m_k \\quad \\text{oÃ¹ } m_k \\sim \\text{Bernoulli}(1-p)',
        label: 'Dropout (pendant l\'entraÃ®nement)',
      },
      {
        type: 'callout',
        content: 'ğŸ§  La **Batch Normalization** normalise les activations de chaque couche pour avoir une moyenne de 0 et une variance de 1. Elle agit Ã  la fois comme rÃ©gularisation et accÃ©lÃ©rateur d\'entraÃ®nement.',
      },
    ],
    exercises: [],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• RÃ©gularisation â•â•

# â”€â”€ 1. Weight Decay (L2) â”€â”€
model = nn.Sequential(
    nn.Linear(10, 50), nn.ReLU(),
    nn.Linear(50, 1)
)

# Adam avec weight_decay = L2 regularization
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)
print("âœ“ Weight decay activÃ© (Î»=0.01)")

# â”€â”€ 2. Dropout â”€â”€
model_with_dropout = nn.Sequential(
    nn.Linear(10, 50),
    nn.ReLU(),
    nn.Dropout(p=0.3),    # 30% des neurones dÃ©sactivÃ©s
    nn.Linear(50, 50),
    nn.ReLU(),
    nn.Dropout(p=0.3),
    nn.Linear(50, 1)
)

# En mode training vs eval
x = torch.randn(1, 10)

model_with_dropout.train()
out1 = model_with_dropout(x)
out2 = model_with_dropout(x)
print(f"\\nTrain mode (dropout actif):")
print(f"  Sortie 1: {out1.item():.4f}")
print(f"  Sortie 2: {out2.item():.4f} (diffÃ©rent!)")

model_with_dropout.eval()
out3 = model_with_dropout(x)
out4 = model_with_dropout(x)
print(f"\\nEval mode (dropout dÃ©sactivÃ©):")
print(f"  Sortie 1: {out3.item():.4f}")
print(f"  Sortie 2: {out4.item():.4f} (identique!)")

# â”€â”€ 3. Batch Normalization â”€â”€
bn_model = nn.Sequential(
    nn.Linear(10, 50),
    nn.BatchNorm1d(50),  # Normalise les activations
    nn.ReLU(),
    nn.Linear(50, 1)
)
print(f"\\nâœ“ BatchNorm model: {sum(p.numel() for p in bn_model.parameters())} params")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 9 â€” CNN
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'cnn',
    title: 'RÃ©seaux Convolutifs (CNN)',
    shortTitle: 'CNN',
    description: 'Convolutions 2D, invariance, Ã©quivariance, pooling â€” la vision par ordinateur.',
    status: 'locked',
    progress: 0,
    dependencies: ['regularization'],
    category: 'architectures',
    theory: [
      {
        type: 'text',
        content: `Les **CNN** exploitent une propriÃ©tÃ© clÃ© des images : les patterns locaux (bords, textures) sont les mÃªmes quel que soit leur position. Deux concepts formalisent cette idÃ©e :\n\n- **Invariance** : f[t[x]] = f[x] â€” la sortie ne change pas sous une transformation (ex: classification)\n- **Ã‰quivariance** : f[t[x]] = t[f[x]] â€” la sortie se transforme de la mÃªme faÃ§on (ex: segmentation)`,
      },
      {
        type: 'equation',
        content: 'z_i = \\sum_{m} \\omega_m \\cdot x_{i+m}',
        label: 'Convolution 1D (kernel de taille M)',
      },
      {
        type: 'text',
        content: `La **convolution 2D** applique un filtre (kernel) qui glisse sur l'image. Ce filtre dÃ©tecte des patterns locaux (bords, coins, textures). Les mÃªmes poids sont partagÃ©s partout (equivariance Ã  la translation).\n\n**Pooling** (Max/Average) rÃ©duit la rÃ©solution spatiale et rend le rÃ©seau partiellement invariant Ã  de petites translations.`,
      },
      {
        type: 'equation',
        content: 'z_{ij} = \\sum_{m} \\sum_{n} \\omega_{mn} \\cdot x_{i+m, \\, j+n}',
        label: 'Convolution 2D',
      },
      {
        type: 'callout',
        content: 'âš¡ Un CNN typique empile : Conv â†’ ReLU â†’ Pool â†’ Conv â†’ ReLU â†’ Pool â†’ Flatten â†’ FC. Les premiÃ¨res couches dÃ©tectent des features bas-niveau (bords), les derniÃ¨res des features haut-niveau (visages, objets).',
      },
    ],
    exercises: [
      {
        id: 'cnn-ex1',
        title: 'Construire un CNN pour MNIST',
        instructions: 'CrÃ©ez un CNN simple avec 2 couches convolutives pour classifier des images 28Ã—28.',
        starterCode: `import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = ___  # 1 canal â†’ 16 filtres, kernel 3
        self.conv2 = ___  # 16 â†’ 32 filtres, kernel 3
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = ___     # 32*7*7 â†’ 10 classes
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)  # flatten
        return self.fc(x)

model = SimpleCNN()
x = torch.randn(1, 1, 28, 28)
out = model(x)
print(f"Input: {x.shape}")
print(f"Output: {out.shape}")`,
        solution: `import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = nn.Linear(32 * 7 * 7, 10)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        return self.fc(x)

model = SimpleCNN()
x = torch.randn(1, 1, 28, 28)
out = model(x)
print(f"Input: {x.shape}")
print(f"Output: {out.shape}")`,
        hints: [
          'nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)',
          'AprÃ¨s 2 MaxPool(2,2) sur 28Ã—28 â†’ 7Ã—7',
          'nn.Linear(32 * 7 * 7, 10)',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• RÃ©seaux Convolutifs (CNN) â•â•

# â”€â”€ 1. Convolution 2D simple â”€â”€
conv = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)
x = torch.randn(1, 1, 28, 28)  # batch=1, channels=1, H=28, W=28
out = conv(x)
print(f"Conv2d: {x.shape} â†’ {out.shape}")
print(f"ParamÃ¨tres conv: {conv.weight.shape} = {conv.weight.numel()} poids + {conv.bias.numel()} biais")

# â”€â”€ 2. CNN complet â”€â”€
class MNISTNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1),   # 28Ã—28 â†’ 28Ã—28
            nn.ReLU(),
            nn.MaxPool2d(2),                    # â†’ 14Ã—14
            nn.Conv2d(16, 32, 3, padding=1),   # â†’ 14Ã—14
            nn.ReLU(),
            nn.MaxPool2d(2),                    # â†’ 7Ã—7
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(32 * 7 * 7, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        return self.classifier(self.features(x))

model = MNISTNet()
out = model(torch.randn(4, 1, 28, 28))
print(f"\\nMNISTNet: batch=4 â†’ {out.shape}")
print(f"ParamÃ¨tres: {sum(p.numel() for p in model.parameters()):,}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 10 â€” RESIDUAL NETWORKS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'resnet',
    title: 'RÃ©seaux RÃ©siduels (ResNet)',
    shortTitle: 'ResNet',
    description: 'Connexions rÃ©siduelles, skip connections et Batch Normalization.',
    status: 'locked',
    progress: 0,
    dependencies: ['cnn'],
    category: 'architectures',
    theory: [
      {
        type: 'text',
        content: `Les rÃ©seaux trÃ¨s profonds (>20 couches) souffrent du problÃ¨me des **gradients Ã©vanescents** : le gradient devient exponentiellement petit en traversant chaque couche. Les **connexions rÃ©siduelles** (skip connections) rÃ©solvent ce problÃ¨me en ajoutant l'entrÃ©e directement Ã  la sortie de chaque bloc.`,
      },
      {
        type: 'equation',
        content: '\\mathbf{h}_{k+1} = \\mathbf{h}_k + f_k(\\mathbf{h}_k)',
        label: 'Connexion rÃ©siduelle',
        highlightVar: 'hidden',
      },
      {
        type: 'text',
        content: `Au lieu d'apprendre la transformation complÃ¨te h â†’ h', le rÃ©seau apprend le **rÃ©sidu** f(h) = h' - h. Si le rÃ©sidu est proche de zÃ©ro, le gradient passe directement via le skip connection.\n\nUn **bloc rÃ©siduel** typique contient :\n- BatchNorm â†’ ReLU â†’ Conv â†’ BatchNorm â†’ ReLU â†’ Conv â†’ + input`,
      },
      {
        type: 'callout',
        content: 'ğŸ§  ResNet a permis d\'entraÃ®ner des rÃ©seaux de 152+ couches. Sans skip connections, mÃªme des rÃ©seaux de 30 couches Ã©taient difficiles Ã  entraÃ®ner. L\'idÃ©e clÃ© : le rÃ©seau peut toujours "copier" l\'entrÃ©e si les couches ne sont pas utiles.',
      },
    ],
    exercises: [],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• RÃ©seaux RÃ©siduels â•â•

class ResidualBlock(nn.Module):
    """Bloc rÃ©siduel : sortie = entrÃ©e + f(entrÃ©e)"""
    def __init__(self, channels):
        super().__init__()
        self.block = nn.Sequential(
            nn.BatchNorm2d(channels),
            nn.ReLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.BatchNorm2d(channels),
            nn.ReLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
        )
    
    def forward(self, x):
        return x + self.block(x)  # Skip connection !

class SimpleResNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.res1 = ResidualBlock(32)
        self.res2 = ResidualBlock(32)
        self.res3 = ResidualBlock(32)
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(32, 10)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.res1(x)
        x = self.res2(x)
        x = self.res3(x)
        x = self.pool(x).flatten(1)
        return self.fc(x)

model = SimpleResNet()
x = torch.randn(2, 1, 28, 28)
out = model(x)
print(f"SimpleResNet: {x.shape} â†’ {out.shape}")
print(f"ParamÃ¨tres: {sum(p.numel() for p in model.parameters()):,}")
print(f"Profondeur: 1 conv + 6 conv (3 blocs Ã— 2) = 7 couches conv")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 11 â€” RNN/LSTM
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'rnn',
    title: 'RÃ©seaux RÃ©currents (RNN/LSTM)',
    shortTitle: 'RNN',
    description: 'Traitement des sÃ©quences avec mÃ©moire temporelle et portes.',
    status: 'locked',
    progress: 0,
    dependencies: ['regularization'],
    category: 'architectures',
    theory: [
      {
        type: 'text',
        content: `Les **RNN** traitent des sÃ©quences en maintenant un **Ã©tat cachÃ©** (hidden state) qui encode l'historique. Ã€ chaque pas de temps t, le RNN reÃ§oit l'entrÃ©e xâ‚œ et l'Ã©tat prÃ©cÃ©dent hâ‚œâ‚‹â‚, et produit un nouvel Ã©tat hâ‚œ.\n\nLe problÃ¨me : les gradients s'Ã©vanouissent sur les longues sÃ©quences (long-range dependencies).`,
      },
      {
        type: 'equation',
        content: '\\mathbf{h}_t = \\tanh(\\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{W}_{xh} \\mathbf{x}_t + \\mathbf{b}_h)',
        label: 'RNN â€” Ã‰tat cachÃ©',
      },
      {
        type: 'text',
        content: `Le **LSTM** (Long Short-Term Memory) rÃ©sout le vanishing gradient avec des **portes** (gates) qui contrÃ´lent le flux d'information :\n\n- **Porte d'oubli** (forget gate) : quelle info effacer de la mÃ©moire\n- **Porte d'entrÃ©e** (input gate) : quelle nouvelle info stocker\n- **Porte de sortie** (output gate) : quelle info envoyer en sortie`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} \\mathbf{f}_t &= \\sigma(\\mathbf{W}_f [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_f) \\\\ \\mathbf{i}_t &= \\sigma(\\mathbf{W}_i [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_i) \\\\ \\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tanh(\\mathbf{W}_c [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_c) \\end{aligned}',
        label: 'LSTM â€” Portes et cellule mÃ©moire',
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ Les Transformers ont largement remplacÃ© les RNN/LSTM pour la plupart des tÃ¢ches sÃ©quentielles (NLP, audio). Cependant, les RNN restent utiles pour les sÃ©quences trÃ¨s longues et le traitement en temps rÃ©el.',
      },
    ],
    exercises: [],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• RÃ©seaux RÃ©currents â•â•

# â”€â”€ 1. RNN simple â”€â”€
rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
x = torch.randn(1, 5, 10)  # batch=1, seq_len=5, features=10
output, h_n = rnn(x)
print(f"RNN Output: {output.shape}")    # (1, 5, 20)
print(f"RNN Hidden: {h_n.shape}")       # (2, 1, 20)

# â”€â”€ 2. LSTM â”€â”€
lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
output, (h_n, c_n) = lstm(x)
print(f"\\nLSTM Output: {output.shape}")
print(f"LSTM Hidden: {h_n.shape}")
print(f"LSTM Cell:   {c_n.shape}")

# â”€â”€ 3. LSTM pour classification de sÃ©quences â”€â”€
class SeqClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x):
        output, (h_n, _) = self.lstm(x)
        # Utiliser le dernier Ã©tat cachÃ©
        return self.fc(h_n.squeeze(0))

clf = SeqClassifier(10, 32, 5)
x = torch.randn(4, 20, 10)  # batch=4, seq_len=20, features=10
out = clf(x)
print(f"\\nClassification: {x.shape} â†’ {out.shape}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 12 â€” TRANSFORMERS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'attention',
    title: 'Attention & Transformers',
    shortTitle: 'Transformer',
    description: 'Self-Attention, Multi-Head Attention et l\'architecture qui a rÃ©volutionnÃ© le NLP.',
    status: 'locked',
    progress: 0,
    dependencies: ['rnn'],
    category: 'advanced',
    theory: [
      {
        type: 'text',
        content: `Le **Transformer** est l'architecture dominante en Deep Learning moderne. Son mÃ©canisme clÃ© est le **dot-product self-attention** qui permet Ã  chaque Ã©lÃ©ment d'une sÃ©quence de "consulter" tous les autres.\n\nPour chaque entrÃ©e xâ‚˜, on calcule trois vecteurs :\n- **Value** vâ‚˜ : le contenu Ã  transmettre\n- **Query** qâ‚™ : "quelle information cherche la position n ?"\n- **Key** kâ‚˜ : "quelle information offre la position m ?"`,
      },
      {
        type: 'equation',
        content: '\\text{sa}_n[\\mathbf{x}_\\bullet] = \\sum_{m=1}^{N} a[\\mathbf{x}_m, \\mathbf{x}_n] \\cdot \\mathbf{v}_m',
        label: 'Self-Attention Output',
      },
      {
        type: 'text',
        content: `Les poids d'attention sont calculÃ©s par produit scalaire queries Ã— keys, divisÃ© par âˆšdâ‚– pour la stabilitÃ© numÃ©rique, puis passÃ©s par softmax. Le nombre de poids d'attention croÃ®t quadratiquement avec la longueur de sÃ©quence N.`,
      },
      {
        type: 'equation',
        content: '\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\!\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right) \\mathbf{V}',
        label: 'Scaled Dot-Product Attention',
      },
      {
        type: 'text',
        content: `Le **Multi-Head Attention** exÃ©cute H mÃ©canismes d'attention en parallÃ¨le, chacun apprenant des types de relations diffÃ©rents. BERT utilise H=12 tÃªtes, GPT-3 utilise H=96 tÃªtes.\n\nUne couche Transformer complÃ¨te = Multi-Head Attention + Add&Norm + FFN + Add&Norm.`,
      },
      {
        type: 'callout',
        content: 'ğŸ§  Le self-attention est un **hypernetwork** : une branche du rÃ©seau (Q,K) calcule les poids pour une autre branche (V). C\'est ce qui rend les Transformers si flexibles â€” les connexions dÃ©pendent des donnÃ©es elles-mÃªmes.',
      },
    ],
    exercises: [
      {
        id: 'attn-ex1',
        title: 'Self-Attention from scratch',
        instructions: 'ImplÃ©mentez le scaled dot-product attention manuellement (sans nn.MultiheadAttention).',
        starterCode: `import torch
import torch.nn as nn
import math

# Dimensions
d_model = 64
seq_len = 5
batch = 1

# Projections Q, K, V
W_q = nn.Linear(d_model, d_model)
W_k = nn.Linear(d_model, d_model)
W_v = nn.Linear(d_model, d_model)

x = torch.randn(batch, seq_len, d_model)

Q = W_q(x)
K = W_k(x)
V = W_v(x)

# Scaled dot-product attention
scores = ___  # Q @ K^T / sqrt(d_k)
weights = ___  # softmax(scores)
output = ___   # weights @ V

print(f"Scores shape: {scores.shape}")
print(f"Attention weights shape: {weights.shape}")
print(f"Output shape: {output.shape}")`,
        solution: `import torch
import torch.nn as nn
import math

d_model = 64
seq_len = 5
batch = 1

W_q = nn.Linear(d_model, d_model)
W_k = nn.Linear(d_model, d_model)
W_v = nn.Linear(d_model, d_model)

x = torch.randn(batch, seq_len, d_model)

Q = W_q(x)
K = W_k(x)
V = W_v(x)

scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_model)
weights = torch.softmax(scores, dim=-1)
output = weights @ V

print(f"Scores shape: {scores.shape}")
print(f"Attention weights shape: {weights.shape}")
print(f"Output shape: {output.shape}")`,
        hints: [
          'scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_model)',
          'weights = torch.softmax(scores, dim=-1)',
          'output = weights @ V',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn
import math

# â•â• Self-Attention & Transformers â•â•

class SelfAttention(nn.Module):
    """Scaled Dot-Product Self-Attention avec Multi-Head"""
    def __init__(self, d_model, n_heads):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_k = d_model // n_heads
        self.n_heads = n_heads
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, x):
        B, T, C = x.shape
        
        # Projections Q, K, V puis split en tÃªtes
        Q = self.W_q(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)
        
        # Scaled dot-product attention
        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)
        attn = torch.softmax(scores, dim=-1)
        
        # Combiner les tÃªtes
        out = (attn @ V).transpose(1, 2).contiguous().view(B, T, C)
        return self.W_o(out)

# Test
attn = SelfAttention(d_model=64, n_heads=8)
x = torch.randn(2, 10, 64)  # batch=2, seq_len=10, d_model=64
out = attn(x)
print(f"Input:  {x.shape}")
print(f"Output: {out.shape}")
print(f"Params: {sum(p.numel() for p in attn.parameters()):,}")
print(f"Heads:  {attn.n_heads}, d_k: {attn.d_k}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 13 â€” GANs
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'gan',
    title: 'Generative Adversarial Networks (GAN)',
    shortTitle: 'GAN',
    description: 'GÃ©nÃ©ration d\'images via un duel GÃ©nÃ©rateur vs Discriminateur.',
    status: 'locked',
    progress: 0,
    dependencies: ['attention'],
    category: 'advanced',
    theory: [
      {
        type: 'text',
        content: `Un **GAN** met en compÃ©tition deux rÃ©seaux :\n\n- **GÃ©nÃ©rateur G** : transforme du bruit alÃ©atoire z en donnÃ©es rÃ©alistes G(z)\n- **Discriminateur D** : distingue les donnÃ©es rÃ©elles des donnÃ©es gÃ©nÃ©rÃ©es\n\nLe gÃ©nÃ©rateur cherche Ã  tromper le discriminateur. Le discriminateur cherche Ã  ne pas Ãªtre trompÃ©. Ce jeu adversarial conduit le gÃ©nÃ©rateur Ã  produire des donnÃ©es de plus en plus rÃ©alistes.`,
      },
      {
        type: 'equation',
        content: '\\min_G \\max_D \\; \\mathbb{E}_{x}[\\log D(x)] + \\mathbb{E}_{z}[\\log(1 - D(G(z)))]',
        label: 'Objectif du GAN (Minimax)',
      },
      {
        type: 'callout',
        content: 'âš¡ Les GANs sont notoirement difficiles Ã  entraÃ®ner (mode collapse, instabilitÃ©). Des variantes comme WGAN, StyleGAN, et la progressive growing ont rÃ©solu beaucoup de ces problÃ¨mes.',
      },
    ],
    exercises: [],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• Generative Adversarial Network (GAN) â•â•

# GÃ©nÃ©rateur : bruit â†’ image
class Generator(nn.Module):
    def __init__(self, latent_dim=100, img_dim=784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, img_dim),
            nn.Tanh()
        )
    
    def forward(self, z):
        return self.net(z)

# Discriminateur : image â†’ rÃ©el/faux
class Discriminator(nn.Module):
    def __init__(self, img_dim=784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(img_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.net(x)

G = Generator()
D = Discriminator()

# GÃ©nÃ©rer une image depuis du bruit
z = torch.randn(1, 100)
fake_img = G(z)
score = D(fake_img)

print(f"Bruit z: {z.shape}")
print(f"Image gÃ©nÃ©rÃ©e: {fake_img.shape}")
print(f"Score discriminateur: {score.item():.4f} (0=faux, 1=vrai)")
print(f"\\nG params: {sum(p.numel() for p in G.parameters()):,}")
print(f"D params: {sum(p.numel() for p in D.parameters()):,}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 14 â€” DIFFUSION MODELS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'diffusion',
    title: 'ModÃ¨les de Diffusion',
    shortTitle: 'Diffusion',
    description: 'Le processus de bruitage/dÃ©bruitage qui gÃ©nÃ¨re des images photorÃ©alistes.',
    status: 'locked',
    progress: 0,
    dependencies: ['attention'],
    category: 'advanced',
    theory: [
      {
        type: 'text',
        content: `Les **modÃ¨les de diffusion** apprennent Ã  gÃ©nÃ©rer des donnÃ©es en inversant un processus de bruitage progressif. Le modÃ¨le apprend Ã  **dÃ©bruiter** â€” Ã  chaque Ã©tape, il enlÃ¨ve un peu de bruit pour reconstruire l'image originale.\n\n- **Forward process** (encoder) : ajouter progressivement du bruit gaussien Ã  l'image\n- **Reverse process** (decoder) : apprendre Ã  retirer le bruit Ã©tape par Ã©tape`,
      },
      {
        type: 'equation',
        content: 'q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1-\\beta_t} \\, \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I})',
        label: 'Forward Process (ajout de bruit)',
      },
      {
        type: 'equation',
        content: '\\mathcal{L} = \\mathbb{E}_{t, \\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\| \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\|^2 \\right]',
        label: 'Objectif simplifiÃ© (prÃ©dire le bruit)',
        highlightVar: 'loss',
      },
      {
        type: 'callout',
        content: 'ğŸ§  DALL-E, Stable Diffusion, et Midjourney utilisent tous des modÃ¨les de diffusion. L\'idÃ©e clÃ© : au lieu de gÃ©nÃ©rer une image d\'un coup, on la "dÃ©bruite" progressivement depuis du bruit pur en T Ã©tapes (typiquement T=1000).',
      },
    ],
    exercises: [],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• ModÃ¨le de Diffusion â€” Concept simplifiÃ© â•â•

class SimpleDenoiser(nn.Module):
    """RÃ©seau qui prÃ©dit le bruit ajoutÃ© Ã  une image"""
    def __init__(self, dim=784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim + 1, 512),  # +1 pour le timestep t
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, dim)
        )
    
    def forward(self, x_noisy, t):
        """PrÃ©dit le bruit epsilon Ã  partir de x_noisy et t"""
        t_embed = t.unsqueeze(-1)  # (batch, 1)
        inp = torch.cat([x_noisy, t_embed], dim=-1)
        return self.net(inp)

# â”€â”€ Forward process : ajouter du bruit â”€â”€
T = 1000
betas = torch.linspace(0.0001, 0.02, T)
alphas = 1 - betas
alpha_bar = torch.cumprod(alphas, dim=0)

def add_noise(x0, t, noise=None):
    """Ajoute du bruit au timestep t"""
    if noise is None:
        noise = torch.randn_like(x0)
    sqrt_ab = torch.sqrt(alpha_bar[t]).unsqueeze(-1)
    sqrt_1_ab = torch.sqrt(1 - alpha_bar[t]).unsqueeze(-1)
    return sqrt_ab * x0 + sqrt_1_ab * noise, noise

# Test
model = SimpleDenoiser(dim=784)
x0 = torch.randn(4, 784)  # 4 images "propres"
t = torch.randint(0, T, (4,))  # timesteps alÃ©atoires

x_noisy, true_noise = add_noise(x0, t)
pred_noise = model(x_noisy, t.float() / T)

loss = nn.MSELoss()(pred_noise, true_noise)
print(f"x0 shape: {x0.shape}")
print(f"x_noisy shape: {x_noisy.shape}")
print(f"Loss: {loss.item():.4f}")
print(f"\\nObjectif: prÃ©dire le bruit Îµ ajoutÃ© Ã  l'image")
`,
  },
];

// â”€â”€ Graph positions for Roadmap visualization â”€â”€
export const nodePositions: Record<string, { x: number; y: number }> = {
  // Row 1 â€” Fundamentals
  'supervised-learning': { x: 100, y: 80 },
  'tensors':             { x: 350, y: 80 },
  'shallow-networks':    { x: 600, y: 80 },
  'deep-networks':       { x: 850, y: 80 },
  // Row 2 â€” Training
  'loss-functions':      { x: 100, y: 260 },
  'gradient-descent':    { x: 350, y: 260 },
  'backprop':            { x: 600, y: 260 },
  'regularization':      { x: 850, y: 260 },
  // Row 3 â€” Architectures
  'cnn':                 { x: 100, y: 440 },
  'resnet':              { x: 350, y: 440 },
  'rnn':                 { x: 600, y: 440 },
  // Row 4 â€” Advanced
  'attention':           { x: 350, y: 620 },
  'gan':                 { x: 600, y: 620 },
  'diffusion':           { x: 850, y: 620 },
};

export function getTotalProgress(nodes: CourseNode[]): number {
  const completed = nodes.filter(n => n.status === 'completed').length;
  return Math.round((completed / nodes.length) * 100);
}

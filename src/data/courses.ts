/* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   Course Data Model & Sample Content
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */

export interface CourseNode {
  id: string;
  title: string;
  shortTitle: string;
  description: string;
  status: 'locked' | 'available' | 'in-progress' | 'completed';
  progress: number; // 0-100
  dependencies: string[];
  category: 'fundamentals' | 'architectures' | 'training' | 'advanced';
  exercises: Exercise[];
  theory: TheoryBlock[];
  codeTemplate: string;
  expectedOutput?: string;
}

export interface Exercise {
  id: string;
  title: string;
  instructions: string;
  starterCode: string;
  solution: string;
  hints: string[];
  completed: boolean;
}

export interface TheoryBlock {
  type: 'text' | 'equation' | 'diagram' | 'callout';
  content: string;
  label?: string;
  highlightVar?: string; // variable name that links to code
}

// â”€â”€ Sample Course Data â”€â”€
// Content enriched from "Understanding Deep Learning" by Simon J.D. Prince (MIT Press)
export const courseNodes: CourseNode[] = [
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 1 â€” SUPERVISED LEARNING INTRO
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'supervised-learning',
    title: 'Introduction Ã  l\'Apprentissage SupervisÃ©',
    shortTitle: 'SupervisÃ©',
    description: 'Les bases de l\'apprentissage supervisÃ© : modÃ¨les, paramÃ¨tres, fonctions de perte et entraÃ®nement.',
    status: 'available',
    progress: 0,
    dependencies: [],
    category: 'fundamentals',
    theory: [
      {
        type: 'text',
        content: `En **apprentissage supervisÃ©**, on construit un modÃ¨le **f[x, Ï•]** qui prend une entrÃ©e **x** et produit une prÃ©diction **y**. Le modÃ¨le est une Ã©quation mathÃ©matique de forme fixe contenant des **paramÃ¨tres Ï•** qui dÃ©terminent la relation entre entrÃ©e et sortie.`,
      },
      {
        type: 'equation',
        content: 'y = f[\\mathbf{x}, \\boldsymbol{\\phi}]',
        label: 'ModÃ¨le de prÃ©diction',
        highlightVar: 'y',
      },
      {
        type: 'text',
        content: `**EntraÃ®ner** un modÃ¨le consiste Ã  trouver les paramÃ¨tres **Ï•** qui minimisent une **fonction de perte L[Ï•]**. Cette perte quantifie l'Ã©cart entre les prÃ©dictions du modÃ¨le et les sorties rÃ©elles du jeu de donnÃ©es d'entraÃ®nement {xáµ¢, yáµ¢}.`,
      },
      {
        type: 'equation',
        content: '\\hat{\\boldsymbol{\\phi}} = \\underset{\\boldsymbol{\\phi}}{\\text{argmin}} \\; \\mathcal{L}[\\boldsymbol{\\phi}]',
        label: 'Minimisation de la perte',
        highlightVar: 'loss',
      },
      {
        type: 'text',
        content: `L'exemple le plus simple est la **rÃ©gression linÃ©aire 1D** : le modÃ¨le dÃ©crit une droite y = Ï•â‚€ + Ï•â‚x, oÃ¹ Ï•â‚€ est l'ordonnÃ©e Ã  l'origine et Ï•â‚ la pente. La perte des **moindres carrÃ©s** mesure la somme des carrÃ©s des Ã©carts :`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}[\\boldsymbol{\\phi}] = \\sum_{i=1}^{I} (f[x_i, \\boldsymbol{\\phi}] - y_i)^2',
        label: 'Perte des moindres carrÃ©s (Least Squares)',
        highlightVar: 'loss',
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ AprÃ¨s l\'entraÃ®nement, on Ã©value le modÃ¨le sur des **donnÃ©es de test** sÃ©parÃ©es pour mesurer sa capacitÃ© de **gÃ©nÃ©ralisation** â€” c\'est-Ã -dire sa performance sur des exemples qu\'il n\'a jamais vus.',
      },
    ],
    exercises: [
      {
        id: 'sl-ex1',
        title: 'RÃ©gression linÃ©aire depuis zÃ©ro',
        instructions: 'ImplÃ©mentez une rÃ©gression linÃ©aire simple y = Ï•â‚€ + Ï•â‚x et calculez la perte des moindres carrÃ©s.',
        starterCode: `import torch

# DonnÃ©es d'entraÃ®nement
x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
y = torch.tensor([2.1, 3.9, 6.2, 7.8, 10.1])

# ParamÃ¨tres du modÃ¨le (Ã  initialiser)
phi_0 = ___  # ordonnÃ©e Ã  l'origine
phi_1 = ___  # pente

# PrÃ©diction : y = phi_0 + phi_1 * x
y_pred = ___

# Perte des moindres carrÃ©s
loss = ___

print(f"PrÃ©dictions: {y_pred}")
print(f"Perte: {loss.item():.4f}")`,
        solution: `import torch

# DonnÃ©es d'entraÃ®nement
x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
y = torch.tensor([2.1, 3.9, 6.2, 7.8, 10.1])

# ParamÃ¨tres du modÃ¨le
phi_0 = torch.tensor(0.0)  # ordonnÃ©e Ã  l'origine
phi_1 = torch.tensor(2.0)  # pente

# PrÃ©diction : y = phi_0 + phi_1 * x
y_pred = phi_0 + phi_1 * x

# Perte des moindres carrÃ©s
loss = torch.sum((y_pred - y) ** 2)

print(f"PrÃ©dictions: {y_pred}")
print(f"Perte: {loss.item():.4f}")`,
        hints: [
          'Initialisez phi_0 et phi_1 avec torch.tensor(valeur)',
          'y_pred = phi_0 + phi_1 * x',
          'loss = torch.sum((y_pred - y) ** 2)',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch

# â•â• Apprentissage SupervisÃ© â€” RÃ©gression LinÃ©aire â•â•

# DonnÃ©es d'entraÃ®nement (entrÃ©e â†’ sortie)
x_train = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
y_train = torch.tensor([2.1, 3.9, 6.2, 7.8, 10.1])

# ModÃ¨le : y = phi_0 + phi_1 * x
phi_0 = torch.tensor(0.0, requires_grad=True)
phi_1 = torch.tensor(0.0, requires_grad=True)

# EntraÃ®nement par descente de gradient
learning_rate = 0.01
for epoch in range(100):
    # Forward pass
    y_pred = phi_0 + phi_1 * x_train
    
    # Perte des moindres carrÃ©s
    loss = torch.sum((y_pred - y_train) ** 2)
    
    # Backward pass
    loss.backward()
    
    # Mise Ã  jour des paramÃ¨tres
    with torch.no_grad():
        phi_0 -= learning_rate * phi_0.grad
        phi_1 -= learning_rate * phi_1.grad
        phi_0.grad.zero_()
        phi_1.grad.zero_()
    
    if (epoch + 1) % 20 == 0:
        print(f"Epoch {epoch+1}: loss={loss.item():.4f}, y={phi_1.item():.2f}x + {phi_0.item():.2f}")

print(f"\\nModÃ¨le final: y = {phi_1.item():.2f}x + {phi_0.item():.2f}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 2 â€” TENSEURS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'tensors',
    title: 'Tenseurs & OpÃ©rations',
    shortTitle: 'Tenseurs',
    description: 'Comprendre les tenseurs â€” la structure de donnÃ©es fondamentale du Deep Learning â€” et leurs opÃ©rations.',
    status: 'available',
    progress: 0,
    dependencies: ['supervised-learning'],
    category: 'fundamentals',
    theory: [
      {
        type: 'text',
        content: `Un **tenseur** est la structure de donnÃ©es fondamentale du Deep Learning. C'est une gÃ©nÃ©ralisation des matrices Ã  N dimensions. En Deep Learning, toutes les donnÃ©es (images, texte, audio) et tous les paramÃ¨tres des modÃ¨les sont reprÃ©sentÃ©s comme des tenseurs.\n\n- **Scalaire** : tenseur de rang 0 (un seul nombre)\n- **Vecteur** : tenseur de rang 1 (liste de nombres)\n- **Matrice** : tenseur de rang 2 (grille 2D)\n- **Tenseur 3D+** : rang 3+ (ex: image RGB = 3Ã—HÃ—W)`,
      },
      {
        type: 'equation',
        content: '\\mathbf{T} \\in \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_n}',
        label: 'Forme d\'un tenseur de rang n',
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ En PyTorch, `torch.tensor()` crÃ©e un tenseur depuis des donnÃ©es Python. Utilisez `.shape` pour inspecter ses dimensions et `.dtype` pour son type.',
      },
      {
        type: 'text',
        content: `La **multiplication matricielle** est l'opÃ©ration la plus fondamentale en Deep Learning. C'est elle qui implÃ©mente les transformations linÃ©aires au cÅ“ur de chaque couche neuronale. Pour que AÃ—B soit dÃ©fini, le nombre de colonnes de A doit Ã©galer le nombre de lignes de B.`,
      },
      {
        type: 'equation',
        content: '\\mathbf{C} = \\mathbf{A} \\mathbf{B} \\quad \\text{oÃ¹ } C_{ij} = \\sum_{k} A_{ik} B_{kj}',
        label: 'Multiplication matricielle',
        highlightVar: 'result',
      },
      {
        type: 'text',
        content: `Le **broadcasting** permet d'effectuer des opÃ©rations entre tenseurs de tailles diffÃ©rentes. PyTorch aligne automatiquement les dimensions en partant de la droite et expand les dimensions de taille 1.`,
      },
    ],
    exercises: [
      {
        id: 'tensors-ex1',
        title: 'CrÃ©er et manipuler des tenseurs',
        instructions: 'CrÃ©ez un tenseur 3Ã—4 rempli de uns, multipliez-le par un tenseur 4Ã—2 alÃ©atoire, et affichez les formes.',
        starterCode: `import torch

# CrÃ©ez un tenseur 3x4 rempli de uns
A = ___

# CrÃ©ez un tenseur 4x2 alÃ©atoire (normal)
B = ___

# Multiplication matricielle
result = ___

print(f"Shape de A: {A.shape}")
print(f"Shape de B: {B.shape}")
print(f"Shape du rÃ©sultat: {result.shape}")
print(f"RÃ©sultat:\\n{result}")`,
        solution: `import torch

# CrÃ©ez un tenseur 3x4 rempli de uns
A = torch.ones(3, 4)

# CrÃ©ez un tenseur 4x2 alÃ©atoire (normal)
B = torch.randn(4, 2)

# Multiplication matricielle
result = torch.matmul(A, B)

print(f"Shape de A: {A.shape}")
print(f"Shape de B: {B.shape}")
print(f"Shape du rÃ©sultat: {result.shape}")
print(f"RÃ©sultat:\\n{result}")`,
        hints: [
          'torch.ones(rows, cols) crÃ©e un tenseur de uns',
          'torch.randn(rows, cols) gÃ©nÃ¨re des valeurs alÃ©atoires normales',
          'torch.matmul(A, B) ou A @ B pour la multiplication matricielle',
        ],
        completed: false,
      },
      {
        id: 'tensors-ex2',
        title: 'Broadcasting et opÃ©rations',
        instructions: 'CrÃ©ez un tenseur 3Ã—1 et un tenseur 1Ã—4. Additionnez-les pour obtenir un tenseur 3Ã—4 grÃ¢ce au broadcasting.',
        starterCode: `import torch

# Tenseur colonne 3x1
col = ___

# Tenseur ligne 1x4
row = ___

# Broadcasting : 3x1 + 1x4 â†’ 3x4
result = ___

print(f"col shape: {col.shape}")
print(f"row shape: {row.shape}")
print(f"result shape: {result.shape}")
print(f"result:\\n{result}")`,
        solution: `import torch

# Tenseur colonne 3x1
col = torch.tensor([[1.0], [2.0], [3.0]])

# Tenseur ligne 1x4
row = torch.tensor([[10.0, 20.0, 30.0, 40.0]])

# Broadcasting : 3x1 + 1x4 â†’ 3x4
result = col + row

print(f"col shape: {col.shape}")
print(f"row shape: {row.shape}")
print(f"result shape: {result.shape}")
print(f"result:\\n{result}")`,
        hints: [
          'Utilisez torch.tensor([[1.0], [2.0], [3.0]]) pour une colonne 3x1',
          'Le broadcasting aligne les dimensions depuis la droite',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch

# â•â• Exploration des Tenseurs â•â•

# Scalaire (rang 0)
scalar = torch.tensor(42.0)
print(f"Scalaire: {scalar}, shape: {scalar.shape}, ndim: {scalar.ndim}")

# Vecteur (rang 1)
vector = torch.tensor([1.0, 2.0, 3.0])
print(f"Vecteur: {vector}, shape: {vector.shape}")

# Matrice (rang 2)
matrix = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float32)
print(f"Matrice shape: {matrix.shape}")

# Tenseur 3D (rang 3) â€” comme une image RGB
image = torch.randn(3, 224, 224)
print(f"Image tensor shape: {image.shape}")

# â”€â”€ OpÃ©rations fondamentales â”€â”€
A = torch.ones(3, 4)
B = torch.randn(4, 2)
result = A @ B  # multiplication matricielle
print(f"\\n{A.shape} Ã— {B.shape} = {result.shape}")

# Broadcasting
col = torch.tensor([[1.0], [2.0], [3.0]])  # 3x1
row = torch.tensor([[10.0, 20.0, 30.0]])    # 1x3
print(f"\\nBroadcasting: {col.shape} + {row.shape} = {(col + row).shape}")
print(col + row)
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 3 â€” RÃ‰SEAUX SUPERFICIELS
  // (Ch. 3 â€” Understanding Deep Learning)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'shallow-networks',
    title: 'RÃ©seaux de Neurones Superficiels',
    shortTitle: 'Shallow Net',
    description: 'Comprendre les rÃ©seaux Ã  une couche cachÃ©e, ReLU, les rÃ©gions linÃ©aires et le thÃ©orÃ¨me d\'approximation universelle (Ch. 3 â€” UDL).',
    status: 'available',
    progress: 0,
    dependencies: ['tensors'],
    category: 'fundamentals',
    theory: [
      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 1 : INTRODUCTION â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `Le chapitre 2 a introduit la rÃ©gression linÃ©aire (une droite). Mais une droite ne peut pas capturer de relations complexes. Les **rÃ©seaux superficiels** (shallow neural networks) dÃ©crivent des **fonctions linÃ©aires par morceaux** (piecewise linear functions) suffisamment expressives pour approximer n'importe quelle relation entre entrÃ©es et sorties.`,
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ Un rÃ©seau "superficiel" dÃ©signe un rÃ©seau avec **une seule couche cachÃ©e** (hidden layer). Ce terme contraste avec "profond" (deep), qui dÃ©signe les rÃ©seaux Ã  plusieurs couches cachÃ©es (Ch. 4).',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 2 : L'EXEMPLE DU RÃ‰SEAU â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.1 â€” Exemple de rÃ©seau neuronal\n\nConsidÃ©rons un rÃ©seau avec **10 paramÃ¨tres** Ï• = {Ï•â‚€, Ï•â‚, Ï•â‚‚, Ï•â‚ƒ, Î¸â‚â‚€, Î¸â‚â‚, Î¸â‚‚â‚€, Î¸â‚‚â‚, Î¸â‚ƒâ‚€, Î¸â‚ƒâ‚} qui transforme un scalaire x en un scalaire y. Le calcul se fait en **3 Ã©tapes** :`,
      },
      {
        type: 'diagram',
        content: `      Ã‰TAPE 1             Ã‰TAPE 2              Ã‰TAPE 3
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ 3 fonctions â”‚   â”‚  Activation  â”‚   â”‚   Combinaison     â”‚
  â”‚ linÃ©aires   â”‚â”€â”€â–¶â”‚   ReLU a[â€¢]  â”‚â”€â”€â–¶â”‚   linÃ©aire        â”‚
  â”‚ de l'entrÃ©e â”‚   â”‚  (clip < 0)  â”‚   â”‚   + offset Ï•â‚€     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Î¸â‚â‚€ + Î¸â‚â‚Â·x  â”€â”€â–¶  hâ‚ = ReLU[â€¢] â”€â”€â”
                                      â”œâ”€â”€â–¶ y = Ï•â‚€ + Ï•â‚hâ‚ + Ï•â‚‚hâ‚‚ + Ï•â‚ƒhâ‚ƒ
  Î¸â‚‚â‚€ + Î¸â‚‚â‚Â·x  â”€â”€â–¶  hâ‚‚ = ReLU[â€¢] â”€â”€â”¤
                                      â”‚
  Î¸â‚ƒâ‚€ + Î¸â‚ƒâ‚Â·x  â”€â”€â–¶  hâ‚ƒ = ReLU[â€¢] â”€â”€â”˜`,
        label: 'Fig. 3.3 â€” Pipeline de calcul d\'un rÃ©seau superficiel',
      },
      {
        type: 'equation',
        content: 'y = f[x, \\boldsymbol{\\phi}] = \\phi_0 + \\phi_1 \\, a[\\theta_{10} + \\theta_{11} x] + \\phi_2 \\, a[\\theta_{20} + \\theta_{21} x] + \\phi_3 \\, a[\\theta_{30} + \\theta_{31} x]',
        label: 'Ã‰q. 3.1 â€” RÃ©seau superficiel (Shallow Network)',
        highlightVar: 'output',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 3 : ReLU â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.1.1 â€” La fonction d'activation ReLU\n\nLa fonction d'activation **a[â€¢]** est ce qui rend le rÃ©seau **non-linÃ©aire**. Sans elle, le rÃ©seau ne serait qu'une autre fonction linÃ©aire (voir ProblÃ¨me 3.1). Le choix le plus courant est le **ReLU** (Rectified Linear Unit) :`,
      },
      {
        type: 'equation',
        content: 'a[z] = \\text{ReLU}[z] = \\max(0, z) = \\begin{cases} 0 & \\text{si } z < 0 \\\\ z & \\text{si } z \\geq 0 \\end{cases}',
        label: 'Ã‰q. 3.2 â€” Rectified Linear Unit (ReLU)',
        highlightVar: 'relu',
      },
      {
        type: 'diagram',
        content: `  y â–²
    â”‚        â•±
    â”‚       â•±
    â”‚      â•±   â† pente = 1
    â”‚     â•±
    â”‚    â•±
  â”€â”€â”¼â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ z
    â”‚  â•±
    â”‚ (clipped Ã  0 pour z < 0)
    â”‚`,
        label: 'Fig. 3.1 â€” Graphe du ReLU : retourne z si z â‰¥ 0, sinon 0',
      },
      {
        type: 'text',
        content: `**En PyTorch**, le ReLU existe sous 3 formes :\n\n- **\`torch.relu(tensor)\`** â€” fonction simple appliquÃ©e Ã  un tenseur\n- **\`torch.clamp(tensor, min=0)\`** â€” Ã©quivalent plus explicite\n- **\`nn.ReLU()\`** â€” module rÃ©utilisable dans un \`nn.Sequential\` ou \`nn.Module\`\n\n**Pourquoi ReLU est si populaire ?** Sa dÃ©rivÃ©e vaut 1 pour z > 0 et 0 pour z < 0. Cela rend le gradient stable pendant la backpropagation (contrairement au sigmoid/tanh qui "saturent").`,
      },
      {
        type: 'callout',
        content: 'âš  **Le problÃ¨me du "dying ReLU"** : si toutes les entrÃ©es d\'un neurone sont nÃ©gatives, le ReLU retourne toujours 0 et le gradient est nul. Ce neurone est "mort" â€” il ne peut plus apprendre. Solutions : Leaky ReLU (pente 0.01 pour z < 0), Parametric ReLU, ou ELU.',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 4 : UNITÃ‰S CACHÃ‰ES â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.1.2 â€” Les unitÃ©s cachÃ©es (Hidden Units)\n\nLe calcul se dÃ©compose naturellement en **unitÃ©s cachÃ©es** hâ‚, hâ‚‚, hâ‚ƒ. Chaque unitÃ© est un neurone qui applique une fonction linÃ©aire de l'entrÃ©e puis la passe par ReLU :`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} h_1 &= a[\\theta_{10} + \\theta_{11}x] \\\\ h_2 &= a[\\theta_{20} + \\theta_{21}x] \\\\ h_3 &= a[\\theta_{30} + \\theta_{31}x] \\end{aligned}',
        label: 'Ã‰q. 3.3 â€” Calcul des unitÃ©s cachÃ©es',
        highlightVar: 'relu',
      },
      {
        type: 'text',
        content: `Puis la sortie combine linÃ©airement ces unitÃ©s cachÃ©es :`,
      },
      {
        type: 'equation',
        content: 'y = \\phi_0 + \\phi_1 h_1 + \\phi_2 h_2 + \\phi_3 h_3',
        label: 'Ã‰q. 3.4 â€” Combinaison linÃ©aire de sortie',
        highlightVar: 'output',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 5 : PATTERNS D'ACTIVATION â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.1.3 â€” RÃ©gions linÃ©aires & Patterns d'activation\n\nChaque unitÃ© cachÃ©e crÃ©e un **"joint"** (coude) dans la fonction de sortie â€” le point oÃ¹ la droite Î¸â€¢â‚€ + Î¸â€¢â‚x croise zÃ©ro. De part et d'autre de ce joint, l'unitÃ© est soit **active** (z â‰¥ 0, passe l'entrÃ©e) soit **inactive** (z < 0, retourne 0).\n\nAvec 3 unitÃ©s cachÃ©es, on obtient jusqu'Ã  **4 rÃ©gions linÃ©aires** et **3 joints**. Chaque rÃ©gion correspond Ã  un **pattern d'activation** diffÃ©rent :`,
      },
      {
        type: 'diagram',
        content: `  y â–²
    â”‚         â•±â•²
    â”‚        â•±  â•²          â•±
    â”‚       â•±    â•²        â•±
    â”‚      â•±      â•²      â•±
    â”‚     â•±        â•²    â•±
    â”‚    â•±    R2    â•²  â•±
    â”‚   â•±            â•²â•±
    â”‚  â•±  R1      R3    R4
  â”€â”€â”¼â”€â•±â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–¶ x
    â”‚  jointâ‚   jointâ‚‚   jointâ‚ƒ

  R1 : hâ‚=off, hâ‚‚=off, hâ‚ƒ=off  â†’  pente = 0
  R2 : hâ‚=on,  hâ‚‚=off, hâ‚ƒ=off  â†’  pente = Ï•â‚Â·Î¸â‚â‚
  R3 : hâ‚=on,  hâ‚‚=on,  hâ‚ƒ=off  â†’  pente = Ï•â‚Â·Î¸â‚â‚ + Ï•â‚‚Â·Î¸â‚‚â‚
  R4 : hâ‚=on,  hâ‚‚=on,  hâ‚ƒ=on   â†’  pente = Ï•â‚Â·Î¸â‚â‚ + Ï•â‚‚Â·Î¸â‚‚â‚ + Ï•â‚ƒÂ·Î¸â‚ƒâ‚`,
        label: 'Fig. 3.2 â€” Fonction linÃ©aire par morceaux avec 4 rÃ©gions',
      },
      {
        type: 'callout',
        content: 'ğŸ§  **Intuition clÃ©** : la pente de chaque rÃ©gion est la somme des pentes Î¸â€¢â‚ Ã— Ï•â€¢ des unitÃ©s **actives** dans cette rÃ©gion. L\'offset Ï•â‚€ contrÃ´le la hauteur globale. C\'est ainsi qu\'on "dessine" des fonctions complexes morceau par morceau.',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 6 : NOTATION MATRICIELLE â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.2 â€” Notation matricielle & PyTorch\n\nOn regroupe le calcul en notation matricielle. Soit **Î²â‚€** le vecteur de biais de la couche cachÃ©e, **Î©â‚€** la matrice de poids d'entrÃ©e, **Î²â‚** le biais de sortie, et **Ï‰â‚** les poids de sortie :`,
      },
      {
        type: 'equation',
        content: '\\mathbf{h} = a\\!\\left[\\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x}\\right] \\qquad y = \\beta_1 + \\boldsymbol{\\omega}_1^T \\mathbf{h}',
        label: 'Notation matricielle compacte',
        highlightVar: 'hidden',
      },
      {
        type: 'text',
        content: `**En PyTorch, \`nn.Linear(in, out)\`** implÃ©mente exactement cette opÃ©ration :\n\n- Il stocke une matrice de poids **W** de taille (out Ã— in)\n- Un vecteur de biais **b** de taille (out)\n- La sortie est : **output = x @ W.T + b**\n\nUn rÃ©seau superficiel complet se construit avec :`,
      },
      {
        type: 'diagram',
        content: `  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  model = nn.Sequential(                          â”‚
  â”‚      nn.Linear(D_i, D),    # Î©â‚€Â·x + Î²â‚€          â”‚
  â”‚      nn.ReLU(),            # a[â€¢]                â”‚
  â”‚      nn.Linear(D, D_o)     # Ï‰â‚áµ€Â·h + Î²â‚          â”‚
  â”‚  )                                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  OÃ¹ :
    D_i = dimension d'entrÃ©e
    D   = nombre d'unitÃ©s cachÃ©es (hidden units)
    D_o = dimension de sortie`,
        label: 'Construction PyTorch d\'un rÃ©seau superficiel',
      },
      {
        type: 'text',
        content: `**Fonctions PyTorch utiles pour cette Ã©tape :**\n\n- **\`nn.Sequential(*layers)\`** : empile des couches en sÃ©quence, forward automatique\n- **\`nn.Linear(in_features, out_features)\`** : couche dense (transformation affine)\n- **\`nn.ReLU()\`** : module d'activation ReLU (rÃ©utilisable)\n- **\`model.parameters()\`** : itÃ©rateur sur tous les poids/biais du modÃ¨le\n- **\`sum(p.numel() for p in model.parameters())\`** : compte le nombre total de paramÃ¨tres`,
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 7 : NOMBRE DE PARAMÃˆTRES â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.3 â€” Comptage des paramÃ¨tres\n\nUn rÃ©seau superficiel avec **Dáµ¢** entrÃ©es, **D** unitÃ©s cachÃ©es et **Dâ‚’** sorties a :`,
      },
      {
        type: 'equation',
        content: 'N_{\\text{params}} = \\underbrace{D \\cdot (D_i + 1)}_{\\text{couche cachÃ©e}} + \\underbrace{D_o \\cdot (D + 1)}_{\\text{couche de sortie}}',
        label: 'Ã‰q. â€” Nombre de paramÃ¨tres (ProblÃ¨me 3.17)',
      },
      {
        type: 'text',
        content: `Exemple : Dáµ¢ = 1, D = 3, Dâ‚’ = 1 â†’ 3Ã—(1+1) + 1Ã—(3+1) = 6 + 4 = **10 paramÃ¨tres** â€” exactement les 10 de l'Ã‰q. 3.1 !\n\nExemple 2 : Dáµ¢ = 784 (image 28Ã—28), D = 100, Dâ‚’ = 10 â†’ 100Ã—785 + 10Ã—101 = 78,500 + 1,010 = **79,510 paramÃ¨tres**.`,
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 8 : THÃ‰ORÃˆME D'APPROXIMATION â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.4 â€” ThÃ©orÃ¨me d'approximation universelle\n\nAvec D unitÃ©s cachÃ©es et ReLU, le rÃ©seau crÃ©e au maximum **D + 1 rÃ©gions linÃ©aires**. Plus la fonction cible est complexe, plus il faut de rÃ©gions (et donc d'unitÃ©s) pour l'approximer :`,
      },
      {
        type: 'diagram',
        content: `  D = 2           D = 5               D = 20
  â–²               â–²                   â–²
  â”‚ â•±â•²            â”‚    â•±â•²              â”‚  Â·âˆ¼âˆ¼âˆ¼âˆ¼Â·
  â”‚â•±  â•²  â•±       â”‚   â•±  â•²  â•±â•²        â”‚ âˆ«   f(x) dx
  â”‚    â•²â•±         â”‚  â•±    â•²â•±  â•² â•±    â”‚ â‰ˆ somme de
  â”‚               â”‚ â•±          â•²â•±     â”‚   segments
  â””â”€â”€â”€â”€â”€â”€â–¶       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
  3 rÃ©gions       6 rÃ©gions           â‰ˆ courbe lisse`,
        label: 'Fig. 3.5 â€” Approximation : plus de hidden units â†’ plus de rÃ©gions â†’ meilleure fidÃ©litÃ©',
      },
      {
        type: 'callout',
        content: 'ğŸ§  **ThÃ©orÃ¨me d\'approximation universelle** (Cybenko 1989, Hornik 1991) : pour toute fonction continue f dÃ©finie sur un compact et tout Îµ > 0, il existe un rÃ©seau superficiel avec suffisamment d\'unitÃ©s cachÃ©es tel que |f(x) - rÃ©seau(x)| < Îµ pour tout x. En d\'autres termes, un rÃ©seau Ã  **une seule couche cachÃ©e peut approximer n\'importe quelle fonction continue** !',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 9 : ENTRÃ‰ES/SORTIES MULTIDIMENSIONNELLES â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.5 â€” EntrÃ©es et sorties multidimensionnelles\n\n**EntrÃ©es multiples (Dáµ¢ > 1)** : chaque unitÃ© cachÃ©e reÃ§oit une combinaison linÃ©aire de **toutes** les entrÃ©es. Par exemple avec 2 entrÃ©es x = [xâ‚, xâ‚‚]áµ€ :`,
      },
      {
        type: 'equation',
        content: 'h_d = a\\!\\left[\\theta_{d0} + \\theta_{d1} x_1 + \\theta_{d2} x_2\\right]',
        label: 'Ã‰q. 3.9 â€” UnitÃ© cachÃ©e avec 2 entrÃ©es',
      },
      {
        type: 'text',
        content: `En 2D, le ReLU crÃ©e des **hyperplans** (droites) qui divisent le plan d'entrÃ©e en **rÃ©gions convexes polygonales**. Chaque rÃ©gion a une surface linÃ©aire diffÃ©rente.\n\n**Sorties multiples (Dâ‚’ > 1)** : on utilise une combinaison linÃ©aire **diffÃ©rente** des mÃªmes unitÃ©s cachÃ©es pour chaque sortie. Les joints restent aux mÃªmes positions, mais les pentes varient.`,
      },
      {
        type: 'text',
        content: `**Nombre de rÃ©gions en haute dimension** :\n\nAvec Dáµ¢ â‰¥ 2 et D unitÃ©s cachÃ©es, le nombre maximum de rÃ©gions est donnÃ© par la formule de Zaslavsky (1975) :`,
      },
      {
        type: 'equation',
        content: 'N_{\\text{regions}} = \\sum_{j=0}^{D_i} \\binom{D}{j}',
        label: 'Formule de Zaslavsky â€” Nombre max de rÃ©gions',
      },
      {
        type: 'text',
        content: `Avec Dáµ¢ dimensions et D â‰¥ Dáµ¢ unitÃ©s cachÃ©es, on crÃ©e au minimum **2^Dáµ¢** rÃ©gions (en alignant chaque hyperplan avec un axe de coordonnÃ©es). Exemple : D = 500, Dáµ¢ = 100 â†’ plus de **10Â¹â°â·** rÃ©gions !`,
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 10 : TERMINOLOGIE â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.6 â€” Terminologie\n\nLe rÃ©seau est dÃ©crit en **couches** (layers) :\n\n- **Input layer** (couche d'entrÃ©e) : les donnÃ©es x\n- **Hidden layer** (couche cachÃ©e) : les neurones hd avec ReLU\n- **Output layer** (couche de sortie) : la prÃ©diction y\n\nAutres termes importants :`,
      },
      {
        type: 'diagram',
        content: `  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘                    VOCABULAIRE                            â•‘
  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
  â•‘ Multi-layer      â”‚ Tout rÃ©seau avec â‰¥ 1 couche cachÃ©e    â•‘
  â•‘ perceptron (MLP) â”‚ (terme historique)                    â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ Neurone / Unit   â”‚ Un Ã©lÃ©ment de la couche cachÃ©e        â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ PrÃ©-activation   â”‚ Valeur AVANT le ReLU : Î¸â‚€ + Î¸â‚x      â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ Activation       â”‚ Valeur APRÃˆS le ReLU : a[Î¸â‚€ + Î¸â‚x]   â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ Weights (poids)  â”‚ ParamÃ¨tres de pente (Î¸â‚â‚, Ï•â‚, â€¦)     â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ Biases (biais)   â”‚ ParamÃ¨tres d'offset (Î¸â‚â‚€, Ï•â‚€, â€¦)     â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ Feed-forward     â”‚ Graphe acyclique (pas de boucles)     â•‘
  â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
  â•‘ Fully connected  â”‚ Chaque neurone connectÃ© Ã  tous les    â•‘
  â•‘                  â”‚ neurones de la couche suivante        â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`,
        label: 'Fig. 3.12 â€” Terminologie des rÃ©seaux de neurones',
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 11 : AUTRES ACTIVATIONS â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'text',
        content: `## 3.7 â€” Autres fonctions d'activation\n\nLe ReLU n'est pas la seule option. Voici les alternatives les plus importantes :`,
      },
      {
        type: 'diagram',
        content: `  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Activation      â”‚ Formule                               â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Sigmoid Ïƒ(z)    â”‚  1 / (1 + eâ»á¶»)          âˆˆ (0, 1)     â”‚
  â”‚ Tanh            â”‚  (eá¶» - eâ»á¶»)/(eá¶» + eâ»á¶»)  âˆˆ (-1, 1)   â”‚
  â”‚ Leaky ReLU      â”‚  max(0.01z, z)                        â”‚
  â”‚ Parametric ReLU â”‚  max(Î±z, z)   Î± appris                â”‚
  â”‚ ELU             â”‚  z si zâ‰¥0, Î±(eá¶»-1) sinon             â”‚
  â”‚ Swish / SiLU    â”‚  z Â· Ïƒ(Î²z)    Î² appris                â”‚
  â”‚ GELU            â”‚  z Â· Î¦(z)     Î¦ = CDF gaussienne      â”‚
  â”‚ Softplus        â”‚  log(1 + eá¶»)  version lisse du ReLU   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`,
        label: 'Fig. 3.13 â€” Catalogue des fonctions d\'activation',
      },
      {
        type: 'text',
        content: `**En PyTorch**, chaque activation est disponible en module :\n\n- \`nn.ReLU()\`, \`nn.LeakyReLU(0.01)\`, \`nn.ELU(alpha=1.0)\`\n- \`nn.Sigmoid()\`, \`nn.Tanh()\`, \`nn.SiLU()\` (= Swish)\n- \`nn.GELU()\`, \`nn.Softplus()\`\n- \`nn.PReLU()\` â€” le paramÃ¨tre Î± est appris pendant l'entraÃ®nement`,
      },

      // â”€â”€â”€â”€â”€â”€â”€â”€ SECTION 12 : RÃ‰SUMÃ‰ â”€â”€â”€â”€â”€â”€â”€â”€
      {
        type: 'callout',
        content: 'âš¡ **RÃ©sumÃ© du Chapitre 3** :\n(1) Les rÃ©seaux superficiels calculent des fonctions linÃ©aires par morceaux\n(2) Chaque unitÃ© cachÃ©e ajoute un "joint" et une rÃ©gion linÃ©aire\n(3) Avec assez d\'unitÃ©s, on approxime n\'importe quelle fonction continue\n(4) Le ReLU est l\'activation standard car son gradient est simple et stable\n(5) Le nombre de paramÃ¨tres est DÂ·(Dáµ¢+1) + Dâ‚’Â·(D+1)',
      },
    ],
    exercises: [
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 1 â€” THÃ‰ORIQUE : Activation linÃ©aire
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-th1',
        title: 'ğŸ§  ThÃ©orie â€” Activation linÃ©aire (Prob. 3.1)',
        instructions: 'ProblÃ¨me 3.1 du livre : que se passe-t-il si la fonction d\'activation est **linÃ©aire** a[z] = Ïˆâ‚€ + Ïˆâ‚z au lieu de ReLU ? Prouvez-le en code : crÃ©ez un rÃ©seau avec activation linÃ©aire et montrez que le rÃ©sultat est toujours une simple droite (fonction affine), quel que soit le nombre d\'unitÃ©s cachÃ©es.',
        starterCode: `import torch

# Activation linÃ©aire : a[z] = psi_0 + psi_1 * z
psi_0, psi_1 = 0.5, 2.0

def linear_activation(z):
    return psi_0 + psi_1 * z

x = torch.tensor(1.5)

# ParamÃ¨tres couche cachÃ©e
theta = torch.tensor([[0.5, -1.0],
                       [-0.3, 0.8],
                       [0.1, 1.2]])

# ParamÃ¨tres sortie
phi = torch.tensor([0.2, 0.5, -0.3, 0.7])

# Calculez h1, h2, h3 avec activation LINÃ‰AIRE
h1 = ___
h2 = ___
h3 = ___

# Sortie
y = ___

print(f"h1={h1.item():.4f}, h2={h2.item():.4f}, h3={h3.item():.4f}")
print(f"y = {y.item():.4f}")

# Maintenant montrez que y = A*x + B (constantes)
# Calculez A et B thÃ©oriquement
A = psi_1 * (phi[1]*theta[0,1] + phi[2]*theta[1,1] + phi[3]*theta[2,1])
B_offset = psi_0 * (phi[1] + phi[2] + phi[3])
B_theta  = psi_1 * (phi[1]*theta[0,0] + phi[2]*theta[1,0] + phi[3]*theta[2,0])
B = phi[0] + B_offset + B_theta

print(f"\\nVÃ©rification : y = {A:.4f} * x + {B:.4f}")
print(f"Calcul direct : {A * 1.5 + B:.4f}")
print(f"Conclusion : avec activation linÃ©aire, le rÃ©seau est juste une DROITE !")`,
        solution: `import torch

psi_0, psi_1 = 0.5, 2.0

def linear_activation(z):
    return psi_0 + psi_1 * z

x = torch.tensor(1.5)

theta = torch.tensor([[0.5, -1.0],
                       [-0.3, 0.8],
                       [0.1, 1.2]])

phi = torch.tensor([0.2, 0.5, -0.3, 0.7])

h1 = linear_activation(theta[0, 0] + theta[0, 1] * x)
h2 = linear_activation(theta[1, 0] + theta[1, 1] * x)
h3 = linear_activation(theta[2, 0] + theta[2, 1] * x)

y = phi[0] + phi[1] * h1 + phi[2] * h2 + phi[3] * h3

print(f"h1={h1.item():.4f}, h2={h2.item():.4f}, h3={h3.item():.4f}")
print(f"y = {y.item():.4f}")

A = psi_1 * (phi[1]*theta[0,1] + phi[2]*theta[1,1] + phi[3]*theta[2,1])
B_offset = psi_0 * (phi[1] + phi[2] + phi[3])
B_theta  = psi_1 * (phi[1]*theta[0,0] + phi[2]*theta[1,0] + phi[3]*theta[2,0])
B = phi[0] + B_offset + B_theta

print(f"\\nVÃ©rification : y = {A:.4f} * x + {B:.4f}")
print(f"Calcul direct : {A * 1.5 + B:.4f}")
print(f"Conclusion : avec activation linÃ©aire, le rÃ©seau est juste une DROITE !")`,
        hints: [
          'h1 = linear_activation(theta[0,0] + theta[0,1] * x)',
          'y = phi[0] + phi[1]*h1 + phi[2]*h2 + phi[3]*h3',
          'Le rÃ©sultat est toujours de la forme AÂ·x + B, donc une droite !',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 2 â€” PRATIQUE : Forward pass ReLU
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-pr1',
        title: 'ğŸ’» Pratique â€” Forward pass avec ReLU',
        instructions: 'ImplÃ©mentez un rÃ©seau superficiel avec 3 unitÃ©s cachÃ©es et activation ReLU. Calculez la sortie pour x = -1.0, 0.0, 0.5, 1.5 et identifiez les patterns d\'activation (quelles unitÃ©s sont actives/inactives Ã  chaque x).',
        starterCode: `import torch

def relu(z):
    """ReLU : max(0, z)"""
    return ___

# ParamÃ¨tres (tirÃ©s de la Figure 3.2a du livre)
theta = torch.tensor([[-0.2, 0.4],   # Î¸â‚â‚€, Î¸â‚â‚
                       [-0.9, 0.9],   # Î¸â‚‚â‚€, Î¸â‚‚â‚
                       [ 1.1, -0.7]]) # Î¸â‚ƒâ‚€, Î¸â‚ƒâ‚

phi = torch.tensor([-0.23, -1.3, 1.3, 0.66])  # Ï•â‚€, Ï•â‚, Ï•â‚‚, Ï•â‚ƒ

def shallow_forward(x, theta, phi):
    """Forward pass d'un rÃ©seau superficiel"""
    # PrÃ©-activations
    z1 = ___
    z2 = ___
    z3 = ___
    
    # Activations (unitÃ©s cachÃ©es)
    h1 = relu(z1)
    h2 = relu(z2)
    h3 = relu(z3)
    
    # Sortie
    y = ___
    
    # Pattern d'activation : 1 si actif, 0 si inactif
    pattern = (f"h1={'ON' if h1 > 0 else 'off'}, "
               f"h2={'ON' if h2 > 0 else 'off'}, "
               f"h3={'ON' if h3 > 0 else 'off'}")
    
    return y.item(), pattern

# Test sur plusieurs entrÃ©es
for x_val in [-1.0, 0.0, 0.5, 1.0, 1.5, 2.5]:
    x = torch.tensor(x_val)
    y, pattern = shallow_forward(x, theta, phi)
    print(f"x={x_val:+.1f} â†’ y={y:.4f}  [{pattern}]")`,
        solution: `import torch

def relu(z):
    """ReLU : max(0, z)"""
    return torch.clamp(z, min=0)

theta = torch.tensor([[-0.2, 0.4],
                       [-0.9, 0.9],
                       [ 1.1, -0.7]])

phi = torch.tensor([-0.23, -1.3, 1.3, 0.66])

def shallow_forward(x, theta, phi):
    z1 = theta[0, 0] + theta[0, 1] * x
    z2 = theta[1, 0] + theta[1, 1] * x
    z3 = theta[2, 0] + theta[2, 1] * x
    
    h1 = relu(z1)
    h2 = relu(z2)
    h3 = relu(z3)
    
    y = phi[0] + phi[1] * h1 + phi[2] * h2 + phi[3] * h3
    
    pattern = (f"h1={'ON' if h1 > 0 else 'off'}, "
               f"h2={'ON' if h2 > 0 else 'off'}, "
               f"h3={'ON' if h3 > 0 else 'off'}")
    
    return y.item(), pattern

for x_val in [-1.0, 0.0, 0.5, 1.0, 1.5, 2.5]:
    x = torch.tensor(x_val)
    y, pattern = shallow_forward(x, theta, phi)
    print(f"x={x_val:+.1f} â†’ y={y:.4f}  [{pattern}]")`,
        hints: [
          'relu(z) = torch.clamp(z, min=0)',
          'z1 = theta[0, 0] + theta[0, 1] * x  (prÃ©-activation)',
          'y = phi[0] + phi[1]*h1 + phi[2]*h2 + phi[3]*h3',
          'Un neurone est ON si sa prÃ©-activation z > 0',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 3 â€” THÃ‰ORIQUE : HomogÃ©nÃ©itÃ© du ReLU
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-th2',
        title: 'ğŸ§  ThÃ©orie â€” PropriÃ©tÃ© du ReLU (Prob. 3.5)',
        instructions: 'ProblÃ¨me 3.5 : prouvez numÃ©riquement que ReLU(Î±Â·z) = Î±Â·ReLU(z) pour tout Î± â‰¥ 0 (propriÃ©tÃ© d\'homogÃ©nÃ©itÃ© non-nÃ©gative). Puis montrez que cette propriÃ©tÃ© NE tient PAS pour Î± < 0. Testez avec diffÃ©rentes valeurs de z et Î±.',
        starterCode: `import torch

def relu(z):
    return torch.clamp(z, min=0)

# Testez la propriÃ©tÃ© : ReLU(Î±Â·z) == Î±Â·ReLU(z) pour Î± â‰¥ 0
z_values = torch.tensor([-2.0, -1.0, 0.0, 1.0, 3.0])

print("â•â•â• Î± â‰¥ 0 : propriÃ©tÃ© VRAIE â•â•â•")
for alpha in [0.0, 0.5, 1.0, 2.0, 10.0]:
    lhs = relu(alpha * z_values)   # ReLU(Î±Â·z)
    rhs = alpha * relu(z_values)   # Î±Â·ReLU(z)
    equal = torch.allclose(lhs, rhs)
    print(f"  Î±={alpha:4.1f} : ReLU(Î±z) = {lhs.tolist()}")
    print(f"          Î±Â·ReLU(z) = {rhs.tolist()} â†’ {'âœ“ Ã‰GAL' if equal else 'âœ— DIFFÃ‰RENT'}")

print("\\nâ•â•â• Î± < 0 : propriÃ©tÃ© FAUSSE â•â•â•")
alpha = -1.0
lhs = relu(alpha * z_values)
rhs = alpha * relu(z_values)
print(f"  Î±={alpha:4.1f} : ReLU(Î±z) = {lhs.tolist()}")
print(f"          Î±Â·ReLU(z) = {rhs.tolist()}")
print(f"  â†’ {'âœ“ Ã‰GAL' if torch.allclose(lhs, rhs) else 'âœ— DIFFÃ‰RENT'}")

# Question bonus : pourquoi c'est important ?
print("\\nğŸ’¡ ConsÃ©quence (Prob 3.6) :")
print("   Si on multiplie Î¸â‚â‚€,Î¸â‚â‚ par Î±>0 et divise Ï•â‚ par Î±,")
print("   le rÃ©seau donne EXACTEMENT la mÃªme sortie.")
print("   â†’ Il y a une infinitÃ© de combinaisons de paramÃ¨tres Ã©quivalentes !")`,
        solution: `import torch

def relu(z):
    return torch.clamp(z, min=0)

z_values = torch.tensor([-2.0, -1.0, 0.0, 1.0, 3.0])

print("â•â•â• Î± â‰¥ 0 : propriÃ©tÃ© VRAIE â•â•â•")
for alpha in [0.0, 0.5, 1.0, 2.0, 10.0]:
    lhs = relu(alpha * z_values)
    rhs = alpha * relu(z_values)
    equal = torch.allclose(lhs, rhs)
    print(f"  Î±={alpha:4.1f} : ReLU(Î±z) = {lhs.tolist()}")
    print(f"          Î±Â·ReLU(z) = {rhs.tolist()} â†’ {'âœ“ Ã‰GAL' if equal else 'âœ— DIFFÃ‰RENT'}")

print("\\nâ•â•â• Î± < 0 : propriÃ©tÃ© FAUSSE â•â•â•")
alpha = -1.0
lhs = relu(alpha * z_values)
rhs = alpha * relu(z_values)
print(f"  Î±={alpha:4.1f} : ReLU(Î±z) = {lhs.tolist()}")
print(f"          Î±Â·ReLU(z) = {rhs.tolist()}")
print(f"  â†’ {'âœ“ Ã‰GAL' if torch.allclose(lhs, rhs) else 'âœ— DIFFÃ‰RENT'}")

print("\\nğŸ’¡ ConsÃ©quence (Prob 3.6) :")
print("   Si on multiplie Î¸â‚â‚€,Î¸â‚â‚ par Î±>0 et divise Ï•â‚ par Î±,")
print("   le rÃ©seau donne EXACTEMENT la mÃªme sortie.")
print("   â†’ Il y a une infinitÃ© de combinaisons de paramÃ¨tres Ã©quivalentes !")`,
        hints: [
          'Pour Î± â‰¥ 0 et z â‰¥ 0 : ReLU(Î±Â·z) = Î±Â·z = Î±Â·ReLU(z) âœ“',
          'Pour Î± â‰¥ 0 et z < 0 : ReLU(Î±Â·z) = 0 = Î±Â·0 = Î±Â·ReLU(z) âœ“',
          'Pour Î± < 0 et z > 0 : ReLU(Î±Â·z) = 0 â‰  Î±Â·z = Î±Â·ReLU(z) âœ—',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 4 â€” PRATIQUE : nn.Sequential
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-pr2',
        title: 'ğŸ’» Pratique â€” RÃ©seau PyTorch nn.Sequential',
        instructions: 'Construisez un rÃ©seau superficiel avec `nn.Sequential` : 1 entrÃ©e, D=20 unitÃ©s cachÃ©es, 1 sortie. Comptez les paramÃ¨tres, puis passez un batch de 50 entrÃ©es Ã  travers le rÃ©seau.',
        starterCode: `import torch
import torch.nn as nn

torch.manual_seed(42)

# Construisez le rÃ©seau superficiel
D = 20  # unitÃ©s cachÃ©es
model = nn.Sequential(
    ___,  # couche 1 : entrÃ©e â†’ cachÃ©e
    ___,  # activation ReLU
    ___,  # couche 2 : cachÃ©e â†’ sortie
)

# Comptez les paramÃ¨tres
n_params = ___
print(f"Architecture: {model}")
print(f"ParamÃ¨tres: {n_params}")

# VÃ©rification thÃ©orique
D_i, D_o = 1, 1
n_theorique = D * (D_i + 1) + D_o * (D + 1)
print(f"Formule: {D}Ã—({D_i}+1) + {D_o}Ã—({D}+1) = {n_theorique}")

# Passez un batch de 50 entrÃ©es
x = torch.linspace(-3, 3, 50).unsqueeze(1)  # (50, 1)
y = model(x)  # forward pass

print(f"\\nInput shape:  {x.shape}")
print(f"Output shape: {y.shape}")
print(f"PremiÃ¨res sorties: {y[:5].squeeze().tolist()}")

# Inspectez les poids de la couche cachÃ©e
W1 = model[0].weight  # matrice de poids
b1 = model[0].bias    # vecteur de biais
print(f"\\nPoids couche cachÃ©e: {W1.shape} â†’ {W1.numel()} poids")
print(f"Biais couche cachÃ©e: {b1.shape} â†’ {b1.numel()} biais")`,
        solution: `import torch
import torch.nn as nn

torch.manual_seed(42)

D = 20
model = nn.Sequential(
    nn.Linear(1, D),   # couche 1 : entrÃ©e â†’ cachÃ©e
    nn.ReLU(),          # activation ReLU
    nn.Linear(D, 1),   # couche 2 : cachÃ©e â†’ sortie
)

n_params = sum(p.numel() for p in model.parameters())
print(f"Architecture: {model}")
print(f"ParamÃ¨tres: {n_params}")

D_i, D_o = 1, 1
n_theorique = D * (D_i + 1) + D_o * (D + 1)
print(f"Formule: {D}Ã—({D_i}+1) + {D_o}Ã—({D}+1) = {n_theorique}")

x = torch.linspace(-3, 3, 50).unsqueeze(1)
y = model(x)

print(f"\\nInput shape:  {x.shape}")
print(f"Output shape: {y.shape}")
print(f"PremiÃ¨res sorties: {y[:5].squeeze().tolist()}")

W1 = model[0].weight
b1 = model[0].bias
print(f"\\nPoids couche cachÃ©e: {W1.shape} â†’ {W1.numel()} poids")
print(f"Biais couche cachÃ©e: {b1.shape} â†’ {b1.numel()} biais")`,
        hints: [
          'nn.Linear(1, D) pour la couche d\'entrÃ©e vers cachÃ©e',
          'nn.ReLU() comme activation',
          'nn.Linear(D, 1) pour la couche cachÃ©e vers sortie',
          'sum(p.numel() for p in model.parameters()) pour le total',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 5 â€” THÃ‰ORIQUE : Compter les rÃ©gions
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-th3',
        title: 'ğŸ§  ThÃ©orie â€” Compter les rÃ©gions linÃ©aires (Prob. 3.18)',
        instructions: 'ImplÃ©mentez la formule de Zaslavsky pour calculer le nombre maximum de rÃ©gions linÃ©aires. VÃ©rifiez que D=3 en 2D donne 7 rÃ©gions (comme Fig. 3.8j). Explorez comment le nombre de rÃ©gions explose en haute dimension.',
        starterCode: `import math

def binomial(n, k):
    """Coefficient binomial C(n, k)"""
    if k > n or k < 0:
        return 0
    return math.comb(n, k)

def max_regions(D, D_i):
    """
    Nombre max de rÃ©gions d'un shallow network
    D : nombre d'unitÃ©s cachÃ©es
    D_i : dimension de l'entrÃ©e
    Formule de Zaslavsky (1975)
    """
    total = ___  # Î£ C(D, j) pour j = 0 Ã  min(D, D_i)
    return total

# VÃ©rifications du livre
print("â•â•â• VÃ©rifications â•â•â•")
print(f"D=3, D_i=1 : {max_regions(3, 1)} rÃ©gions (attendu: 4)")
print(f"D=3, D_i=2 : {max_regions(3, 2)} rÃ©gions (attendu: 7)")
print(f"D=5, D_i=2 : {max_regions(5, 2)} rÃ©gions (attendu: 16)")

# Exploration
print("\\nâ•â•â• Explosion combinatoire â•â•â•")
for D_i in [1, 2, 5, 10, 50, 100]:
    D = max(D_i, 10)
    r = max_regions(D, D_i)
    print(f"D_i={D_i:3d}, D={D:3d} â†’ {r:.2e} rÃ©gions max")

# Cas massif du livre
D, D_i = 500, 100
r = max_regions(D, D_i)
print(f"\\nD=500, D_i=100 â†’ ~10^{math.log10(r):.0f} rÃ©gions !")`,
        solution: `import math

def binomial(n, k):
    if k > n or k < 0:
        return 0
    return math.comb(n, k)

def max_regions(D, D_i):
    total = sum(binomial(D, j) for j in range(min(D, D_i) + 1))
    return total

print("â•â•â• VÃ©rifications â•â•â•")
print(f"D=3, D_i=1 : {max_regions(3, 1)} rÃ©gions (attendu: 4)")
print(f"D=3, D_i=2 : {max_regions(3, 2)} rÃ©gions (attendu: 7)")
print(f"D=5, D_i=2 : {max_regions(5, 2)} rÃ©gions (attendu: 16)")

print("\\nâ•â•â• Explosion combinatoire â•â•â•")
for D_i in [1, 2, 5, 10, 50, 100]:
    D = max(D_i, 10)
    r = max_regions(D, D_i)
    print(f"D_i={D_i:3d}, D={D:3d} â†’ {r:.2e} rÃ©gions max")

D, D_i = 500, 100
r = max_regions(D, D_i)
print(f"\\nD=500, D_i=100 â†’ ~10^{math.log10(r):.0f} rÃ©gions !")`,
        hints: [
          'La formule est : Î£ C(D, j) pour j de 0 Ã  min(D, D_i)',
          'sum(binomial(D, j) for j in range(min(D, D_i) + 1))',
          'Pour D_i=1 : C(D,0) + C(D,1) = 1 + D = D+1 âœ“',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 6 â€” PRATIQUE : Entrainer un shallow network
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-pr3',
        title: 'ğŸ’» Pratique â€” EntraÃ®ner un shallow network',
        instructions: 'EntraÃ®nez un rÃ©seau superficiel pour approximer la fonction sin(x) sur [-Ï€, Ï€]. Observez comment le nombre d\'unitÃ©s cachÃ©es D affecte la qualitÃ© de l\'approximation.',
        starterCode: `import torch
import torch.nn as nn
import torch.optim as optim
import math

torch.manual_seed(42)

# DonnÃ©es : y = sin(x) sur [-Ï€, Ï€]
x_train = torch.linspace(-math.pi, math.pi, 200).unsqueeze(1)
y_train = torch.sin(x_train)

def train_shallow(D, n_epochs=2000, lr=0.01):
    """EntraÃ®ne un rÃ©seau superficiel avec D unitÃ©s cachÃ©es"""
    model = nn.Sequential(
        ___,   # entrÃ©e â†’ D unitÃ©s
        ___,   # ReLU
        ___,   # D unitÃ©s â†’ sortie
    )
    
    optimizer = optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()
    
    for epoch in range(n_epochs):
        pred = model(x_train)
        loss = loss_fn(pred, y_train)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    final_loss = loss_fn(model(x_train), y_train).item()
    return model, final_loss

# Comparer diffÃ©rentes capacitÃ©s
print("D (units)  â”‚  Params  â”‚  MSE Loss")
print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
for D in [3, 5, 10, 20, 50]:
    model, loss = train_shallow(D)
    n_params = sum(p.numel() for p in model.parameters())
    print(f"  D={D:3d}     â”‚  {n_params:5d}   â”‚  {loss:.6f}")

print("\\nâ†’ Plus de hidden units = meilleure approximation de sin(x)")
print("  Cela illustre le thÃ©orÃ¨me d'approximation universelle !")`,
        solution: `import torch
import torch.nn as nn
import torch.optim as optim
import math

torch.manual_seed(42)

x_train = torch.linspace(-math.pi, math.pi, 200).unsqueeze(1)
y_train = torch.sin(x_train)

def train_shallow(D, n_epochs=2000, lr=0.01):
    model = nn.Sequential(
        nn.Linear(1, D),
        nn.ReLU(),
        nn.Linear(D, 1),
    )
    
    optimizer = optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()
    
    for epoch in range(n_epochs):
        pred = model(x_train)
        loss = loss_fn(pred, y_train)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    final_loss = loss_fn(model(x_train), y_train).item()
    return model, final_loss

print("D (units)  â”‚  Params  â”‚  MSE Loss")
print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
for D in [3, 5, 10, 20, 50]:
    model, loss = train_shallow(D)
    n_params = sum(p.numel() for p in model.parameters())
    print(f"  D={D:3d}     â”‚  {n_params:5d}   â”‚  {loss:.6f}")

print("\\nâ†’ Plus de hidden units = meilleure approximation de sin(x)")
print("  Cela illustre le thÃ©orÃ¨me d'approximation universelle !")`,
        hints: [
          'nn.Linear(1, D) pour la couche d\'entrÃ©e',
          'nn.ReLU() pour l\'activation',
          'nn.Linear(D, 1) pour la couche de sortie',
          'Adam converge plus vite que SGD pour ce problÃ¨me',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 7 â€” THÃ‰ORIQUE : Pentes des rÃ©gions
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-th4',
        title: 'ğŸ§  ThÃ©orie â€” Pentes des rÃ©gions lin. (Prob. 3.3)',
        instructions: 'ProblÃ¨me 3.3 : calculez les positions des joints et les pentes de chaque rÃ©gion linÃ©aire. Montrez que la somme d\'une pente intermÃ©diaire est la somme des pentes des unitÃ©s actives dans cette rÃ©gion.',
        starterCode: `import torch

# ParamÃ¨tres de la Figure 3.2a du livre
theta = torch.tensor([[-0.2, 0.4],   # Î¸â‚â‚€, Î¸â‚â‚
                       [-0.9, 0.9],   # Î¸â‚‚â‚€, Î¸â‚‚â‚
                       [ 1.1, -0.7]]) # Î¸â‚ƒâ‚€, Î¸â‚ƒâ‚

phi = torch.tensor([-0.23, -1.3, 1.3, 0.66])

# â”€â”€ Positions des joints â”€â”€
# Un joint est lÃ  oÃ¹ Î¸_{d0} + Î¸_{d1}Â·x = 0
# Donc x_joint = -Î¸_{d0} / Î¸_{d1}
joint1 = ___  # = -theta[0,0] / theta[0,1]
joint2 = ___
joint3 = ___

print("â•â•â• Positions des joints â•â•â•")
print(f"Joint 1 (hâ‚ s'active) : x = {joint1:.4f}")
print(f"Joint 2 (hâ‚‚ s'active) : x = {joint2:.4f}")
print(f"Joint 3 (hâ‚ƒ s'active) : x = {joint3:.4f}")

# Triez les joints pour identifier les rÃ©gions
joints = sorted([(joint1.item(), 1), (joint2.item(), 2), (joint3.item(), 3)])
print(f"\\nJoints triÃ©s: {[(f'x={j:.2f}', f'h{i}') for j, i in joints]}")

# â”€â”€ Pentes des rÃ©gions â”€â”€
# La pente d'une rÃ©gion = Î£ (Ï•_d Â· Î¸_{d1}) pour chaque unitÃ© d ACTIVE
print("\\nâ•â•â• Pentes des rÃ©gions â•â•â•")
print(f"Ï•â‚Â·Î¸â‚â‚ = {phi[1]*theta[0,1]:.4f}")
print(f"Ï•â‚‚Â·Î¸â‚‚â‚ = {phi[2]*theta[1,1]:.4f}")
print(f"Ï•â‚ƒÂ·Î¸â‚ƒâ‚ = {phi[3]*theta[2,1]:.4f}")

# Identifiez quelles unitÃ©s sont actives dans chaque rÃ©gion
# et calculez la pente correspondante`,
        solution: `import torch

theta = torch.tensor([[-0.2, 0.4],
                       [-0.9, 0.9],
                       [ 1.1, -0.7]])

phi = torch.tensor([-0.23, -1.3, 1.3, 0.66])

joint1 = -theta[0, 0] / theta[0, 1]
joint2 = -theta[1, 0] / theta[1, 1]
joint3 = -theta[2, 0] / theta[2, 1]

print("â•â•â• Positions des joints â•â•â•")
print(f"Joint 1 (hâ‚ s'active) : x = {joint1:.4f}")
print(f"Joint 2 (hâ‚‚ s'active) : x = {joint2:.4f}")
print(f"Joint 3 (hâ‚ƒ s'active) : x = {joint3:.4f}")

joints = sorted([(joint1.item(), 1), (joint2.item(), 2), (joint3.item(), 3)])
print(f"\\nJoints triÃ©s: {[(f'x={j:.2f}', f'h{i}') for j, i in joints]}")

print("\\nâ•â•â• Pentes des rÃ©gions â•â•â•")
print(f"Ï•â‚Â·Î¸â‚â‚ = {phi[1]*theta[0,1]:.4f}")
print(f"Ï•â‚‚Â·Î¸â‚‚â‚ = {phi[2]*theta[1,1]:.4f}")
print(f"Ï•â‚ƒÂ·Î¸â‚ƒâ‚ = {phi[3]*theta[2,1]:.4f}")`,
        hints: [
          'Le joint du neurone d est Ã  x = -Î¸_{d0} / Î¸_{d1}',
          'La pente dans une rÃ©gion = somme de Ï•_d Â· Î¸_{d1} pour les neurones actifs',
          'joint1 = -theta[0, 0] / theta[0, 1]',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 8 â€” PRATIQUE : Comparer les activations
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-pr4',
        title: 'ğŸ’» Pratique â€” Comparer ReLU, Sigmoid, Tanh, GELU',
        instructions: 'CrÃ©ez 4 rÃ©seaux superficiels identiques (D=10) mais avec des activations diffÃ©rentes : ReLU, Sigmoid, Tanh, GELU. EntraÃ®nez-les sur y=sin(x) et comparez les pertes finales.',
        starterCode: `import torch
import torch.nn as nn
import torch.optim as optim
import math

torch.manual_seed(0)

# DonnÃ©es
x = torch.linspace(-math.pi, math.pi, 200).unsqueeze(1)
y = torch.sin(x)

D = 10  # unitÃ©s cachÃ©es

# Dictionnaire des activations Ã  tester
activations = {
    'ReLU':    ___,
    'Sigmoid': ___,
    'Tanh':    ___,
    'GELU':    ___,
}

results = {}

for name, act_fn in activations.items():
    torch.manual_seed(0)  # mÃªme init pour comparaison juste
    
    model = nn.Sequential(
        nn.Linear(1, D),
        act_fn,
        nn.Linear(D, 1)
    )
    
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    loss_fn = nn.MSELoss()
    
    for epoch in range(1000):
        pred = model(x)
        loss = loss_fn(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    final_loss = loss.item()
    results[name] = final_loss

# Affichage
print("â•â•â• Comparaison des activations (D=10, 1000 epochs) â•â•â•")
print(f"{'Activation':<12} â”‚ {'MSE Loss':>10}")
print(f"{'â”€'*12}â”€â”¼â”€{'â”€'*10}")
for name, loss in sorted(results.items(), key=lambda x: x[1]):
    bar = 'â–ˆ' * int(min(50, 50 * (1 - loss/max(results.values()))))
    print(f"{name:<12} â”‚ {loss:10.6f}  {bar}")`,
        solution: `import torch
import torch.nn as nn
import torch.optim as optim
import math

torch.manual_seed(0)

x = torch.linspace(-math.pi, math.pi, 200).unsqueeze(1)
y = torch.sin(x)

D = 10

activations = {
    'ReLU':    nn.ReLU(),
    'Sigmoid': nn.Sigmoid(),
    'Tanh':    nn.Tanh(),
    'GELU':    nn.GELU(),
}

results = {}

for name, act_fn in activations.items():
    torch.manual_seed(0)
    
    model = nn.Sequential(
        nn.Linear(1, D),
        act_fn,
        nn.Linear(D, 1)
    )
    
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    loss_fn = nn.MSELoss()
    
    for epoch in range(1000):
        pred = model(x)
        loss = loss_fn(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    final_loss = loss.item()
    results[name] = final_loss

print("â•â•â• Comparaison des activations (D=10, 1000 epochs) â•â•â•")
print(f"{'Activation':<12} â”‚ {'MSE Loss':>10}")
print(f"{'â”€'*12}â”€â”¼â”€{'â”€'*10}")
for name, loss in sorted(results.items(), key=lambda x: x[1]):
    bar = 'â–ˆ' * int(min(50, 50 * (1 - loss/max(results.values()))))
    print(f"{name:<12} â”‚ {loss:10.6f}  {bar}")`,
        hints: [
          'nn.ReLU(), nn.Sigmoid(), nn.Tanh(), nn.GELU()',
          'Utilisez torch.manual_seed(0) avant chaque modÃ¨le pour la reproductibilitÃ©',
          'GELU et Tanh tendent Ã  mieux approximer les fonctions lisses que ReLU',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 9 â€” PRATIQUE : nn.Module personnalisÃ©
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-pr5',
        title: 'ğŸ’» Pratique â€” nn.Module personnalisÃ©',
        instructions: 'ImplÃ©mentez un rÃ©seau superficiel en tant que classe `nn.Module` personnalisÃ©e (pas `nn.Sequential`). Ajoutez une mÃ©thode `count_params()` et une mÃ©thode `get_activation_pattern(x)` qui retourne quelles unitÃ©s sont actives.',
        starterCode: `import torch
import torch.nn as nn

class ShallowNetwork(nn.Module):
    def __init__(self, D_i, D, D_o):
        super().__init__()
        self.hidden = ___     # nn.Linear(D_i, D)
        self.output = ___     # nn.Linear(D, D_o)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        """Forward pass : x â†’ ReLU(Wx+b) â†’ sortie"""
        h = ___   # couche cachÃ©e + activation
        y = ___   # couche de sortie
        return y
    
    def count_params(self):
        """Retourne le nombre total de paramÃ¨tres"""
        return ___
    
    def get_activation_pattern(self, x):
        """Retourne un tenseur binaire (1=actif, 0=inactif)"""
        pre_act = self.hidden(x)       # prÃ©-activations
        pattern = (pre_act > 0).int()  # 1 si actif, 0 sinon
        return pattern

# CrÃ©ez et testez le rÃ©seau
model = ShallowNetwork(D_i=2, D=5, D_o=1)

print(f"Architecture: {model}")
print(f"ParamÃ¨tres: {model.count_params()}")

# Test
x = torch.tensor([[1.0, -0.5]])
y = model(x)
pattern = model.get_activation_pattern(x)

print(f"\\nEntrÃ©e:  {x.tolist()}")
print(f"Sortie:  {y.item():.4f}")
print(f"Pattern: {pattern.tolist()[0]} (1=ON, 0=off)")
print(f"UnitÃ©s actives: {pattern.sum().item()}/{model.hidden.out_features}")`,
        solution: `import torch
import torch.nn as nn

class ShallowNetwork(nn.Module):
    def __init__(self, D_i, D, D_o):
        super().__init__()
        self.hidden = nn.Linear(D_i, D)
        self.output = nn.Linear(D, D_o)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        h = self.relu(self.hidden(x))
        y = self.output(h)
        return y
    
    def count_params(self):
        return sum(p.numel() for p in self.parameters())
    
    def get_activation_pattern(self, x):
        pre_act = self.hidden(x)
        pattern = (pre_act > 0).int()
        return pattern

model = ShallowNetwork(D_i=2, D=5, D_o=1)

print(f"Architecture: {model}")
print(f"ParamÃ¨tres: {model.count_params()}")

x = torch.tensor([[1.0, -0.5]])
y = model(x)
pattern = model.get_activation_pattern(x)

print(f"\\nEntrÃ©e:  {x.tolist()}")
print(f"Sortie:  {y.item():.4f}")
print(f"Pattern: {pattern.tolist()[0]} (1=ON, 0=off)")
print(f"UnitÃ©s actives: {pattern.sum().item()}/{model.hidden.out_features}")`,
        hints: [
          'self.hidden = nn.Linear(D_i, D)',
          'h = self.relu(self.hidden(x)) â€” appliquer ReLU aux prÃ©-activations',
          'count_params: sum(p.numel() for p in self.parameters())',
        ],
        completed: false,
      },

      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      // EXERCICE 10 â€” THÃ‰ORIQUE : UnicitÃ© de la solution
      // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {
        id: 'shallow-th5',
        title: 'ğŸ§  ThÃ©orie â€” UnicitÃ© de la solution (Prob. 3.7)',
        instructions: 'ProblÃ¨me 3.7 : la perte des moindres carrÃ©s a-t-elle un minimum unique ? Montrez qu\'il existe une infinitÃ© de combinaisons de paramÃ¨tres qui donnent exactement la mÃªme fonction (mÃªme perte). DÃ©montrez-le en construisant 2 rÃ©seaux avec des paramÃ¨tres diffÃ©rents mais la mÃªme sortie.',
        starterCode: `import torch

def relu(z):
    return torch.clamp(z, min=0)

def shallow_net(x, theta, phi):
    h1 = relu(theta[0,0] + theta[0,1] * x)
    h2 = relu(theta[1,0] + theta[1,1] * x)
    h3 = relu(theta[2,0] + theta[2,1] * x)
    return phi[0] + phi[1]*h1 + phi[2]*h2 + phi[3]*h3

# RÃ©seau A â€” paramÃ¨tres originaux
theta_A = torch.tensor([[-0.2, 0.4], [-0.9, 0.9], [1.1, -0.7]])
phi_A   = torch.tensor([-0.23, -1.3, 1.3, 0.66])

# RÃ©seau B â€” MÃŠMES rÃ©sultats mais paramÃ¨tres DIFFÃ‰RENTS
# Astuce 1 : multiplier Î¸â‚ par Î± et diviser Ï•â‚ par Î± (Prob. 3.6)
alpha = 2.0
theta_B = theta_A.clone()
theta_B[0] = theta_A[0] * alpha  # multiplie Î¸â‚â‚€, Î¸â‚â‚ par Î±
phi_B = phi_A.clone()
phi_B[1] = phi_A[1] / alpha      # divise Ï•â‚ par Î±

# Astuce 2 : permuter les unitÃ©s cachÃ©es (hâ‚ â†” hâ‚‚)
theta_C = torch.tensor([[-0.9, 0.9], [-0.2, 0.4], [1.1, -0.7]])  # permutation
phi_C   = torch.tensor([-0.23, 1.3, -1.3, 0.66])  # Ï•â‚ â†” Ï•â‚‚ Ã©changÃ©s

# VÃ©rification
x_test = torch.linspace(-2, 3, 10)
print("   x    â”‚   Net A   â”‚   Net B   â”‚   Net C   â”‚  B=A?  C=A?")
print("â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
for x in x_test:
    yA = shallow_net(x, theta_A, phi_A).item()
    yB = shallow_net(x, theta_B, phi_B).item()
    yC = shallow_net(x, theta_C, phi_C).item()
    eq_B = "âœ“" if abs(yA - yB) < 1e-5 else "âœ—"
    eq_C = "âœ“" if abs(yA - yC) < 1e-5 else "âœ—"
    print(f"  {x:+.2f}  â”‚  {yA:+.4f}  â”‚  {yB:+.4f}  â”‚  {yC:+.4f}  â”‚  {eq_B}     {eq_C}")

print("\\nğŸ’¡ Conclusion : le minimum de la perte N'EST PAS unique !")
print("   â†’ Il existe des symÃ©tries : scaling (Ã—Î±) et permutation.")
print("   â†’ Le paysage de perte a une infinitÃ© de minima globaux Ã©quivalents.")`,
        solution: `import torch

def relu(z):
    return torch.clamp(z, min=0)

def shallow_net(x, theta, phi):
    h1 = relu(theta[0,0] + theta[0,1] * x)
    h2 = relu(theta[1,0] + theta[1,1] * x)
    h3 = relu(theta[2,0] + theta[2,1] * x)
    return phi[0] + phi[1]*h1 + phi[2]*h2 + phi[3]*h3

theta_A = torch.tensor([[-0.2, 0.4], [-0.9, 0.9], [1.1, -0.7]])
phi_A   = torch.tensor([-0.23, -1.3, 1.3, 0.66])

alpha = 2.0
theta_B = theta_A.clone()
theta_B[0] = theta_A[0] * alpha
phi_B = phi_A.clone()
phi_B[1] = phi_A[1] / alpha

theta_C = torch.tensor([[-0.9, 0.9], [-0.2, 0.4], [1.1, -0.7]])
phi_C   = torch.tensor([-0.23, 1.3, -1.3, 0.66])

x_test = torch.linspace(-2, 3, 10)
print("   x    â”‚   Net A   â”‚   Net B   â”‚   Net C   â”‚  B=A?  C=A?")
print("â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
for x in x_test:
    yA = shallow_net(x, theta_A, phi_A).item()
    yB = shallow_net(x, theta_B, phi_B).item()
    yC = shallow_net(x, theta_C, phi_C).item()
    eq_B = "âœ“" if abs(yA - yB) < 1e-5 else "âœ—"
    eq_C = "âœ“" if abs(yA - yC) < 1e-5 else "âœ—"
    print(f"  {x:+.2f}  â”‚  {yA:+.4f}  â”‚  {yB:+.4f}  â”‚  {yC:+.4f}  â”‚  {eq_B}     {eq_C}")

print("\\nğŸ’¡ Conclusion : le minimum de la perte N'EST PAS unique !")
print("   â†’ Il existe des symÃ©tries : scaling (Ã—Î±) et permutation.")
print("   â†’ Le paysage de perte a une infinitÃ© de minima globaux Ã©quivalents.")`,
        hints: [
          'Par homogÃ©nÃ©itÃ© du ReLU : ReLU(Î±Â·z) = Î±Â·ReLU(z) pour Î± > 0',
          'Donc (Î±Â·Î¸) passÃ© par ReLU puis (Ï•/Î±) = mÃªme rÃ©sultat',
          'Permuter les unitÃ©s cachÃ©es revient Ã  rÃ©arranger les indices',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RÃ©seaux Superficiels â€” Ch. 3 Understanding Deep Learning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ 1. ImplÃ©mentation manuelle (Ã‰q. 3.1) â”€â”€
def shallow_network_manual(x, theta, phi):
    """
    RÃ©seau superficiel : y = Ï•â‚€ + Î£ Ï•d Â· ReLU(Î¸dâ‚€ + Î¸dâ‚Â·x)
    
    x:     entrÃ©e scalaire (tensor)
    theta: paramÃ¨tres couche cachÃ©e â€” shape (D, 2)
           theta[d, 0] = biais,  theta[d, 1] = pente
    phi:   paramÃ¨tres de sortie â€” shape (D+1,)
           phi[0] = offset, phi[1:] = poids de recombinaison
    """
    D = theta.shape[0]
    
    # PrÃ©-activations : z_d = Î¸_{d0} + Î¸_{d1}Â·x
    z = theta[:, 0] + theta[:, 1] * x
    
    # Activations : h_d = ReLU(z_d)
    h = torch.relu(z)
    
    # Sortie : y = Ï•â‚€ + Î£ Ï•_d Â· h_d
    y = phi[0] + torch.sum(phi[1:] * h)
    
    return y, z, h

# ParamÃ¨tres du livre (Figure 3.2a)
theta = torch.tensor([[-0.2, 0.4],
                       [-0.9, 0.9],
                       [ 1.1, -0.7]])
phi = torch.tensor([-0.23, -1.3, 1.3, 0.66])

print("â”€â”€ Forward pass manuel â”€â”€")
for x_val in [-1.5, 0.0, 0.5, 1.0, 2.0]:
    x = torch.tensor(x_val)
    y, z, h = shallow_network_manual(x, theta, phi)
    active = ["ON" if hi > 0 else "off" for hi in h]
    print(f"x={x_val:+.1f} â†’ y={y:.4f}  pattern=[{', '.join(active)}]")

# â”€â”€ 2. MÃªme rÃ©seau avec PyTorch nn.Module â”€â”€
print("\\nâ”€â”€ RÃ©seau PyTorch nn.Sequential â”€â”€")

# Initialiser avec les MÃŠMES paramÃ¨tres
model = nn.Sequential(
    nn.Linear(1, 3),  # couche cachÃ©e
    nn.ReLU(),
    nn.Linear(3, 1),  # couche de sortie
)

# Copier nos paramÃ¨tres dans le modÃ¨le PyTorch
with torch.no_grad():
    model[0].weight.copy_(theta[:, 1:])  # pentes
    model[0].bias.copy_(theta[:, 0])     # biais
    model[2].weight.copy_(phi[1:].unsqueeze(0))
    model[2].bias.copy_(phi[:1])

for x_val in [-1.5, 0.0, 0.5, 1.0, 2.0]:
    x = torch.tensor([[x_val]])
    y = model(x)
    print(f"x={x_val:+.1f} â†’ y={y.item():.4f}")

# â”€â”€ 3. Comptage de paramÃ¨tres â”€â”€
print(f"\\nâ”€â”€ ParamÃ¨tres â”€â”€")
for name, param in model.named_parameters():
    print(f"  {name}: {param.shape} = {param.numel()} params")
print(f"  Total: {sum(p.numel() for p in model.parameters())}")
print(f"  Formule: D(D_i+1) + D_o(D+1) = 3(1+1) + 1(3+1) = 10 âœ“")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 4 â€” RÃ‰SEAUX PROFONDS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'deep-networks',
    title: 'RÃ©seaux de Neurones Profonds',
    shortTitle: 'Deep Nets',
    description: 'Composition de rÃ©seaux, profondeur vs largeur, et notation matricielle.',
    status: 'locked',
    progress: 0,
    dependencies: ['shallow-networks'],
    category: 'fundamentals',
    theory: [
      {
        type: 'text',
        content: `Un **rÃ©seau profond** est obtenu en **composant** plusieurs rÃ©seaux superficiels : la sortie du premier devient l'entrÃ©e du second, et ainsi de suite. Cette composition crÃ©e des fonctions beaucoup plus complexes.\n\nAvec ReLU, un rÃ©seau profond de K couches de D unitÃ©s cachÃ©es chacune peut crÃ©er jusqu'Ã  **(D+1)^K** rÃ©gions linÃ©aires, contre D+1 pour un rÃ©seau superficiel.`,
      },
      {
        type: 'equation',
        content: '\\mathbf{h}_k = a[\\boldsymbol{\\beta}_k + \\boldsymbol{\\Omega}_k \\mathbf{h}_{k-1}]',
        label: 'Couche k du rÃ©seau profond',
        highlightVar: 'hidden',
      },
      {
        type: 'text',
        content: `En notation matricielle, chaque couche applique une transformation affine (multiplication par la matrice de poids **Î©k** + biais **Î²k**) suivie d'une activation. Le rÃ©seau complet est :\n\n- Forward pass : on calcule sÃ©quentiellement hâ‚, hâ‚‚, ..., hK\n- La sortie finale fâ‚ƒ est le rÃ©sultat de la derniÃ¨re couche`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} \\mathbf{f}_0 &= \\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x} \\\\ \\mathbf{h}_k &= a[\\mathbf{f}_{k-1}] \\\\ \\mathbf{f}_k &= \\boldsymbol{\\beta}_k + \\boldsymbol{\\Omega}_k \\mathbf{h}_k \\end{aligned}',
        label: 'Forward pass complet',
        highlightVar: 'hidden',
      },
      {
        type: 'callout',
        content: 'âš¡ **Profondeur vs Largeur** : un rÃ©seau profond avec le mÃªme nombre total de paramÃ¨tres qu\'un rÃ©seau superficiel large peut reprÃ©senter des fonctions exponentiellement plus complexes. C\'est pourquoi le "deep" learning est si puissant.',
      },
    ],
    exercises: [
      {
        id: 'deep-ex1',
        title: 'Construire un rÃ©seau Ã  3 couches',
        instructions: 'CrÃ©ez un rÃ©seau nn.Module avec 3 couches cachÃ©es (784â†’256â†’128â†’64â†’10). Comptez les paramÃ¨tres.',
        starterCode: `import torch
import torch.nn as nn

class DeepNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = ___
        self.layer2 = ___
        self.layer3 = ___
        self.output = ___
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.relu(self.layer3(x))
        return self.output(x)

model = DeepNet()
print(model)

total = sum(p.numel() for p in model.parameters())
print(f"\\nTotal paramÃ¨tres: {total:,}")`,
        solution: `import torch
import torch.nn as nn

class DeepNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(784, 256)
        self.layer2 = nn.Linear(256, 128)
        self.layer3 = nn.Linear(128, 64)
        self.output = nn.Linear(64, 10)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.relu(self.layer3(x))
        return self.output(x)

model = DeepNet()
print(model)

total = sum(p.numel() for p in model.parameters())
print(f"\\nTotal paramÃ¨tres: {total:,}")`,
        hints: [
          'nn.Linear(in_features, out_features) crÃ©e une couche dense',
          'Couche 1: 784â†’256, Couche 2: 256â†’128, Couche 3: 128â†’64, Sortie: 64â†’10',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• RÃ©seaux Profonds vs Superficiels â•â•

# RÃ©seau SUPERFICIEL (1 couche cachÃ©e large)
shallow = nn.Sequential(
    nn.Linear(1, 100),
    nn.ReLU(),
    nn.Linear(100, 1)
)

# RÃ©seau PROFOND (3 couches cachÃ©es Ã©troites)
deep = nn.Sequential(
    nn.Linear(1, 20),
    nn.ReLU(),
    nn.Linear(20, 20),
    nn.ReLU(),
    nn.Linear(20, 20),
    nn.ReLU(),
    nn.Linear(20, 1)
)

shallow_params = sum(p.numel() for p in shallow.parameters())
deep_params = sum(p.numel() for p in deep.parameters())

print(f"Shallow: {shallow_params} paramÃ¨tres")
print(f"Deep:    {deep_params} paramÃ¨tres")

# Les deux ont ~300 paramÃ¨tres mais le deep peut
# reprÃ©senter des fonctions beaucoup plus complexes !

# Forward pass
x = torch.randn(5, 1)
print(f"\\nShallow output: {shallow(x).squeeze().tolist()}")
print(f"Deep output:    {deep(x).squeeze().tolist()}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 5 â€” FONCTIONS DE PERTE
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'loss-functions',
    title: 'Fonctions de Perte (Loss Functions)',
    shortTitle: 'Loss',
    description: 'Maximum de vraisemblance, MSE, Cross-Entropy â€” mesurer l\'erreur du modÃ¨le.',
    status: 'locked',
    progress: 0,
    dependencies: ['deep-networks'],
    category: 'training',
    theory: [
      {
        type: 'text',
        content: `La **fonction de perte** mesure Ã  quel point les prÃ©dictions sont Ã©loignÃ©es de la rÃ©alitÃ©. En Deep Learning, on construit les fonctions de perte via le **maximum de vraisemblance** : le modÃ¨le prÃ©dit une distribution de probabilitÃ© Pr(y|x), et on cherche les paramÃ¨tres qui maximisent la probabilitÃ© des donnÃ©es observÃ©es.`,
      },
      {
        type: 'equation',
        content: '\\hat{\\boldsymbol{\\phi}} = \\underset{\\boldsymbol{\\phi}}{\\text{argmax}} \\prod_{i=1}^{I} Pr(y_i | x_i)',
        label: 'Maximum de vraisemblance',
      },
      {
        type: 'text',
        content: `En prenant le logarithme nÃ©gatif (pour transformer le produit en somme et la maximisation en minimisation), on obtient la **perte de log-vraisemblance nÃ©gative**. Pour la rÃ©gression avec bruit gaussien, cela donne le **MSE** (Mean Squared Error) :`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}_{MSE} = \\frac{1}{I}\\sum_{i=1}^{I}(y_i - f[x_i, \\boldsymbol{\\phi}])^2',
        label: 'Mean Squared Error (RÃ©gression)',
        highlightVar: 'loss',
      },
      {
        type: 'text',
        content: `Pour la **classification binaire**, le modÃ¨le prÃ©dit une probabilitÃ© via sigmoid, et la perte est la **cross-entropie binaire**. Pour la classification **multi-classe** avec K classes, on utilise softmax + cross-entropie :`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}_{CE} = -\\sum_{i=1}^{I} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{y}_{ik})',
        label: 'Cross-Entropy Loss (Classification)',
        highlightVar: 'loss',
      },
      {
        type: 'callout',
        content: 'âš¡ La **cross-entropie** mesure la "distance" entre la distribution prÃ©dite et la distribution rÃ©elle. Elle est toujours â‰¥ 0 et vaut 0 seulement quand la prÃ©diction est parfaite.',
      },
    ],
    exercises: [
      {
        id: 'loss-ex1',
        title: 'Comparer MSE et Cross-Entropy',
        instructions: 'Calculez la perte MSE pour un problÃ¨me de rÃ©gression et la perte Cross-Entropy pour un problÃ¨me de classification.',
        starterCode: `import torch
import torch.nn as nn

# â”€â”€ RÃ©gression : MSE â”€â”€
predictions = torch.tensor([2.5, 3.2, 4.1])
targets = torch.tensor([3.0, 3.0, 4.0])

mse = nn.MSELoss()
loss_mse = ___

# â”€â”€ Classification : Cross-Entropy â”€â”€
logits = torch.tensor([[2.0, 1.0, 0.1],
                        [0.5, 2.5, 0.3]])
labels = torch.tensor([0, 1])

ce = nn.CrossEntropyLoss()
loss_ce = ___

print(f"MSE Loss: {loss_mse.item():.4f}")
print(f"CE Loss:  {loss_ce.item():.4f}")`,
        solution: `import torch
import torch.nn as nn

# â”€â”€ RÃ©gression : MSE â”€â”€
predictions = torch.tensor([2.5, 3.2, 4.1])
targets = torch.tensor([3.0, 3.0, 4.0])

mse = nn.MSELoss()
loss_mse = mse(predictions, targets)

# â”€â”€ Classification : Cross-Entropy â”€â”€
logits = torch.tensor([[2.0, 1.0, 0.1],
                        [0.5, 2.5, 0.3]])
labels = torch.tensor([0, 1])

ce = nn.CrossEntropyLoss()
loss_ce = ce(logits, labels)

print(f"MSE Loss: {loss_mse.item():.4f}")
print(f"CE Loss:  {loss_ce.item():.4f}")`,
        hints: [
          'loss_mse = mse(predictions, targets)',
          'CrossEntropyLoss prend des logits (avant softmax) et des labels entiers',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• Fonctions de Perte â€” Depuis le Maximum de Vraisemblance â•â•

# â”€â”€ 1. MSE pour la rÃ©gression â”€â”€
pred = torch.tensor([2.5, 3.2, 4.1, 1.8])
target = torch.tensor([3.0, 3.0, 4.0, 2.0])

mse = nn.MSELoss()
print(f"MSE Loss: {mse(pred, target).item():.4f}")

# Calcul manuel
manual_mse = torch.mean((pred - target) ** 2)
print(f"MSE manuelle: {manual_mse.item():.4f}")

# â”€â”€ 2. Binary Cross-Entropy â”€â”€
# Pour classification binaire (sortie sigmoid)
pred_prob = torch.tensor([0.9, 0.2, 0.8])
target_bin = torch.tensor([1.0, 0.0, 1.0])

bce = nn.BCELoss()
print(f"\\nBCE Loss: {bce(pred_prob, target_bin).item():.4f}")

# â”€â”€ 3. Cross-Entropy pour multi-classe â”€â”€
logits = torch.tensor([[2.0, 1.0, 0.1],
                        [0.5, 2.5, 0.3],
                        [0.1, 0.3, 3.0]])
labels = torch.tensor([0, 1, 2])  # classes correctes

ce = nn.CrossEntropyLoss()
print(f"CE Loss: {ce(logits, labels).item():.4f}")

# VÃ©rifier les probabilitÃ©s avec softmax
probs = torch.softmax(logits, dim=1)
print(f"\\nProbabilitÃ©s prÃ©dites:\\n{probs}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 6 â€” DESCENTE DE GRADIENT
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'gradient-descent',
    title: 'Descente de Gradient & Optimisation',
    shortTitle: 'Gradient',
    description: 'Gradient descent, SGD, Momentum et Adam â€” comment entraÃ®ner un rÃ©seau.',
    status: 'locked',
    progress: 0,
    dependencies: ['loss-functions'],
    category: 'training',
    theory: [
      {
        type: 'text',
        content: `La **descente de gradient** est l'algorithme itÃ©ratif standard pour entraÃ®ner les rÃ©seaux de neurones. On part de paramÃ¨tres initiaux alÃ©atoires, puis on rÃ©pÃ¨te deux Ã©tapes :\n\n1. **Calculer le gradient** âˆ‚L/âˆ‚Ï• de la perte par rapport aux paramÃ¨tres\n2. **Mettre Ã  jour** les paramÃ¨tres dans la direction opposÃ©e au gradient`,
      },
      {
        type: 'equation',
        content: '\\boldsymbol{\\phi} \\leftarrow \\boldsymbol{\\phi} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\phi}}',
        label: 'RÃ¨gle de mise Ã  jour (Gradient Descent)',
        highlightVar: 'grad',
      },
      {
        type: 'text',
        content: `Le **SGD** (Stochastic Gradient Descent) utilise un **mini-batch** alÃ©atoire au lieu de tout le dataset, ce qui est bien plus rapide. Le **Momentum** ajoute une "inertie" qui lisse les mises Ã  jour. **Adam** combine momentum + adaptation du learning rate par paramÃ¨tre.`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} \\mathbf{m}_t &= \\beta_1 \\mathbf{m}_{t-1} + (1-\\beta_1) \\mathbf{g}_t \\\\ \\mathbf{v}_t &= \\beta_2 \\mathbf{v}_{t-1} + (1-\\beta_2) \\mathbf{g}_t^2 \\\\ \\boldsymbol{\\phi}_t &= \\boldsymbol{\\phi}_{t-1} - \\alpha \\frac{\\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon} \\end{aligned}',
        label: 'Algorithme Adam',
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ Le **learning rate Î±** est l\'hyperparamÃ¨tre le plus important. Trop grand â†’ la perte diverge. Trop petit â†’ l\'entraÃ®nement est trop lent. Adam (Î± â‰ˆ 0.001) est le choix par dÃ©faut en pratique.',
      },
    ],
    exercises: [
      {
        id: 'gd-ex1',
        title: 'SGD vs Adam',
        instructions: 'EntraÃ®nez le mÃªme modÃ¨le avec SGD et Adam. Comparez la vitesse de convergence.',
        starterCode: `import torch
import torch.nn as nn
import torch.optim as optim

torch.manual_seed(42)

# DonnÃ©es : y = 3x + 2
x = torch.randn(100, 1)
y = 3 * x + 2 + torch.randn(100, 1) * 0.1

# ModÃ¨le avec SGD
model_sgd = nn.Linear(1, 1)
opt_sgd = ___

# ModÃ¨le avec Adam
model_adam = nn.Linear(1, 1)
opt_adam = ___

loss_fn = nn.MSELoss()

for epoch in range(200):
    # SGD step
    loss_sgd = loss_fn(model_sgd(x), y)
    opt_sgd.zero_grad()
    loss_sgd.backward()
    opt_sgd.step()
    
    # Adam step
    loss_adam = loss_fn(model_adam(x), y)
    opt_adam.zero_grad()
    loss_adam.backward()
    opt_adam.step()
    
    if (epoch+1) % 50 == 0:
        print(f"Epoch {epoch+1}: SGD={loss_sgd.item():.4f}, Adam={loss_adam.item():.4f}")`,
        solution: `import torch
import torch.nn as nn
import torch.optim as optim

torch.manual_seed(42)

x = torch.randn(100, 1)
y = 3 * x + 2 + torch.randn(100, 1) * 0.1

model_sgd = nn.Linear(1, 1)
opt_sgd = optim.SGD(model_sgd.parameters(), lr=0.01)

model_adam = nn.Linear(1, 1)
opt_adam = optim.Adam(model_adam.parameters(), lr=0.01)

loss_fn = nn.MSELoss()

for epoch in range(200):
    loss_sgd = loss_fn(model_sgd(x), y)
    opt_sgd.zero_grad()
    loss_sgd.backward()
    opt_sgd.step()
    
    loss_adam = loss_fn(model_adam(x), y)
    opt_adam.zero_grad()
    loss_adam.backward()
    opt_adam.step()
    
    if (epoch+1) % 50 == 0:
        print(f"Epoch {epoch+1}: SGD={loss_sgd.item():.4f}, Adam={loss_adam.item():.4f}")`,
        hints: [
          'optim.SGD(model.parameters(), lr=0.01)',
          'optim.Adam(model.parameters(), lr=0.01)',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn
import torch.optim as optim

# â•â• Descente de Gradient â€” SGD, Momentum, Adam â•â•

torch.manual_seed(42)

# DonnÃ©es synthÃ©tiques : y = 3x + 2 + bruit
x = torch.randn(200, 1)
y = 3 * x + 2 + torch.randn(200, 1) * 0.1

model = nn.Linear(1, 1)
optimizer = optim.Adam(model.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

# Boucle d'entraÃ®nement
for epoch in range(200):
    # Mini-batch (ici on utilise tout le dataset)
    pred = model(x)
    loss = loss_fn(pred, y)
    
    # Gradient â†’ mise Ã  jour
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 40 == 0:
        w, b = model.weight.item(), model.bias.item()
        print(f"Epoch {epoch+1:3d}: loss={loss.item():.4f}, y={w:.3f}x + {b:.3f}")

w, b = model.weight.item(), model.bias.item()
print(f"\\nâœ“ Appris : y = {w:.2f}x + {b:.2f}")
print(f"  RÃ©el  : y = 3.00x + 2.00")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 7 â€” BACKPROPAGATION
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'backprop',
    title: 'Backpropagation & Autograd',
    shortTitle: 'Backprop',
    description: 'Propagation arriÃ¨re du gradient â€” le cÅ“ur de l\'apprentissage.',
    status: 'locked',
    progress: 0,
    dependencies: ['gradient-descent'],
    category: 'training',
    theory: [
      {
        type: 'text',
        content: `La **backpropagation** est l'algorithme qui calcule efficacement les gradients dans un rÃ©seau profond. Elle utilise la **rÃ¨gle de la chaÃ®ne** pour propager le gradient de la perte depuis la sortie jusqu'aux entrÃ©es, couche par couche.\n\nLe processus se dÃ©roule en deux passes :\n1. **Forward pass** : calcul de toutes les activations hâ‚, hâ‚‚, ... et de la perte\n2. **Backward pass** : propagation des gradients de la sortie vers l'entrÃ©e`,
      },
      {
        type: 'equation',
        content: '\\frac{\\partial \\ell}{\\partial \\mathbf{f}_k} = \\frac{\\partial \\mathbf{h}_{k+1}}{\\partial \\mathbf{f}_k} \\cdot \\frac{\\partial \\mathbf{f}_{k+1}}{\\partial \\mathbf{h}_{k+1}} \\cdot \\frac{\\partial \\ell}{\\partial \\mathbf{f}_{k+1}}',
        label: 'RÃ¨gle de la chaÃ®ne (Backprop)',
        highlightVar: 'grad',
      },
      {
        type: 'text',
        content: `Pour chaque couche k, on calcule les gradients par rapport aux poids **Î©k** et biais **Î²k** :\n\n- âˆ‚â„“/âˆ‚Î©k = âˆ‚â„“/âˆ‚fk Â· hkáµ€\n- âˆ‚â„“/âˆ‚Î²k = âˆ‚â„“/âˆ‚fk\n\nLa dÃ©rivÃ©e de ReLU est simple : elle vaut 1 si l'entrÃ©e > 0, et 0 sinon.`,
      },
      {
        type: 'equation',
        content: '\\frac{\\partial \\, \\text{ReLU}(z)}{\\partial z} = \\begin{cases} 0 & z < 0 \\\\ 1 & z > 0 \\end{cases}',
        label: 'DÃ©rivÃ©e du ReLU',
      },
      {
        type: 'callout',
        content: 'âš¡ PyTorch implÃ©mente la backpropagation automatiquement via **Autograd**. Il suffit d\'appeler `loss.backward()` et les gradients sont calculÃ©s pour tous les paramÃ¨tres avec `requires_grad=True`.',
      },
      {
        type: 'text',
        content: `L'**initialisation des paramÃ¨tres** est cruciale. Si les poids sont trop grands, les gradients explosent. Trop petits, ils s'Ã©vanouissent. L'initialisation de **He** (pour ReLU) choisit les poids avec une variance de 2/n, oÃ¹ n est le nombre d'entrÃ©es.`,
      },
    ],
    exercises: [
      {
        id: 'bp-ex1',
        title: 'Autograd en action',
        instructions: 'Utilisez PyTorch Autograd pour calculer les gradients d\'une expression. VÃ©rifiez les rÃ©sultats manuellement.',
        starterCode: `import torch

# Variables avec suivi du gradient
x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(3.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

# Forward : y = w*x + b, loss = (y - 10)Â²
y = ___
loss = ___

print(f"y = {y.item():.2f}")
print(f"loss = {loss.item():.2f}")

# Backward
loss.backward()

print(f"\\nâˆ‚loss/âˆ‚w = {w.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚x = {x.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚b = {b.grad.item():.2f}")

# VÃ©rification manuelle :
# âˆ‚loss/âˆ‚y = 2(y-10), âˆ‚y/âˆ‚w = x â†’ âˆ‚loss/âˆ‚w = 2(y-10)*x
print(f"\\nVÃ©rification: 2*(y-10)*x = {2*(y.item()-10)*x.item():.2f}")`,
        solution: `import torch

x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(3.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

y = w * x + b
loss = (y - 10) ** 2

print(f"y = {y.item():.2f}")
print(f"loss = {loss.item():.2f}")

loss.backward()

print(f"\\nâˆ‚loss/âˆ‚w = {w.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚x = {x.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚b = {b.grad.item():.2f}")

print(f"\\nVÃ©rification: 2*(y-10)*x = {2*(y.item()-10)*x.item():.2f}")`,
        hints: [
          'y = w * x + b',
          'loss = (y - 10) ** 2',
          'loss.backward() calcule tous les gradients automatiquement',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch

# â•â• Backpropagation â€” Autograd â•â•

# â”€â”€ 1. Calcul simple â”€â”€
x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(3.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

y = w * x + b        # Forward
loss = (y - 10) ** 2  # Perte

print(f"y = wÂ·x + b = {y.item():.2f}")
print(f"loss = (y - 10)Â² = {loss.item():.2f}")

loss.backward()       # Backward

print(f"\\nâˆ‚loss/âˆ‚w = {w.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚x = {x.grad.item():.2f}")
print(f"âˆ‚loss/âˆ‚b = {b.grad.item():.2f}")

# â”€â”€ 2. Graphe de calcul plus complexe â”€â”€
a = torch.tensor(1.5, requires_grad=True)
b = torch.tensor(2.0, requires_grad=True)

c = a * b           # c = 3.0
d = torch.relu(c - 2.5)  # d = ReLU(0.5) = 0.5
e = d ** 2           # e = 0.25

e.backward()
print(f"\\nâ”€â”€ Graphe complexe â”€â”€")
print(f"âˆ‚e/âˆ‚a = {a.grad.item():.4f}")
print(f"âˆ‚e/âˆ‚b = {b.grad.item():.4f}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 8 â€” RÃ‰GULARISATION
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'regularization',
    title: 'RÃ©gularisation & GÃ©nÃ©ralisation',
    shortTitle: 'RÃ©gular.',
    description: 'L2, Dropout, Data Augmentation, Early Stopping â€” Ã©viter le surapprentissage.',
    status: 'locked',
    progress: 0,
    dependencies: ['backprop'],
    category: 'training',
    theory: [
      {
        type: 'text',
        content: `Le **surapprentissage** (overfitting) se produit quand le modÃ¨le mÃ©morise les donnÃ©es d'entraÃ®nement au lieu d'apprendre des patterns gÃ©nÃ©raux. La **rÃ©gularisation** ajoute des contraintes pour favoriser la gÃ©nÃ©ralisation.\n\nSources d'erreur :\n- **Biais** : le modÃ¨le est trop simple (underfitting)\n- **Variance** : le modÃ¨le est trop sensible aux donnÃ©es (overfitting)\n- **Bruit** : erreur irrÃ©ductible dans les donnÃ©es`,
      },
      {
        type: 'equation',
        content: '\\mathcal{L}_{reg} = \\mathcal{L}_{data} + \\lambda \\| \\boldsymbol{\\phi} \\|_2^2',
        label: 'RÃ©gularisation L2 (Weight Decay)',
        highlightVar: 'loss',
      },
      {
        type: 'text',
        content: `Le **Dropout** dÃ©sactive alÃ©atoirement une fraction p des neurones pendant l'entraÃ®nement, forÃ§ant le rÃ©seau Ã  ne pas dÃ©pendre d'un seul neurone. Ã€ l'infÃ©rence, tous les neurones sont actifs mais leurs sorties sont multipliÃ©es par (1-p).\n\n**Early Stopping** : on surveille la perte de validation et on arrÃªte l'entraÃ®nement quand elle commence Ã  augmenter.\n\n**Data Augmentation** : on augmente artificiellement le dataset (rotations, flips, crops pour les images).`,
      },
      {
        type: 'equation',
        content: '\\tilde{h}_k = h_k \\cdot m_k \\quad \\text{oÃ¹ } m_k \\sim \\text{Bernoulli}(1-p)',
        label: 'Dropout (pendant l\'entraÃ®nement)',
      },
      {
        type: 'callout',
        content: 'ğŸ§  La **Batch Normalization** normalise les activations de chaque couche pour avoir une moyenne de 0 et une variance de 1. Elle agit Ã  la fois comme rÃ©gularisation et accÃ©lÃ©rateur d\'entraÃ®nement.',
      },
    ],
    exercises: [],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• RÃ©gularisation â•â•

# â”€â”€ 1. Weight Decay (L2) â”€â”€
model = nn.Sequential(
    nn.Linear(10, 50), nn.ReLU(),
    nn.Linear(50, 1)
)

# Adam avec weight_decay = L2 regularization
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)
print("âœ“ Weight decay activÃ© (Î»=0.01)")

# â”€â”€ 2. Dropout â”€â”€
model_with_dropout = nn.Sequential(
    nn.Linear(10, 50),
    nn.ReLU(),
    nn.Dropout(p=0.3),    # 30% des neurones dÃ©sactivÃ©s
    nn.Linear(50, 50),
    nn.ReLU(),
    nn.Dropout(p=0.3),
    nn.Linear(50, 1)
)

# En mode training vs eval
x = torch.randn(1, 10)

model_with_dropout.train()
out1 = model_with_dropout(x)
out2 = model_with_dropout(x)
print(f"\\nTrain mode (dropout actif):")
print(f"  Sortie 1: {out1.item():.4f}")
print(f"  Sortie 2: {out2.item():.4f} (diffÃ©rent!)")

model_with_dropout.eval()
out3 = model_with_dropout(x)
out4 = model_with_dropout(x)
print(f"\\nEval mode (dropout dÃ©sactivÃ©):")
print(f"  Sortie 1: {out3.item():.4f}")
print(f"  Sortie 2: {out4.item():.4f} (identique!)")

# â”€â”€ 3. Batch Normalization â”€â”€
bn_model = nn.Sequential(
    nn.Linear(10, 50),
    nn.BatchNorm1d(50),  # Normalise les activations
    nn.ReLU(),
    nn.Linear(50, 1)
)
print(f"\\nâœ“ BatchNorm model: {sum(p.numel() for p in bn_model.parameters())} params")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 9 â€” CNN
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'cnn',
    title: 'RÃ©seaux Convolutifs (CNN)',
    shortTitle: 'CNN',
    description: 'Convolutions 2D, invariance, Ã©quivariance, pooling â€” la vision par ordinateur.',
    status: 'locked',
    progress: 0,
    dependencies: ['regularization'],
    category: 'architectures',
    theory: [
      {
        type: 'text',
        content: `Les **CNN** exploitent une propriÃ©tÃ© clÃ© des images : les patterns locaux (bords, textures) sont les mÃªmes quel que soit leur position. Deux concepts formalisent cette idÃ©e :\n\n- **Invariance** : f[t[x]] = f[x] â€” la sortie ne change pas sous une transformation (ex: classification)\n- **Ã‰quivariance** : f[t[x]] = t[f[x]] â€” la sortie se transforme de la mÃªme faÃ§on (ex: segmentation)`,
      },
      {
        type: 'equation',
        content: 'z_i = \\sum_{m} \\omega_m \\cdot x_{i+m}',
        label: 'Convolution 1D (kernel de taille M)',
      },
      {
        type: 'text',
        content: `La **convolution 2D** applique un filtre (kernel) qui glisse sur l'image. Ce filtre dÃ©tecte des patterns locaux (bords, coins, textures). Les mÃªmes poids sont partagÃ©s partout (equivariance Ã  la translation).\n\n**Pooling** (Max/Average) rÃ©duit la rÃ©solution spatiale et rend le rÃ©seau partiellement invariant Ã  de petites translations.`,
      },
      {
        type: 'equation',
        content: 'z_{ij} = \\sum_{m} \\sum_{n} \\omega_{mn} \\cdot x_{i+m, \\, j+n}',
        label: 'Convolution 2D',
      },
      {
        type: 'callout',
        content: 'âš¡ Un CNN typique empile : Conv â†’ ReLU â†’ Pool â†’ Conv â†’ ReLU â†’ Pool â†’ Flatten â†’ FC. Les premiÃ¨res couches dÃ©tectent des features bas-niveau (bords), les derniÃ¨res des features haut-niveau (visages, objets).',
      },
    ],
    exercises: [
      {
        id: 'cnn-ex1',
        title: 'Construire un CNN pour MNIST',
        instructions: 'CrÃ©ez un CNN simple avec 2 couches convolutives pour classifier des images 28Ã—28.',
        starterCode: `import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = ___  # 1 canal â†’ 16 filtres, kernel 3
        self.conv2 = ___  # 16 â†’ 32 filtres, kernel 3
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = ___     # 32*7*7 â†’ 10 classes
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)  # flatten
        return self.fc(x)

model = SimpleCNN()
x = torch.randn(1, 1, 28, 28)
out = model(x)
print(f"Input: {x.shape}")
print(f"Output: {out.shape}")`,
        solution: `import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = nn.Linear(32 * 7 * 7, 10)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        return self.fc(x)

model = SimpleCNN()
x = torch.randn(1, 1, 28, 28)
out = model(x)
print(f"Input: {x.shape}")
print(f"Output: {out.shape}")`,
        hints: [
          'nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)',
          'AprÃ¨s 2 MaxPool(2,2) sur 28Ã—28 â†’ 7Ã—7',
          'nn.Linear(32 * 7 * 7, 10)',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• RÃ©seaux Convolutifs (CNN) â•â•

# â”€â”€ 1. Convolution 2D simple â”€â”€
conv = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)
x = torch.randn(1, 1, 28, 28)  # batch=1, channels=1, H=28, W=28
out = conv(x)
print(f"Conv2d: {x.shape} â†’ {out.shape}")
print(f"ParamÃ¨tres conv: {conv.weight.shape} = {conv.weight.numel()} poids + {conv.bias.numel()} biais")

# â”€â”€ 2. CNN complet â”€â”€
class MNISTNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1),   # 28Ã—28 â†’ 28Ã—28
            nn.ReLU(),
            nn.MaxPool2d(2),                    # â†’ 14Ã—14
            nn.Conv2d(16, 32, 3, padding=1),   # â†’ 14Ã—14
            nn.ReLU(),
            nn.MaxPool2d(2),                    # â†’ 7Ã—7
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(32 * 7 * 7, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        return self.classifier(self.features(x))

model = MNISTNet()
out = model(torch.randn(4, 1, 28, 28))
print(f"\\nMNISTNet: batch=4 â†’ {out.shape}")
print(f"ParamÃ¨tres: {sum(p.numel() for p in model.parameters()):,}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 10 â€” RESIDUAL NETWORKS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'resnet',
    title: 'RÃ©seaux RÃ©siduels (ResNet)',
    shortTitle: 'ResNet',
    description: 'Connexions rÃ©siduelles, skip connections et Batch Normalization.',
    status: 'locked',
    progress: 0,
    dependencies: ['cnn'],
    category: 'architectures',
    theory: [
      {
        type: 'text',
        content: `Les rÃ©seaux trÃ¨s profonds (>20 couches) souffrent du problÃ¨me des **gradients Ã©vanescents** : le gradient devient exponentiellement petit en traversant chaque couche. Les **connexions rÃ©siduelles** (skip connections) rÃ©solvent ce problÃ¨me en ajoutant l'entrÃ©e directement Ã  la sortie de chaque bloc.`,
      },
      {
        type: 'equation',
        content: '\\mathbf{h}_{k+1} = \\mathbf{h}_k + f_k(\\mathbf{h}_k)',
        label: 'Connexion rÃ©siduelle',
        highlightVar: 'hidden',
      },
      {
        type: 'text',
        content: `Au lieu d'apprendre la transformation complÃ¨te h â†’ h', le rÃ©seau apprend le **rÃ©sidu** f(h) = h' - h. Si le rÃ©sidu est proche de zÃ©ro, le gradient passe directement via le skip connection.\n\nUn **bloc rÃ©siduel** typique contient :\n- BatchNorm â†’ ReLU â†’ Conv â†’ BatchNorm â†’ ReLU â†’ Conv â†’ + input`,
      },
      {
        type: 'callout',
        content: 'ğŸ§  ResNet a permis d\'entraÃ®ner des rÃ©seaux de 152+ couches. Sans skip connections, mÃªme des rÃ©seaux de 30 couches Ã©taient difficiles Ã  entraÃ®ner. L\'idÃ©e clÃ© : le rÃ©seau peut toujours "copier" l\'entrÃ©e si les couches ne sont pas utiles.',
      },
    ],
    exercises: [],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• RÃ©seaux RÃ©siduels â•â•

class ResidualBlock(nn.Module):
    """Bloc rÃ©siduel : sortie = entrÃ©e + f(entrÃ©e)"""
    def __init__(self, channels):
        super().__init__()
        self.block = nn.Sequential(
            nn.BatchNorm2d(channels),
            nn.ReLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.BatchNorm2d(channels),
            nn.ReLU(),
            nn.Conv2d(channels, channels, 3, padding=1),
        )
    
    def forward(self, x):
        return x + self.block(x)  # Skip connection !

class SimpleResNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.res1 = ResidualBlock(32)
        self.res2 = ResidualBlock(32)
        self.res3 = ResidualBlock(32)
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(32, 10)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.res1(x)
        x = self.res2(x)
        x = self.res3(x)
        x = self.pool(x).flatten(1)
        return self.fc(x)

model = SimpleResNet()
x = torch.randn(2, 1, 28, 28)
out = model(x)
print(f"SimpleResNet: {x.shape} â†’ {out.shape}")
print(f"ParamÃ¨tres: {sum(p.numel() for p in model.parameters()):,}")
print(f"Profondeur: 1 conv + 6 conv (3 blocs Ã— 2) = 7 couches conv")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 11 â€” RNN/LSTM
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'rnn',
    title: 'RÃ©seaux RÃ©currents (RNN/LSTM)',
    shortTitle: 'RNN',
    description: 'Traitement des sÃ©quences avec mÃ©moire temporelle et portes.',
    status: 'locked',
    progress: 0,
    dependencies: ['regularization'],
    category: 'architectures',
    theory: [
      {
        type: 'text',
        content: `Les **RNN** traitent des sÃ©quences en maintenant un **Ã©tat cachÃ©** (hidden state) qui encode l'historique. Ã€ chaque pas de temps t, le RNN reÃ§oit l'entrÃ©e xâ‚œ et l'Ã©tat prÃ©cÃ©dent hâ‚œâ‚‹â‚, et produit un nouvel Ã©tat hâ‚œ.\n\nLe problÃ¨me : les gradients s'Ã©vanouissent sur les longues sÃ©quences (long-range dependencies).`,
      },
      {
        type: 'equation',
        content: '\\mathbf{h}_t = \\tanh(\\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{W}_{xh} \\mathbf{x}_t + \\mathbf{b}_h)',
        label: 'RNN â€” Ã‰tat cachÃ©',
      },
      {
        type: 'text',
        content: `Le **LSTM** (Long Short-Term Memory) rÃ©sout le vanishing gradient avec des **portes** (gates) qui contrÃ´lent le flux d'information :\n\n- **Porte d'oubli** (forget gate) : quelle info effacer de la mÃ©moire\n- **Porte d'entrÃ©e** (input gate) : quelle nouvelle info stocker\n- **Porte de sortie** (output gate) : quelle info envoyer en sortie`,
      },
      {
        type: 'equation',
        content: '\\begin{aligned} \\mathbf{f}_t &= \\sigma(\\mathbf{W}_f [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_f) \\\\ \\mathbf{i}_t &= \\sigma(\\mathbf{W}_i [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_i) \\\\ \\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tanh(\\mathbf{W}_c [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_c) \\end{aligned}',
        label: 'LSTM â€” Portes et cellule mÃ©moire',
      },
      {
        type: 'callout',
        content: 'ğŸ’¡ Les Transformers ont largement remplacÃ© les RNN/LSTM pour la plupart des tÃ¢ches sÃ©quentielles (NLP, audio). Cependant, les RNN restent utiles pour les sÃ©quences trÃ¨s longues et le traitement en temps rÃ©el.',
      },
    ],
    exercises: [],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• RÃ©seaux RÃ©currents â•â•

# â”€â”€ 1. RNN simple â”€â”€
rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
x = torch.randn(1, 5, 10)  # batch=1, seq_len=5, features=10
output, h_n = rnn(x)
print(f"RNN Output: {output.shape}")    # (1, 5, 20)
print(f"RNN Hidden: {h_n.shape}")       # (2, 1, 20)

# â”€â”€ 2. LSTM â”€â”€
lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
output, (h_n, c_n) = lstm(x)
print(f"\\nLSTM Output: {output.shape}")
print(f"LSTM Hidden: {h_n.shape}")
print(f"LSTM Cell:   {c_n.shape}")

# â”€â”€ 3. LSTM pour classification de sÃ©quences â”€â”€
class SeqClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x):
        output, (h_n, _) = self.lstm(x)
        # Utiliser le dernier Ã©tat cachÃ©
        return self.fc(h_n.squeeze(0))

clf = SeqClassifier(10, 32, 5)
x = torch.randn(4, 20, 10)  # batch=4, seq_len=20, features=10
out = clf(x)
print(f"\\nClassification: {x.shape} â†’ {out.shape}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 12 â€” TRANSFORMERS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'attention',
    title: 'Attention & Transformers',
    shortTitle: 'Transformer',
    description: 'Self-Attention, Multi-Head Attention et l\'architecture qui a rÃ©volutionnÃ© le NLP.',
    status: 'locked',
    progress: 0,
    dependencies: ['rnn'],
    category: 'advanced',
    theory: [
      {
        type: 'text',
        content: `Le **Transformer** est l'architecture dominante en Deep Learning moderne. Son mÃ©canisme clÃ© est le **dot-product self-attention** qui permet Ã  chaque Ã©lÃ©ment d'une sÃ©quence de "consulter" tous les autres.\n\nPour chaque entrÃ©e xâ‚˜, on calcule trois vecteurs :\n- **Value** vâ‚˜ : le contenu Ã  transmettre\n- **Query** qâ‚™ : "quelle information cherche la position n ?"\n- **Key** kâ‚˜ : "quelle information offre la position m ?"`,
      },
      {
        type: 'equation',
        content: '\\text{sa}_n[\\mathbf{x}_\\bullet] = \\sum_{m=1}^{N} a[\\mathbf{x}_m, \\mathbf{x}_n] \\cdot \\mathbf{v}_m',
        label: 'Self-Attention Output',
      },
      {
        type: 'text',
        content: `Les poids d'attention sont calculÃ©s par produit scalaire queries Ã— keys, divisÃ© par âˆšdâ‚– pour la stabilitÃ© numÃ©rique, puis passÃ©s par softmax. Le nombre de poids d'attention croÃ®t quadratiquement avec la longueur de sÃ©quence N.`,
      },
      {
        type: 'equation',
        content: '\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\!\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right) \\mathbf{V}',
        label: 'Scaled Dot-Product Attention',
      },
      {
        type: 'text',
        content: `Le **Multi-Head Attention** exÃ©cute H mÃ©canismes d'attention en parallÃ¨le, chacun apprenant des types de relations diffÃ©rents. BERT utilise H=12 tÃªtes, GPT-3 utilise H=96 tÃªtes.\n\nUne couche Transformer complÃ¨te = Multi-Head Attention + Add&Norm + FFN + Add&Norm.`,
      },
      {
        type: 'callout',
        content: 'ğŸ§  Le self-attention est un **hypernetwork** : une branche du rÃ©seau (Q,K) calcule les poids pour une autre branche (V). C\'est ce qui rend les Transformers si flexibles â€” les connexions dÃ©pendent des donnÃ©es elles-mÃªmes.',
      },
    ],
    exercises: [
      {
        id: 'attn-ex1',
        title: 'Self-Attention from scratch',
        instructions: 'ImplÃ©mentez le scaled dot-product attention manuellement (sans nn.MultiheadAttention).',
        starterCode: `import torch
import torch.nn as nn
import math

# Dimensions
d_model = 64
seq_len = 5
batch = 1

# Projections Q, K, V
W_q = nn.Linear(d_model, d_model)
W_k = nn.Linear(d_model, d_model)
W_v = nn.Linear(d_model, d_model)

x = torch.randn(batch, seq_len, d_model)

Q = W_q(x)
K = W_k(x)
V = W_v(x)

# Scaled dot-product attention
scores = ___  # Q @ K^T / sqrt(d_k)
weights = ___  # softmax(scores)
output = ___   # weights @ V

print(f"Scores shape: {scores.shape}")
print(f"Attention weights shape: {weights.shape}")
print(f"Output shape: {output.shape}")`,
        solution: `import torch
import torch.nn as nn
import math

d_model = 64
seq_len = 5
batch = 1

W_q = nn.Linear(d_model, d_model)
W_k = nn.Linear(d_model, d_model)
W_v = nn.Linear(d_model, d_model)

x = torch.randn(batch, seq_len, d_model)

Q = W_q(x)
K = W_k(x)
V = W_v(x)

scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_model)
weights = torch.softmax(scores, dim=-1)
output = weights @ V

print(f"Scores shape: {scores.shape}")
print(f"Attention weights shape: {weights.shape}")
print(f"Output shape: {output.shape}")`,
        hints: [
          'scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_model)',
          'weights = torch.softmax(scores, dim=-1)',
          'output = weights @ V',
        ],
        completed: false,
      },
    ],
    codeTemplate: `import torch
import torch.nn as nn
import math

# â•â• Self-Attention & Transformers â•â•

class SelfAttention(nn.Module):
    """Scaled Dot-Product Self-Attention avec Multi-Head"""
    def __init__(self, d_model, n_heads):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_k = d_model // n_heads
        self.n_heads = n_heads
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, x):
        B, T, C = x.shape
        
        # Projections Q, K, V puis split en tÃªtes
        Q = self.W_q(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)
        
        # Scaled dot-product attention
        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)
        attn = torch.softmax(scores, dim=-1)
        
        # Combiner les tÃªtes
        out = (attn @ V).transpose(1, 2).contiguous().view(B, T, C)
        return self.W_o(out)

# Test
attn = SelfAttention(d_model=64, n_heads=8)
x = torch.randn(2, 10, 64)  # batch=2, seq_len=10, d_model=64
out = attn(x)
print(f"Input:  {x.shape}")
print(f"Output: {out.shape}")
print(f"Params: {sum(p.numel() for p in attn.parameters()):,}")
print(f"Heads:  {attn.n_heads}, d_k: {attn.d_k}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 13 â€” GANs
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'gan',
    title: 'Generative Adversarial Networks (GAN)',
    shortTitle: 'GAN',
    description: 'GÃ©nÃ©ration d\'images via un duel GÃ©nÃ©rateur vs Discriminateur.',
    status: 'locked',
    progress: 0,
    dependencies: ['attention'],
    category: 'advanced',
    theory: [
      {
        type: 'text',
        content: `Un **GAN** met en compÃ©tition deux rÃ©seaux :\n\n- **GÃ©nÃ©rateur G** : transforme du bruit alÃ©atoire z en donnÃ©es rÃ©alistes G(z)\n- **Discriminateur D** : distingue les donnÃ©es rÃ©elles des donnÃ©es gÃ©nÃ©rÃ©es\n\nLe gÃ©nÃ©rateur cherche Ã  tromper le discriminateur. Le discriminateur cherche Ã  ne pas Ãªtre trompÃ©. Ce jeu adversarial conduit le gÃ©nÃ©rateur Ã  produire des donnÃ©es de plus en plus rÃ©alistes.`,
      },
      {
        type: 'equation',
        content: '\\min_G \\max_D \\; \\mathbb{E}_{x}[\\log D(x)] + \\mathbb{E}_{z}[\\log(1 - D(G(z)))]',
        label: 'Objectif du GAN (Minimax)',
      },
      {
        type: 'callout',
        content: 'âš¡ Les GANs sont notoirement difficiles Ã  entraÃ®ner (mode collapse, instabilitÃ©). Des variantes comme WGAN, StyleGAN, et la progressive growing ont rÃ©solu beaucoup de ces problÃ¨mes.',
      },
    ],
    exercises: [],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• Generative Adversarial Network (GAN) â•â•

# GÃ©nÃ©rateur : bruit â†’ image
class Generator(nn.Module):
    def __init__(self, latent_dim=100, img_dim=784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, img_dim),
            nn.Tanh()
        )
    
    def forward(self, z):
        return self.net(z)

# Discriminateur : image â†’ rÃ©el/faux
class Discriminator(nn.Module):
    def __init__(self, img_dim=784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(img_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.net(x)

G = Generator()
D = Discriminator()

# GÃ©nÃ©rer une image depuis du bruit
z = torch.randn(1, 100)
fake_img = G(z)
score = D(fake_img)

print(f"Bruit z: {z.shape}")
print(f"Image gÃ©nÃ©rÃ©e: {fake_img.shape}")
print(f"Score discriminateur: {score.item():.4f} (0=faux, 1=vrai)")
print(f"\\nG params: {sum(p.numel() for p in G.parameters()):,}")
print(f"D params: {sum(p.numel() for p in D.parameters()):,}")
`,
  },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // MODULE 14 â€” DIFFUSION MODELS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  {
    id: 'diffusion',
    title: 'ModÃ¨les de Diffusion',
    shortTitle: 'Diffusion',
    description: 'Le processus de bruitage/dÃ©bruitage qui gÃ©nÃ¨re des images photorÃ©alistes.',
    status: 'locked',
    progress: 0,
    dependencies: ['attention'],
    category: 'advanced',
    theory: [
      {
        type: 'text',
        content: `Les **modÃ¨les de diffusion** apprennent Ã  gÃ©nÃ©rer des donnÃ©es en inversant un processus de bruitage progressif. Le modÃ¨le apprend Ã  **dÃ©bruiter** â€” Ã  chaque Ã©tape, il enlÃ¨ve un peu de bruit pour reconstruire l'image originale.\n\n- **Forward process** (encoder) : ajouter progressivement du bruit gaussien Ã  l'image\n- **Reverse process** (decoder) : apprendre Ã  retirer le bruit Ã©tape par Ã©tape`,
      },
      {
        type: 'equation',
        content: 'q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1-\\beta_t} \\, \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I})',
        label: 'Forward Process (ajout de bruit)',
      },
      {
        type: 'equation',
        content: '\\mathcal{L} = \\mathbb{E}_{t, \\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\left[ \\| \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\|^2 \\right]',
        label: 'Objectif simplifiÃ© (prÃ©dire le bruit)',
        highlightVar: 'loss',
      },
      {
        type: 'callout',
        content: 'ğŸ§  DALL-E, Stable Diffusion, et Midjourney utilisent tous des modÃ¨les de diffusion. L\'idÃ©e clÃ© : au lieu de gÃ©nÃ©rer une image d\'un coup, on la "dÃ©bruite" progressivement depuis du bruit pur en T Ã©tapes (typiquement T=1000).',
      },
    ],
    exercises: [],
    codeTemplate: `import torch
import torch.nn as nn

# â•â• ModÃ¨le de Diffusion â€” Concept simplifiÃ© â•â•

class SimpleDenoiser(nn.Module):
    """RÃ©seau qui prÃ©dit le bruit ajoutÃ© Ã  une image"""
    def __init__(self, dim=784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim + 1, 512),  # +1 pour le timestep t
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, dim)
        )
    
    def forward(self, x_noisy, t):
        """PrÃ©dit le bruit epsilon Ã  partir de x_noisy et t"""
        t_embed = t.unsqueeze(-1)  # (batch, 1)
        inp = torch.cat([x_noisy, t_embed], dim=-1)
        return self.net(inp)

# â”€â”€ Forward process : ajouter du bruit â”€â”€
T = 1000
betas = torch.linspace(0.0001, 0.02, T)
alphas = 1 - betas
alpha_bar = torch.cumprod(alphas, dim=0)

def add_noise(x0, t, noise=None):
    """Ajoute du bruit au timestep t"""
    if noise is None:
        noise = torch.randn_like(x0)
    sqrt_ab = torch.sqrt(alpha_bar[t]).unsqueeze(-1)
    sqrt_1_ab = torch.sqrt(1 - alpha_bar[t]).unsqueeze(-1)
    return sqrt_ab * x0 + sqrt_1_ab * noise, noise

# Test
model = SimpleDenoiser(dim=784)
x0 = torch.randn(4, 784)  # 4 images "propres"
t = torch.randint(0, T, (4,))  # timesteps alÃ©atoires

x_noisy, true_noise = add_noise(x0, t)
pred_noise = model(x_noisy, t.float() / T)

loss = nn.MSELoss()(pred_noise, true_noise)
print(f"x0 shape: {x0.shape}")
print(f"x_noisy shape: {x_noisy.shape}")
print(f"Loss: {loss.item():.4f}")
print(f"\\nObjectif: prÃ©dire le bruit Îµ ajoutÃ© Ã  l'image")
`,
  },
];

// â”€â”€ Graph positions for Roadmap visualization â”€â”€
export const nodePositions: Record<string, { x: number; y: number }> = {
  // Row 1 â€” Fundamentals
  'supervised-learning': { x: 100, y: 80 },
  'tensors':             { x: 350, y: 80 },
  'shallow-networks':    { x: 600, y: 80 },
  'deep-networks':       { x: 850, y: 80 },
  // Row 2 â€” Training
  'loss-functions':      { x: 100, y: 260 },
  'gradient-descent':    { x: 350, y: 260 },
  'backprop':            { x: 600, y: 260 },
  'regularization':      { x: 850, y: 260 },
  // Row 3 â€” Architectures
  'cnn':                 { x: 100, y: 440 },
  'resnet':              { x: 350, y: 440 },
  'rnn':                 { x: 600, y: 440 },
  // Row 4 â€” Advanced
  'attention':           { x: 350, y: 620 },
  'gan':                 { x: 600, y: 620 },
  'diffusion':           { x: 850, y: 620 },
};

export function getTotalProgress(nodes: CourseNode[]): number {
  const completed = nodes.filter(n => n.status === 'completed').length;
  return Math.round((completed / nodes.length) * 100);
}
